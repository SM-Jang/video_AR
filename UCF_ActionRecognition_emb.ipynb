{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import natsort\n",
    "import bezier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from dataset import encoder\n",
    "from model import get_pretrained_model\n",
    "from dataset import jpg2np, get_loader\n",
    "from torchvision import models\n",
    "from bezier.hazmat.curve_helpers import evaluate_hodograph, get_curvature\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils import data\n",
    "from UCF_dataset import UCFdataset\n",
    "from model import UCF_DNN, UCF_CNN1D, UCF_CNN2D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  2\n"
     ]
    }
   ],
   "source": [
    "cut = 9812\n",
    "upper, lower = 150, 10000\n",
    "GPU_NUM = 2\n",
    "bs=1\n",
    "upper, lower, dir_path = 150, 10000, './ucf_image'\n",
    "datastyle = 'elementwise'\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select The number of frames between [150, 10000] of UCF101 Dataset\n",
      "The number of selected videos is 9812\n"
     ]
    }
   ],
   "source": [
    "dataset = UCFdataset(upper, lower, dir_path)\n",
    "loader = data.DataLoader(dataset=dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "num_of_classes = 101\n",
    "epochs=10\n",
    "\n",
    "\n",
    "## Embedding ##\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg_embedding = nn.Sequential(vgg16.features,\n",
    "#                    nn.AdaptiveAvgPool2d((1,1))).to(device)\n",
    "\n",
    "# bs_step = 0\n",
    "# for x, y, video_name in loader:\n",
    "#     if '{}.npy'.format(list(video_name)[0]) in  os.listdir('./ucf_embeddings/'): \n",
    "#         bs_step += bs\n",
    "#         continue\n",
    "#     x, y = x.to(device).squeeze(), y.squeeze().long().to(device)\n",
    "\n",
    "\n",
    "#     ## VGG Embedding ##\n",
    "#     embedding = vgg_embedding(x).squeeze().cpu().detach().numpy() # [30, 512]\n",
    "#     np.save('./ucf_embeddings/{}.npy'.format(list(video_name)[0]), embedding)    \n",
    "#     bs_step += bs\n",
    "#     if bs_step % 10 == 0: \n",
    "#         print(\"Batch {}/{} | {}\".format(bs_step, 9812, video_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load vgg16 embedding data ##\n",
    "video_names = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "for embedding in os.listdir('./ucf_embeddings'):\n",
    "    if embedding == '.ipynb_checkpoints': continue\n",
    "    loc1 = embedding.find('_')\n",
    "    loc2 = loc1 + embedding[loc1+1:].find('_')\n",
    "    labels.append(embedding[loc1+1:loc2+1])\n",
    "    video_names.append(embedding[:-4])\n",
    "  \n",
    "\n",
    "    embeddings.append(np.load('./ucf_embeddings/{}'.format(embedding)))\n",
    "\n",
    "\n",
    "y, _ = encoder(labels)\n",
    "\n",
    "embeddings = np.stack(embeddings)\n",
    "embeddings_pca = embeddings.reshape(embeddings.shape[0]*embeddings.shape[1],-1)\n",
    "\n",
    "## PCA ##\n",
    "pca = PCA(3)\n",
    "embeddings_pca = pca.fit_transform(embeddings_pca).reshape(embeddings.shape[0], embeddings.shape[1],-1)\n",
    "embeddings_pca.shape\n",
    "\n",
    "## Bezier Curve and Curvature ##\n",
    "k = dict()\n",
    "curves = []\n",
    "for i, embedding in enumerate(embeddings_pca):\n",
    "#     print(embedding.shape)\n",
    "    curves.append(bezier.Curve.from_nodes(embedding.T))\n",
    "    kappa = []\n",
    "    for s in range(30):\n",
    "        t = s / 30\n",
    "        tangent_vec = curves[i].evaluate_hodograph(t)\n",
    "        kappa.append(get_curvature(embedding.T, tangent_vec, t))\n",
    "    k[video_names[i]] = kappa\n",
    "    if i% 50 == 0:\n",
    "        print(i, len(embeddings_pca))\n",
    "\n",
    "K = np.stack(list(k.values())) # [N, 30]\n",
    "K.shape\n",
    "\n",
    "## Elementwise operation ##\n",
    "result = list()\n",
    "for video in range(len(embeddings)):\n",
    "    elementwise = list()\n",
    "    for frame in range(30):\n",
    "        elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "    result.append(np.stack(elementwise))\n",
    "    if (video+1) % 100 == 0: \n",
    "        print(\"[%d / %d] video processing!\" %(video, len(embeddings)))\n",
    "result = torch.from_numpy(np.stack(result))\n",
    "print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9812, 30, 512]) torch.Size([9812, 1])\n"
     ]
    }
   ],
   "source": [
    "## Data Setting for Action Recognition(Prediction) ##\n",
    "datastyle ='embeddings'\n",
    "\n",
    "if datastyle =='embeddings':\n",
    "    X = torch.from_numpy(embeddings)\n",
    "if datastyle == 'elementwise':\n",
    "    X = result\n",
    "Y = torch.from_numpy(y)\n",
    "\n",
    "## train / test split ##\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, shuffle=True, random_state=123)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6708, device='cuda:2', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN_criterion(y_pred, y_true.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_DNN\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.0824 | Accuracy 6.98\n",
      "Epoch 0| Batch 400/9812 | Loss 4.7018 | Accuracy 6.98\n",
      "Epoch 0| Batch 600/9812 | Loss 3.8146 | Accuracy 6.98\n",
      "Epoch 0| Batch 800/9812 | Loss 4.9939 | Accuracy 6.98\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.5152 | Accuracy 6.98\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.7242 | Accuracy 6.98\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.3226 | Accuracy 6.98\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.1394 | Accuracy 6.98\n",
      "Epoch 0| Batch 1800/9812 | Loss 1.8221 | Accuracy 6.98\n",
      "Epoch 0| Batch 2000/9812 | Loss 4.4030 | Accuracy 6.98\n",
      "Epoch 0| Batch 2200/9812 | Loss 4.5080 | Accuracy 6.98\n",
      "Epoch 0| Batch 2400/9812 | Loss 4.6172 | Accuracy 6.98\n",
      "Epoch 0| Batch 2600/9812 | Loss 4.2479 | Accuracy 6.98\n",
      "Epoch 0| Batch 2800/9812 | Loss 4.0724 | Accuracy 6.98\n",
      "Epoch 0| Batch 3000/9812 | Loss 4.8978 | Accuracy 6.98\n",
      "Epoch 0| Batch 3200/9812 | Loss 3.5013 | Accuracy 6.98\n",
      "Epoch 0| Batch 3400/9812 | Loss 5.6813 | Accuracy 6.98\n",
      "Epoch 0| Batch 3600/9812 | Loss 4.6146 | Accuracy 6.98\n",
      "Epoch 0| Batch 3800/9812 | Loss 4.1005 | Accuracy 6.98\n",
      "Epoch 0| Batch 4000/9812 | Loss 3.8579 | Accuracy 6.98\n",
      "Epoch 0| Batch 4200/9812 | Loss 4.1906 | Accuracy 6.98\n",
      "Epoch 0| Batch 4400/9812 | Loss 3.8612 | Accuracy 6.98\n",
      "Epoch 0| Batch 4600/9812 | Loss 2.9588 | Accuracy 6.98\n",
      "Epoch 0| Batch 4800/9812 | Loss 3.9443 | Accuracy 6.98\n",
      "Epoch 0| Batch 5000/9812 | Loss 4.3516 | Accuracy 6.98\n",
      "Epoch 0| Batch 5200/9812 | Loss 3.3282 | Accuracy 6.98\n",
      "Epoch 0| Batch 5400/9812 | Loss 4.7970 | Accuracy 6.98\n",
      "Epoch 0| Batch 5600/9812 | Loss 4.0170 | Accuracy 6.98\n",
      "Epoch 0| Batch 5800/9812 | Loss 2.9483 | Accuracy 6.98\n",
      "Epoch 0| Batch 6000/9812 | Loss 3.0432 | Accuracy 6.98\n",
      "Epoch 0| Batch 6200/9812 | Loss 3.5938 | Accuracy 6.98\n",
      "Epoch 0| Batch 6400/9812 | Loss 3.0768 | Accuracy 6.98\n",
      "Epoch 0| Batch 6600/9812 | Loss 3.9606 | Accuracy 6.98\n",
      "Epoch 0| Batch 6800/9812 | Loss 2.7100 | Accuracy 6.98\n",
      "Epoch 0| Batch 7000/9812 | Loss 4.7251 | Accuracy 6.98\n",
      "Epoch 0| Batch 7200/9812 | Loss 4.3475 | Accuracy 6.98\n",
      "Epoch 0| Batch 7400/9812 | Loss 3.8960 | Accuracy 6.98\n",
      "Epoch 0| Batch 7600/9812 | Loss 3.4022 | Accuracy 6.98\n",
      "Epoch 0| Batch 7800/9812 | Loss 4.1404 | Accuracy 6.98\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 4.5893 | Accuracy 7.62\n",
      "Epoch 1| Batch 200/9812 | Loss 3.7655 | Accuracy 7.62\n",
      "Epoch 1| Batch 400/9812 | Loss 0.1083 | Accuracy 7.62\n",
      "Epoch 1| Batch 600/9812 | Loss 4.0647 | Accuracy 7.62\n",
      "Epoch 1| Batch 800/9812 | Loss 0.8774 | Accuracy 7.62\n",
      "Epoch 1| Batch 1000/9812 | Loss 1.6074 | Accuracy 7.62\n",
      "Epoch 1| Batch 1200/9812 | Loss 4.5643 | Accuracy 7.62\n",
      "Epoch 1| Batch 1400/9812 | Loss 1.0822 | Accuracy 7.62\n",
      "Epoch 1| Batch 1600/9812 | Loss 3.6288 | Accuracy 7.62\n",
      "Epoch 1| Batch 1800/9812 | Loss 5.5428 | Accuracy 7.62\n",
      "Epoch 1| Batch 2000/9812 | Loss 0.8224 | Accuracy 7.62\n",
      "Epoch 1| Batch 2200/9812 | Loss 3.8895 | Accuracy 7.62\n",
      "Epoch 1| Batch 2400/9812 | Loss 2.2598 | Accuracy 7.62\n",
      "Epoch 1| Batch 2600/9812 | Loss 3.5516 | Accuracy 7.62\n",
      "Epoch 1| Batch 2800/9812 | Loss 0.5811 | Accuracy 7.62\n",
      "Epoch 1| Batch 3000/9812 | Loss 3.7519 | Accuracy 7.62\n",
      "Epoch 1| Batch 3200/9812 | Loss 1.0283 | Accuracy 7.62\n",
      "Epoch 1| Batch 3400/9812 | Loss 0.3639 | Accuracy 7.62\n",
      "Epoch 1| Batch 3600/9812 | Loss 2.9495 | Accuracy 7.62\n",
      "Epoch 1| Batch 3800/9812 | Loss 3.1438 | Accuracy 7.62\n",
      "Epoch 1| Batch 4000/9812 | Loss 3.3652 | Accuracy 7.62\n",
      "Epoch 1| Batch 4200/9812 | Loss 4.4224 | Accuracy 7.62\n",
      "Epoch 1| Batch 4400/9812 | Loss 1.9048 | Accuracy 7.62\n",
      "Epoch 1| Batch 4600/9812 | Loss 2.5165 | Accuracy 7.62\n",
      "Epoch 1| Batch 4800/9812 | Loss 0.8880 | Accuracy 7.62\n",
      "Epoch 1| Batch 5000/9812 | Loss 0.3318 | Accuracy 7.62\n",
      "Epoch 1| Batch 5200/9812 | Loss 0.5366 | Accuracy 7.62\n",
      "Epoch 1| Batch 5400/9812 | Loss 2.5818 | Accuracy 7.62\n",
      "Epoch 1| Batch 5600/9812 | Loss 4.4258 | Accuracy 7.62\n",
      "Epoch 1| Batch 5800/9812 | Loss 3.4132 | Accuracy 7.62\n",
      "Epoch 1| Batch 6000/9812 | Loss 3.5235 | Accuracy 7.62\n",
      "Epoch 1| Batch 6200/9812 | Loss 4.4839 | Accuracy 7.62\n",
      "Epoch 1| Batch 6400/9812 | Loss 3.9952 | Accuracy 7.62\n",
      "Epoch 1| Batch 6600/9812 | Loss 3.1230 | Accuracy 7.62\n",
      "Epoch 1| Batch 6800/9812 | Loss 1.6334 | Accuracy 7.62\n",
      "Epoch 1| Batch 7000/9812 | Loss 3.1465 | Accuracy 7.62\n",
      "Epoch 1| Batch 7200/9812 | Loss 2.4918 | Accuracy 7.62\n",
      "Epoch 1| Batch 7400/9812 | Loss 1.8644 | Accuracy 7.62\n",
      "Epoch 1| Batch 7600/9812 | Loss 3.1153 | Accuracy 7.62\n",
      "Epoch 1| Batch 7800/9812 | Loss 5.7200 | Accuracy 7.62\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 2.6120 | Accuracy 25.10\n",
      "Epoch 2| Batch 200/9812 | Loss 3.6057 | Accuracy 25.10\n",
      "Epoch 2| Batch 400/9812 | Loss 2.7302 | Accuracy 25.10\n",
      "Epoch 2| Batch 600/9812 | Loss 0.2022 | Accuracy 25.10\n",
      "Epoch 2| Batch 800/9812 | Loss 0.6697 | Accuracy 25.10\n",
      "Epoch 2| Batch 1000/9812 | Loss 3.8398 | Accuracy 25.10\n",
      "Epoch 2| Batch 1200/9812 | Loss 2.0568 | Accuracy 25.10\n",
      "Epoch 2| Batch 1400/9812 | Loss 0.0300 | Accuracy 25.10\n",
      "Epoch 2| Batch 1600/9812 | Loss 2.3638 | Accuracy 25.10\n",
      "Epoch 2| Batch 1800/9812 | Loss 1.4214 | Accuracy 25.10\n",
      "Epoch 2| Batch 2000/9812 | Loss 0.3758 | Accuracy 25.10\n",
      "Epoch 2| Batch 2200/9812 | Loss 0.7799 | Accuracy 25.10\n",
      "Epoch 2| Batch 2400/9812 | Loss 4.0358 | Accuracy 25.10\n",
      "Epoch 2| Batch 2600/9812 | Loss 2.9200 | Accuracy 25.10\n",
      "Epoch 2| Batch 2800/9812 | Loss 3.1859 | Accuracy 25.10\n",
      "Epoch 2| Batch 3000/9812 | Loss 1.8092 | Accuracy 25.10\n",
      "Epoch 2| Batch 3200/9812 | Loss 1.1837 | Accuracy 25.10\n",
      "Epoch 2| Batch 3400/9812 | Loss 5.2994 | Accuracy 25.10\n",
      "Epoch 2| Batch 3600/9812 | Loss 1.7528 | Accuracy 25.10\n",
      "Epoch 2| Batch 3800/9812 | Loss 1.5531 | Accuracy 25.10\n",
      "Epoch 2| Batch 4000/9812 | Loss 2.4295 | Accuracy 25.10\n",
      "Epoch 2| Batch 4200/9812 | Loss 2.4593 | Accuracy 25.10\n",
      "Epoch 2| Batch 4400/9812 | Loss 0.7508 | Accuracy 25.10\n",
      "Epoch 2| Batch 4600/9812 | Loss 2.2224 | Accuracy 25.10\n",
      "Epoch 2| Batch 4800/9812 | Loss 4.5410 | Accuracy 25.10\n",
      "Epoch 2| Batch 5000/9812 | Loss 0.4372 | Accuracy 25.10\n",
      "Epoch 2| Batch 5200/9812 | Loss 1.0508 | Accuracy 25.10\n",
      "Epoch 2| Batch 5400/9812 | Loss 0.1082 | Accuracy 25.10\n",
      "Epoch 2| Batch 5600/9812 | Loss 0.4161 | Accuracy 25.10\n",
      "Epoch 2| Batch 5800/9812 | Loss 1.8796 | Accuracy 25.10\n",
      "Epoch 2| Batch 6000/9812 | Loss 4.4079 | Accuracy 25.10\n",
      "Epoch 2| Batch 6200/9812 | Loss 0.4575 | Accuracy 25.10\n",
      "Epoch 2| Batch 6400/9812 | Loss 2.9414 | Accuracy 25.10\n",
      "Epoch 2| Batch 6600/9812 | Loss 2.1729 | Accuracy 25.10\n",
      "Epoch 2| Batch 6800/9812 | Loss 2.6933 | Accuracy 25.10\n",
      "Epoch 2| Batch 7000/9812 | Loss 4.0628 | Accuracy 25.10\n",
      "Epoch 2| Batch 7200/9812 | Loss 0.0239 | Accuracy 25.10\n",
      "Epoch 2| Batch 7400/9812 | Loss 1.7853 | Accuracy 25.10\n",
      "Epoch 2| Batch 7600/9812 | Loss 0.0364 | Accuracy 25.10\n",
      "Epoch 2| Batch 7800/9812 | Loss 3.9526 | Accuracy 25.10\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 3.4974 | Accuracy 37.89\n",
      "Epoch 3| Batch 200/9812 | Loss 0.0006 | Accuracy 37.89\n",
      "Epoch 3| Batch 400/9812 | Loss 2.8108 | Accuracy 37.89\n",
      "Epoch 3| Batch 600/9812 | Loss 3.0745 | Accuracy 37.89\n",
      "Epoch 3| Batch 800/9812 | Loss 3.4596 | Accuracy 37.89\n",
      "Epoch 3| Batch 1000/9812 | Loss 0.2620 | Accuracy 37.89\n",
      "Epoch 3| Batch 1200/9812 | Loss 1.4133 | Accuracy 37.89\n",
      "Epoch 3| Batch 1400/9812 | Loss 0.2167 | Accuracy 37.89\n",
      "Epoch 3| Batch 1600/9812 | Loss 2.0883 | Accuracy 37.89\n",
      "Epoch 3| Batch 1800/9812 | Loss 0.0518 | Accuracy 37.89\n",
      "Epoch 3| Batch 2000/9812 | Loss 4.3354 | Accuracy 37.89\n",
      "Epoch 3| Batch 2200/9812 | Loss 1.3093 | Accuracy 37.89\n",
      "Epoch 3| Batch 2400/9812 | Loss 0.9998 | Accuracy 37.89\n",
      "Epoch 3| Batch 2600/9812 | Loss 1.7162 | Accuracy 37.89\n",
      "Epoch 3| Batch 2800/9812 | Loss 0.6684 | Accuracy 37.89\n",
      "Epoch 3| Batch 3000/9812 | Loss 3.8205 | Accuracy 37.89\n",
      "Epoch 3| Batch 3200/9812 | Loss 0.4542 | Accuracy 37.89\n",
      "Epoch 3| Batch 3400/9812 | Loss 1.4063 | Accuracy 37.89\n",
      "Epoch 3| Batch 3600/9812 | Loss 0.0108 | Accuracy 37.89\n",
      "Epoch 3| Batch 3800/9812 | Loss 0.0893 | Accuracy 37.89\n",
      "Epoch 3| Batch 4000/9812 | Loss 0.5841 | Accuracy 37.89\n",
      "Epoch 3| Batch 4200/9812 | Loss 0.3007 | Accuracy 37.89\n",
      "Epoch 3| Batch 4400/9812 | Loss 3.1806 | Accuracy 37.89\n",
      "Epoch 3| Batch 4600/9812 | Loss 0.9886 | Accuracy 37.89\n",
      "Epoch 3| Batch 4800/9812 | Loss 3.7387 | Accuracy 37.89\n",
      "Epoch 3| Batch 5000/9812 | Loss 0.7957 | Accuracy 37.89\n",
      "Epoch 3| Batch 5200/9812 | Loss 1.7683 | Accuracy 37.89\n",
      "Epoch 3| Batch 5400/9812 | Loss 0.3243 | Accuracy 37.89\n",
      "Epoch 3| Batch 5600/9812 | Loss 1.2040 | Accuracy 37.89\n",
      "Epoch 3| Batch 5800/9812 | Loss 1.7648 | Accuracy 37.89\n",
      "Epoch 3| Batch 6000/9812 | Loss 1.3274 | Accuracy 37.89\n",
      "Epoch 3| Batch 6200/9812 | Loss 3.4891 | Accuracy 37.89\n",
      "Epoch 3| Batch 6400/9812 | Loss 0.3930 | Accuracy 37.89\n",
      "Epoch 3| Batch 6600/9812 | Loss 4.1192 | Accuracy 37.89\n",
      "Epoch 3| Batch 6800/9812 | Loss 3.2057 | Accuracy 37.89\n",
      "Epoch 3| Batch 7000/9812 | Loss 3.9509 | Accuracy 37.89\n",
      "Epoch 3| Batch 7200/9812 | Loss 2.5020 | Accuracy 37.89\n",
      "Epoch 3| Batch 7400/9812 | Loss 0.1478 | Accuracy 37.89\n",
      "Epoch 3| Batch 7600/9812 | Loss 3.5148 | Accuracy 37.89\n",
      "Epoch 3| Batch 7800/9812 | Loss 2.3652 | Accuracy 37.89\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 3.3677 | Accuracy 45.59\n",
      "Epoch 4| Batch 200/9812 | Loss 0.0873 | Accuracy 45.59\n",
      "Epoch 4| Batch 400/9812 | Loss 2.4554 | Accuracy 45.59\n",
      "Epoch 4| Batch 600/9812 | Loss 4.0609 | Accuracy 45.59\n",
      "Epoch 4| Batch 800/9812 | Loss 0.2275 | Accuracy 45.59\n",
      "Epoch 4| Batch 1000/9812 | Loss 0.5615 | Accuracy 45.59\n",
      "Epoch 4| Batch 1200/9812 | Loss 1.9393 | Accuracy 45.59\n",
      "Epoch 4| Batch 1400/9812 | Loss 3.8572 | Accuracy 45.59\n",
      "Epoch 4| Batch 1600/9812 | Loss 0.0001 | Accuracy 45.59\n",
      "Epoch 4| Batch 1800/9812 | Loss 3.2211 | Accuracy 45.59\n",
      "Epoch 4| Batch 2000/9812 | Loss 1.1153 | Accuracy 45.59\n",
      "Epoch 4| Batch 2200/9812 | Loss 0.1423 | Accuracy 45.59\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.0108 | Accuracy 45.59\n",
      "Epoch 4| Batch 2600/9812 | Loss 0.1885 | Accuracy 45.59\n",
      "Epoch 4| Batch 2800/9812 | Loss 1.4678 | Accuracy 45.59\n",
      "Epoch 4| Batch 3000/9812 | Loss 0.1936 | Accuracy 45.59\n",
      "Epoch 4| Batch 3200/9812 | Loss 0.1191 | Accuracy 45.59\n",
      "Epoch 4| Batch 3400/9812 | Loss 0.2765 | Accuracy 45.59\n",
      "Epoch 4| Batch 3600/9812 | Loss 0.6932 | Accuracy 45.59\n",
      "Epoch 4| Batch 3800/9812 | Loss 1.6607 | Accuracy 45.59\n",
      "Epoch 4| Batch 4000/9812 | Loss 0.4060 | Accuracy 45.59\n",
      "Epoch 4| Batch 4200/9812 | Loss 4.1933 | Accuracy 45.59\n",
      "Epoch 4| Batch 4400/9812 | Loss 4.8715 | Accuracy 45.59\n",
      "Epoch 4| Batch 4600/9812 | Loss 1.0821 | Accuracy 45.59\n",
      "Epoch 4| Batch 4800/9812 | Loss 2.5785 | Accuracy 45.59\n",
      "Epoch 4| Batch 5000/9812 | Loss 4.5310 | Accuracy 45.59\n",
      "Epoch 4| Batch 5200/9812 | Loss 0.7883 | Accuracy 45.59\n",
      "Epoch 4| Batch 5400/9812 | Loss 1.5102 | Accuracy 45.59\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.7878 | Accuracy 45.59\n",
      "Epoch 4| Batch 5800/9812 | Loss 0.1834 | Accuracy 45.59\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.0715 | Accuracy 45.59\n",
      "Epoch 4| Batch 6200/9812 | Loss 0.0219 | Accuracy 45.59\n",
      "Epoch 4| Batch 6400/9812 | Loss 2.1226 | Accuracy 45.59\n",
      "Epoch 4| Batch 6600/9812 | Loss 8.7775 | Accuracy 45.59\n",
      "Epoch 4| Batch 6800/9812 | Loss 1.7721 | Accuracy 45.59\n",
      "Epoch 4| Batch 7000/9812 | Loss 4.3242 | Accuracy 45.59\n",
      "Epoch 4| Batch 7200/9812 | Loss 0.2597 | Accuracy 45.59\n",
      "Epoch 4| Batch 7400/9812 | Loss 1.1738 | Accuracy 45.59\n",
      "Epoch 4| Batch 7600/9812 | Loss 0.3374 | Accuracy 45.59\n",
      "Epoch 4| Batch 7800/9812 | Loss 0.0113 | Accuracy 45.59\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 2.5815 | Accuracy 53.19\n",
      "Epoch 5| Batch 200/9812 | Loss 0.0000 | Accuracy 53.19\n",
      "Epoch 5| Batch 400/9812 | Loss 0.0001 | Accuracy 53.19\n",
      "Epoch 5| Batch 600/9812 | Loss 0.0547 | Accuracy 53.19\n",
      "Epoch 5| Batch 800/9812 | Loss 2.7642 | Accuracy 53.19\n",
      "Epoch 5| Batch 1000/9812 | Loss 0.1080 | Accuracy 53.19\n",
      "Epoch 5| Batch 1200/9812 | Loss 5.3876 | Accuracy 53.19\n",
      "Epoch 5| Batch 1400/9812 | Loss 0.1246 | Accuracy 53.19\n",
      "Epoch 5| Batch 1600/9812 | Loss 0.1759 | Accuracy 53.19\n",
      "Epoch 5| Batch 1800/9812 | Loss 2.1201 | Accuracy 53.19\n",
      "Epoch 5| Batch 2000/9812 | Loss 0.1350 | Accuracy 53.19\n",
      "Epoch 5| Batch 2200/9812 | Loss 1.1234 | Accuracy 53.19\n",
      "Epoch 5| Batch 2400/9812 | Loss 1.9417 | Accuracy 53.19\n",
      "Epoch 5| Batch 2600/9812 | Loss 0.1368 | Accuracy 53.19\n",
      "Epoch 5| Batch 2800/9812 | Loss 2.4896 | Accuracy 53.19\n",
      "Epoch 5| Batch 3000/9812 | Loss 0.2348 | Accuracy 53.19\n",
      "Epoch 5| Batch 3200/9812 | Loss 1.0674 | Accuracy 53.19\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.6583 | Accuracy 53.19\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.0964 | Accuracy 53.19\n",
      "Epoch 5| Batch 3800/9812 | Loss 2.0281 | Accuracy 53.19\n",
      "Epoch 5| Batch 4000/9812 | Loss 0.1230 | Accuracy 53.19\n",
      "Epoch 5| Batch 4200/9812 | Loss 3.7773 | Accuracy 53.19\n",
      "Epoch 5| Batch 4400/9812 | Loss 2.3443 | Accuracy 53.19\n",
      "Epoch 5| Batch 4600/9812 | Loss 0.0035 | Accuracy 53.19\n",
      "Epoch 5| Batch 4800/9812 | Loss 2.5368 | Accuracy 53.19\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.3840 | Accuracy 53.19\n",
      "Epoch 5| Batch 5200/9812 | Loss 3.1697 | Accuracy 53.19\n",
      "Epoch 5| Batch 5400/9812 | Loss 1.7426 | Accuracy 53.19\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.5077 | Accuracy 53.19\n",
      "Epoch 5| Batch 5800/9812 | Loss 0.3085 | Accuracy 53.19\n",
      "Epoch 5| Batch 6000/9812 | Loss 2.5316 | Accuracy 53.19\n",
      "Epoch 5| Batch 6200/9812 | Loss 1.3834 | Accuracy 53.19\n",
      "Epoch 5| Batch 6400/9812 | Loss 0.3392 | Accuracy 53.19\n",
      "Epoch 5| Batch 6600/9812 | Loss 0.0289 | Accuracy 53.19\n",
      "Epoch 5| Batch 6800/9812 | Loss 0.0018 | Accuracy 53.19\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.2290 | Accuracy 53.19\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.3333 | Accuracy 53.19\n",
      "Epoch 5| Batch 7400/9812 | Loss 0.3326 | Accuracy 53.19\n",
      "Epoch 5| Batch 7600/9812 | Loss 0.5304 | Accuracy 53.19\n",
      "Epoch 5| Batch 7800/9812 | Loss 0.1159 | Accuracy 53.19\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 3.2886 | Accuracy 58.30\n",
      "Epoch 6| Batch 200/9812 | Loss 0.0409 | Accuracy 58.30\n",
      "Epoch 6| Batch 400/9812 | Loss 0.0715 | Accuracy 58.30\n",
      "Epoch 6| Batch 600/9812 | Loss 0.0069 | Accuracy 58.30\n",
      "Epoch 6| Batch 800/9812 | Loss 0.9708 | Accuracy 58.30\n",
      "Epoch 6| Batch 1000/9812 | Loss 0.0008 | Accuracy 58.30\n",
      "Epoch 6| Batch 1200/9812 | Loss 0.4143 | Accuracy 58.30\n",
      "Epoch 6| Batch 1400/9812 | Loss 2.2466 | Accuracy 58.30\n",
      "Epoch 6| Batch 1600/9812 | Loss 1.2936 | Accuracy 58.30\n",
      "Epoch 6| Batch 1800/9812 | Loss 3.0690 | Accuracy 58.30\n",
      "Epoch 6| Batch 2000/9812 | Loss 2.0673 | Accuracy 58.30\n",
      "Epoch 6| Batch 2200/9812 | Loss 3.0111 | Accuracy 58.30\n",
      "Epoch 6| Batch 2400/9812 | Loss 0.2402 | Accuracy 58.30\n",
      "Epoch 6| Batch 2600/9812 | Loss 4.0392 | Accuracy 58.30\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.5053 | Accuracy 58.30\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.1775 | Accuracy 58.30\n",
      "Epoch 6| Batch 3200/9812 | Loss 0.9769 | Accuracy 58.30\n",
      "Epoch 6| Batch 3400/9812 | Loss 0.0262 | Accuracy 58.30\n",
      "Epoch 6| Batch 3600/9812 | Loss 2.3367 | Accuracy 58.30\n",
      "Epoch 6| Batch 3800/9812 | Loss 1.1376 | Accuracy 58.30\n",
      "Epoch 6| Batch 4000/9812 | Loss 0.0996 | Accuracy 58.30\n",
      "Epoch 6| Batch 4200/9812 | Loss 2.5497 | Accuracy 58.30\n",
      "Epoch 6| Batch 4400/9812 | Loss 3.6492 | Accuracy 58.30\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.1604 | Accuracy 58.30\n",
      "Epoch 6| Batch 4800/9812 | Loss 0.0108 | Accuracy 58.30\n",
      "Epoch 6| Batch 5000/9812 | Loss 0.6539 | Accuracy 58.30\n",
      "Epoch 6| Batch 5200/9812 | Loss 0.4754 | Accuracy 58.30\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.3446 | Accuracy 58.30\n",
      "Epoch 6| Batch 5600/9812 | Loss 0.6465 | Accuracy 58.30\n",
      "Epoch 6| Batch 5800/9812 | Loss 0.0249 | Accuracy 58.30\n",
      "Epoch 6| Batch 6000/9812 | Loss 1.5502 | Accuracy 58.30\n",
      "Epoch 6| Batch 6200/9812 | Loss 0.0101 | Accuracy 58.30\n",
      "Epoch 6| Batch 6400/9812 | Loss 1.2701 | Accuracy 58.30\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.3230 | Accuracy 58.30\n",
      "Epoch 6| Batch 6800/9812 | Loss 1.6189 | Accuracy 58.30\n",
      "Epoch 6| Batch 7000/9812 | Loss 0.4112 | Accuracy 58.30\n",
      "Epoch 6| Batch 7200/9812 | Loss 0.3883 | Accuracy 58.30\n",
      "Epoch 6| Batch 7400/9812 | Loss 1.1586 | Accuracy 58.30\n",
      "Epoch 6| Batch 7600/9812 | Loss 1.0382 | Accuracy 58.30\n",
      "Epoch 6| Batch 7800/9812 | Loss 0.2010 | Accuracy 58.30\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 0.1836 | Accuracy 62.77\n",
      "Epoch 7| Batch 200/9812 | Loss 0.0000 | Accuracy 62.77\n",
      "Epoch 7| Batch 400/9812 | Loss 3.2987 | Accuracy 62.77\n",
      "Epoch 7| Batch 600/9812 | Loss 3.1874 | Accuracy 62.77\n",
      "Epoch 7| Batch 800/9812 | Loss 2.7254 | Accuracy 62.77\n",
      "Epoch 7| Batch 1000/9812 | Loss 1.5164 | Accuracy 62.77\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.7151 | Accuracy 62.77\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.1780 | Accuracy 62.77\n",
      "Epoch 7| Batch 1600/9812 | Loss 1.4581 | Accuracy 62.77\n",
      "Epoch 7| Batch 1800/9812 | Loss 5.9375 | Accuracy 62.77\n",
      "Epoch 7| Batch 2000/9812 | Loss 1.6984 | Accuracy 62.77\n",
      "Epoch 7| Batch 2200/9812 | Loss 0.0101 | Accuracy 62.77\n",
      "Epoch 7| Batch 2400/9812 | Loss 0.1306 | Accuracy 62.77\n",
      "Epoch 7| Batch 2600/9812 | Loss 0.2578 | Accuracy 62.77\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.1573 | Accuracy 62.77\n",
      "Epoch 7| Batch 3000/9812 | Loss 0.0010 | Accuracy 62.77\n",
      "Epoch 7| Batch 3200/9812 | Loss 0.0108 | Accuracy 62.77\n",
      "Epoch 7| Batch 3400/9812 | Loss 1.4324 | Accuracy 62.77\n",
      "Epoch 7| Batch 3600/9812 | Loss 0.6147 | Accuracy 62.77\n",
      "Epoch 7| Batch 3800/9812 | Loss 2.1415 | Accuracy 62.77\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.0005 | Accuracy 62.77\n",
      "Epoch 7| Batch 4200/9812 | Loss 1.6704 | Accuracy 62.77\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.5357 | Accuracy 62.77\n",
      "Epoch 7| Batch 4600/9812 | Loss 0.0001 | Accuracy 62.77\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.1668 | Accuracy 62.77\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.0759 | Accuracy 62.77\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.6428 | Accuracy 62.77\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.3885 | Accuracy 62.77\n",
      "Epoch 7| Batch 5600/9812 | Loss 0.1738 | Accuracy 62.77\n",
      "Epoch 7| Batch 5800/9812 | Loss 1.3228 | Accuracy 62.77\n",
      "Epoch 7| Batch 6000/9812 | Loss 0.0020 | Accuracy 62.77\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.1304 | Accuracy 62.77\n",
      "Epoch 7| Batch 6400/9812 | Loss 1.8880 | Accuracy 62.77\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.1628 | Accuracy 62.77\n",
      "Epoch 7| Batch 6800/9812 | Loss 0.1333 | Accuracy 62.77\n",
      "Epoch 7| Batch 7000/9812 | Loss 0.1112 | Accuracy 62.77\n",
      "Epoch 7| Batch 7200/9812 | Loss 0.8647 | Accuracy 62.77\n",
      "Epoch 7| Batch 7400/9812 | Loss 1.0577 | Accuracy 62.77\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.0107 | Accuracy 62.77\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.7388 | Accuracy 62.77\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 1.0696 | Accuracy 66.38\n",
      "Epoch 8| Batch 200/9812 | Loss 0.7692 | Accuracy 66.38\n",
      "Epoch 8| Batch 400/9812 | Loss 2.1106 | Accuracy 66.38\n",
      "Epoch 8| Batch 600/9812 | Loss 0.2704 | Accuracy 66.38\n",
      "Epoch 8| Batch 800/9812 | Loss 0.7582 | Accuracy 66.38\n",
      "Epoch 8| Batch 1000/9812 | Loss 2.2125 | Accuracy 66.38\n",
      "Epoch 8| Batch 1200/9812 | Loss 3.5861 | Accuracy 66.38\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.0913 | Accuracy 66.38\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.0116 | Accuracy 66.38\n",
      "Epoch 8| Batch 1800/9812 | Loss 0.0807 | Accuracy 66.38\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.0001 | Accuracy 66.38\n",
      "Epoch 8| Batch 2200/9812 | Loss 3.1772 | Accuracy 66.38\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0306 | Accuracy 66.38\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.0002 | Accuracy 66.38\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.3323 | Accuracy 66.38\n",
      "Epoch 8| Batch 3000/9812 | Loss 0.0323 | Accuracy 66.38\n",
      "Epoch 8| Batch 3200/9812 | Loss 1.4220 | Accuracy 66.38\n",
      "Epoch 8| Batch 3400/9812 | Loss 1.9829 | Accuracy 66.38\n",
      "Epoch 8| Batch 3600/9812 | Loss 0.0209 | Accuracy 66.38\n",
      "Epoch 8| Batch 3800/9812 | Loss 1.3989 | Accuracy 66.38\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.0318 | Accuracy 66.38\n",
      "Epoch 8| Batch 4200/9812 | Loss 1.4143 | Accuracy 66.38\n",
      "Epoch 8| Batch 4400/9812 | Loss 0.0743 | Accuracy 66.38\n",
      "Epoch 8| Batch 4600/9812 | Loss 0.0274 | Accuracy 66.38\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.1393 | Accuracy 66.38\n",
      "Epoch 8| Batch 5000/9812 | Loss 1.4082 | Accuracy 66.38\n",
      "Epoch 8| Batch 5200/9812 | Loss 0.3235 | Accuracy 66.38\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.3754 | Accuracy 66.38\n",
      "Epoch 8| Batch 5600/9812 | Loss 0.1306 | Accuracy 66.38\n",
      "Epoch 8| Batch 5800/9812 | Loss 0.3401 | Accuracy 66.38\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.1001 | Accuracy 66.38\n",
      "Epoch 8| Batch 6200/9812 | Loss 1.4661 | Accuracy 66.38\n",
      "Epoch 8| Batch 6400/9812 | Loss 6.6102 | Accuracy 66.38\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.1054 | Accuracy 66.38\n",
      "Epoch 8| Batch 6800/9812 | Loss 0.0003 | Accuracy 66.38\n",
      "Epoch 8| Batch 7000/9812 | Loss 2.7033 | Accuracy 66.38\n",
      "Epoch 8| Batch 7200/9812 | Loss 0.0095 | Accuracy 66.38\n",
      "Epoch 8| Batch 7400/9812 | Loss 3.4634 | Accuracy 66.38\n",
      "Epoch 8| Batch 7600/9812 | Loss 3.9998 | Accuracy 66.38\n",
      "Epoch 8| Batch 7800/9812 | Loss 3.6280 | Accuracy 66.38\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 3.1311 | Accuracy 71.19\n",
      "Epoch 9| Batch 200/9812 | Loss 0.0142 | Accuracy 71.19\n",
      "Epoch 9| Batch 400/9812 | Loss 0.5446 | Accuracy 71.19\n",
      "Epoch 9| Batch 600/9812 | Loss 0.0004 | Accuracy 71.19\n",
      "Epoch 9| Batch 800/9812 | Loss 2.2168 | Accuracy 71.19\n",
      "Epoch 9| Batch 1000/9812 | Loss 2.2607 | Accuracy 71.19\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.5813 | Accuracy 71.19\n",
      "Epoch 9| Batch 1400/9812 | Loss 1.1676 | Accuracy 71.19\n",
      "Epoch 9| Batch 1600/9812 | Loss 0.1604 | Accuracy 71.19\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.5128 | Accuracy 71.19\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.4555 | Accuracy 71.19\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.0000 | Accuracy 71.19\n",
      "Epoch 9| Batch 2400/9812 | Loss 1.4687 | Accuracy 71.19\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.0004 | Accuracy 71.19\n",
      "Epoch 9| Batch 2800/9812 | Loss 0.1799 | Accuracy 71.19\n",
      "Epoch 9| Batch 3000/9812 | Loss 5.6643 | Accuracy 71.19\n",
      "Epoch 9| Batch 3200/9812 | Loss 2.4245 | Accuracy 71.19\n",
      "Epoch 9| Batch 3400/9812 | Loss 2.1286 | Accuracy 71.19\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0795 | Accuracy 71.19\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.6003 | Accuracy 71.19\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.4799 | Accuracy 71.19\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.5687 | Accuracy 71.19\n",
      "Epoch 9| Batch 4400/9812 | Loss 0.2251 | Accuracy 71.19\n",
      "Epoch 9| Batch 4600/9812 | Loss 0.0747 | Accuracy 71.19\n",
      "Epoch 9| Batch 4800/9812 | Loss 0.1296 | Accuracy 71.19\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.0000 | Accuracy 71.19\n",
      "Epoch 9| Batch 5200/9812 | Loss 0.0068 | Accuracy 71.19\n",
      "Epoch 9| Batch 5400/9812 | Loss 1.9732 | Accuracy 71.19\n",
      "Epoch 9| Batch 5600/9812 | Loss 0.4353 | Accuracy 71.19\n",
      "Epoch 9| Batch 5800/9812 | Loss 0.0935 | Accuracy 71.19\n",
      "Epoch 9| Batch 6000/9812 | Loss 1.8881 | Accuracy 71.19\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0573 | Accuracy 71.19\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.0000 | Accuracy 71.19\n",
      "Epoch 9| Batch 6600/9812 | Loss 1.4690 | Accuracy 71.19\n",
      "Epoch 9| Batch 6800/9812 | Loss 1.2210 | Accuracy 71.19\n",
      "Epoch 9| Batch 7000/9812 | Loss 0.0065 | Accuracy 71.19\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.0000 | Accuracy 71.19\n",
      "Epoch 9| Batch 7400/9812 | Loss 2.9468 | Accuracy 71.19\n",
      "Epoch 9| Batch 7600/9812 | Loss 0.0655 | Accuracy 71.19\n",
      "Epoch 9| Batch 7800/9812 | Loss 0.2487 | Accuracy 71.19\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.0430 | Accuracy 72.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## DNN ##\n",
    "h_in, h_out =  30*512, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 512, 256, 128, 128, 100\n",
    "DNN = UCF_DNN(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "DNN_criterion = nn.CrossEntropyLoss()\n",
    "DNN_optimizer = optim.Adam(DNN.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "## Training the DNN ##\n",
    "DNN.train()\n",
    "print(\"\\n\\n\\{}\\n\".format(DNN.__class__.__name__ ))\n",
    "epochs=10\n",
    "DNN.train()\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        DNN_optimizer.zero_grad()\n",
    "\n",
    "        y_pred = DNN(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = DNN_criterion(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        DNN_optimizer.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 75.96%\n"
     ]
    }
   ],
   "source": [
    "DNN.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = DNN(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_CNN1D\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.8335 | Accuracy 72.94\n",
      "Epoch 0| Batch 400/9812 | Loss 4.4256 | Accuracy 72.94\n",
      "Epoch 0| Batch 600/9812 | Loss 3.7638 | Accuracy 72.94\n",
      "Epoch 0| Batch 800/9812 | Loss 3.8550 | Accuracy 72.94\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.0580 | Accuracy 72.94\n",
      "Epoch 0| Batch 1200/9812 | Loss 3.3602 | Accuracy 72.94\n",
      "Epoch 0| Batch 1400/9812 | Loss 2.7609 | Accuracy 72.94\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.5629 | Accuracy 72.94\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.8818 | Accuracy 72.94\n",
      "Epoch 0| Batch 2000/9812 | Loss 0.0885 | Accuracy 72.94\n",
      "Epoch 0| Batch 2200/9812 | Loss 4.6886 | Accuracy 72.94\n",
      "Epoch 0| Batch 2400/9812 | Loss 0.8624 | Accuracy 72.94\n",
      "Epoch 0| Batch 2600/9812 | Loss 1.8176 | Accuracy 72.94\n",
      "Epoch 0| Batch 2800/9812 | Loss 2.5114 | Accuracy 72.94\n",
      "Epoch 0| Batch 3000/9812 | Loss 6.3551 | Accuracy 72.94\n",
      "Epoch 0| Batch 3200/9812 | Loss 0.7870 | Accuracy 72.94\n",
      "Epoch 0| Batch 3400/9812 | Loss 0.0000 | Accuracy 72.94\n",
      "Epoch 0| Batch 3600/9812 | Loss 2.5441 | Accuracy 72.94\n",
      "Epoch 0| Batch 3800/9812 | Loss 0.0035 | Accuracy 72.94\n",
      "Epoch 0| Batch 4000/9812 | Loss 4.9859 | Accuracy 72.94\n",
      "Epoch 0| Batch 4200/9812 | Loss 2.7915 | Accuracy 72.94\n",
      "Epoch 0| Batch 4400/9812 | Loss 0.0340 | Accuracy 72.94\n",
      "Epoch 0| Batch 4600/9812 | Loss 0.0288 | Accuracy 72.94\n",
      "Epoch 0| Batch 4800/9812 | Loss 0.2533 | Accuracy 72.94\n",
      "Epoch 0| Batch 5000/9812 | Loss 1.5694 | Accuracy 72.94\n",
      "Epoch 0| Batch 5200/9812 | Loss 0.0085 | Accuracy 72.94\n",
      "Epoch 0| Batch 5400/9812 | Loss 0.1525 | Accuracy 72.94\n",
      "Epoch 0| Batch 5600/9812 | Loss 0.0362 | Accuracy 72.94\n",
      "Epoch 0| Batch 5800/9812 | Loss 2.4957 | Accuracy 72.94\n",
      "Epoch 0| Batch 6000/9812 | Loss 0.2054 | Accuracy 72.94\n",
      "Epoch 0| Batch 6200/9812 | Loss 0.0109 | Accuracy 72.94\n",
      "Epoch 0| Batch 6400/9812 | Loss 0.3258 | Accuracy 72.94\n",
      "Epoch 0| Batch 6600/9812 | Loss 0.7947 | Accuracy 72.94\n",
      "Epoch 0| Batch 6800/9812 | Loss 0.0592 | Accuracy 72.94\n",
      "Epoch 0| Batch 7000/9812 | Loss 0.0078 | Accuracy 72.94\n",
      "Epoch 0| Batch 7200/9812 | Loss 0.1503 | Accuracy 72.94\n",
      "Epoch 0| Batch 7400/9812 | Loss 2.4623 | Accuracy 72.94\n",
      "Epoch 0| Batch 7600/9812 | Loss 0.0000 | Accuracy 72.94\n",
      "Epoch 0| Batch 7800/9812 | Loss 0.0218 | Accuracy 72.94\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 2.8573 | Accuracy 474.84\n",
      "Epoch 1| Batch 200/9812 | Loss 1.7294 | Accuracy 474.84\n",
      "Epoch 1| Batch 400/9812 | Loss 0.1547 | Accuracy 474.84\n",
      "Epoch 1| Batch 600/9812 | Loss 0.0135 | Accuracy 474.84\n",
      "Epoch 1| Batch 800/9812 | Loss 0.3563 | Accuracy 474.84\n",
      "Epoch 1| Batch 1000/9812 | Loss 1.1880 | Accuracy 474.84\n",
      "Epoch 1| Batch 1200/9812 | Loss 0.0016 | Accuracy 474.84\n",
      "Epoch 1| Batch 1400/9812 | Loss 0.0002 | Accuracy 474.84\n",
      "Epoch 1| Batch 1600/9812 | Loss 0.0972 | Accuracy 474.84\n",
      "Epoch 1| Batch 1800/9812 | Loss 1.0644 | Accuracy 474.84\n",
      "Epoch 1| Batch 2000/9812 | Loss 4.1966 | Accuracy 474.84\n",
      "Epoch 1| Batch 2200/9812 | Loss 0.0028 | Accuracy 474.84\n",
      "Epoch 1| Batch 2400/9812 | Loss 0.0029 | Accuracy 474.84\n",
      "Epoch 1| Batch 2600/9812 | Loss 1.0486 | Accuracy 474.84\n",
      "Epoch 1| Batch 2800/9812 | Loss 0.0156 | Accuracy 474.84\n",
      "Epoch 1| Batch 3000/9812 | Loss 4.6932 | Accuracy 474.84\n",
      "Epoch 1| Batch 3200/9812 | Loss 0.3088 | Accuracy 474.84\n",
      "Epoch 1| Batch 3400/9812 | Loss 0.1547 | Accuracy 474.84\n",
      "Epoch 1| Batch 3600/9812 | Loss 1.1913 | Accuracy 474.84\n",
      "Epoch 1| Batch 3800/9812 | Loss 0.0002 | Accuracy 474.84\n",
      "Epoch 1| Batch 4000/9812 | Loss 0.3724 | Accuracy 474.84\n",
      "Epoch 1| Batch 4200/9812 | Loss 0.6484 | Accuracy 474.84\n",
      "Epoch 1| Batch 4400/9812 | Loss 0.0018 | Accuracy 474.84\n",
      "Epoch 1| Batch 4600/9812 | Loss 0.1336 | Accuracy 474.84\n",
      "Epoch 1| Batch 4800/9812 | Loss 0.0000 | Accuracy 474.84\n",
      "Epoch 1| Batch 5000/9812 | Loss 0.9892 | Accuracy 474.84\n",
      "Epoch 1| Batch 5200/9812 | Loss 0.0502 | Accuracy 474.84\n",
      "Epoch 1| Batch 5400/9812 | Loss 0.1157 | Accuracy 474.84\n",
      "Epoch 1| Batch 5600/9812 | Loss 0.8454 | Accuracy 474.84\n",
      "Epoch 1| Batch 5800/9812 | Loss 3.6553 | Accuracy 474.84\n",
      "Epoch 1| Batch 6000/9812 | Loss 0.3106 | Accuracy 474.84\n",
      "Epoch 1| Batch 6200/9812 | Loss 0.0683 | Accuracy 474.84\n",
      "Epoch 1| Batch 6400/9812 | Loss 0.0855 | Accuracy 474.84\n",
      "Epoch 1| Batch 6600/9812 | Loss 0.0000 | Accuracy 474.84\n",
      "Epoch 1| Batch 6800/9812 | Loss 0.0001 | Accuracy 474.84\n",
      "Epoch 1| Batch 7000/9812 | Loss 0.0007 | Accuracy 474.84\n",
      "Epoch 1| Batch 7200/9812 | Loss 0.0392 | Accuracy 474.84\n",
      "Epoch 1| Batch 7400/9812 | Loss 0.1066 | Accuracy 474.84\n",
      "Epoch 1| Batch 7600/9812 | Loss 0.0641 | Accuracy 474.84\n",
      "Epoch 1| Batch 7800/9812 | Loss 0.0091 | Accuracy 474.84\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 200/9812 | Loss 0.0002 | Accuracy 807.24\n",
      "Epoch 2| Batch 400/9812 | Loss 0.0626 | Accuracy 807.24\n",
      "Epoch 2| Batch 600/9812 | Loss 0.0036 | Accuracy 807.24\n",
      "Epoch 2| Batch 800/9812 | Loss 0.0026 | Accuracy 807.24\n",
      "Epoch 2| Batch 1000/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 1200/9812 | Loss 0.6858 | Accuracy 807.24\n",
      "Epoch 2| Batch 1400/9812 | Loss 0.1082 | Accuracy 807.24\n",
      "Epoch 2| Batch 1600/9812 | Loss 0.0319 | Accuracy 807.24\n",
      "Epoch 2| Batch 1800/9812 | Loss 0.0008 | Accuracy 807.24\n",
      "Epoch 2| Batch 2000/9812 | Loss 0.4085 | Accuracy 807.24\n",
      "Epoch 2| Batch 2200/9812 | Loss 0.0134 | Accuracy 807.24\n",
      "Epoch 2| Batch 2400/9812 | Loss 0.0001 | Accuracy 807.24\n",
      "Epoch 2| Batch 2600/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 2800/9812 | Loss 0.0073 | Accuracy 807.24\n",
      "Epoch 2| Batch 3000/9812 | Loss 0.0008 | Accuracy 807.24\n",
      "Epoch 2| Batch 3200/9812 | Loss 0.0006 | Accuracy 807.24\n",
      "Epoch 2| Batch 3400/9812 | Loss 0.3515 | Accuracy 807.24\n",
      "Epoch 2| Batch 3600/9812 | Loss 0.1487 | Accuracy 807.24\n",
      "Epoch 2| Batch 3800/9812 | Loss 0.0165 | Accuracy 807.24\n",
      "Epoch 2| Batch 4000/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 4200/9812 | Loss 0.0001 | Accuracy 807.24\n",
      "Epoch 2| Batch 4400/9812 | Loss 0.0003 | Accuracy 807.24\n",
      "Epoch 2| Batch 4600/9812 | Loss 0.3091 | Accuracy 807.24\n",
      "Epoch 2| Batch 4800/9812 | Loss 0.3696 | Accuracy 807.24\n",
      "Epoch 2| Batch 5000/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 5200/9812 | Loss 0.0004 | Accuracy 807.24\n",
      "Epoch 2| Batch 5400/9812 | Loss 2.7990 | Accuracy 807.24\n",
      "Epoch 2| Batch 5600/9812 | Loss 0.0001 | Accuracy 807.24\n",
      "Epoch 2| Batch 5800/9812 | Loss 0.1244 | Accuracy 807.24\n",
      "Epoch 2| Batch 6000/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 6200/9812 | Loss 0.1946 | Accuracy 807.24\n",
      "Epoch 2| Batch 6400/9812 | Loss 0.0080 | Accuracy 807.24\n",
      "Epoch 2| Batch 6600/9812 | Loss 0.0021 | Accuracy 807.24\n",
      "Epoch 2| Batch 6800/9812 | Loss 0.0006 | Accuracy 807.24\n",
      "Epoch 2| Batch 7000/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "Epoch 2| Batch 7200/9812 | Loss 0.0098 | Accuracy 807.24\n",
      "Epoch 2| Batch 7400/9812 | Loss 1.3289 | Accuracy 807.24\n",
      "Epoch 2| Batch 7600/9812 | Loss 0.0001 | Accuracy 807.24\n",
      "Epoch 2| Batch 7800/9812 | Loss 0.0000 | Accuracy 807.24\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 0.0002 | Accuracy 904.83\n",
      "Epoch 3| Batch 200/9812 | Loss 0.0320 | Accuracy 904.83\n",
      "Epoch 3| Batch 400/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 600/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 800/9812 | Loss 0.0006 | Accuracy 904.83\n",
      "Epoch 3| Batch 1000/9812 | Loss 0.0022 | Accuracy 904.83\n",
      "Epoch 3| Batch 1200/9812 | Loss 0.0007 | Accuracy 904.83\n",
      "Epoch 3| Batch 1400/9812 | Loss 0.0026 | Accuracy 904.83\n",
      "Epoch 3| Batch 1600/9812 | Loss 0.0042 | Accuracy 904.83\n",
      "Epoch 3| Batch 1800/9812 | Loss 2.8283 | Accuracy 904.83\n",
      "Epoch 3| Batch 2000/9812 | Loss 0.0065 | Accuracy 904.83\n",
      "Epoch 3| Batch 2200/9812 | Loss 0.0329 | Accuracy 904.83\n",
      "Epoch 3| Batch 2400/9812 | Loss 0.0005 | Accuracy 904.83\n",
      "Epoch 3| Batch 2600/9812 | Loss 0.0436 | Accuracy 904.83\n",
      "Epoch 3| Batch 2800/9812 | Loss 0.0001 | Accuracy 904.83\n",
      "Epoch 3| Batch 3000/9812 | Loss 0.0001 | Accuracy 904.83\n",
      "Epoch 3| Batch 3200/9812 | Loss 0.0022 | Accuracy 904.83\n",
      "Epoch 3| Batch 3400/9812 | Loss 0.0035 | Accuracy 904.83\n",
      "Epoch 3| Batch 3600/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 3800/9812 | Loss 0.0052 | Accuracy 904.83\n",
      "Epoch 3| Batch 4000/9812 | Loss 0.1083 | Accuracy 904.83\n",
      "Epoch 3| Batch 4200/9812 | Loss 0.0039 | Accuracy 904.83\n",
      "Epoch 3| Batch 4400/9812 | Loss 0.0329 | Accuracy 904.83\n",
      "Epoch 3| Batch 4600/9812 | Loss 0.0066 | Accuracy 904.83\n",
      "Epoch 3| Batch 4800/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 5000/9812 | Loss 0.1307 | Accuracy 904.83\n",
      "Epoch 3| Batch 5200/9812 | Loss 0.0044 | Accuracy 904.83\n",
      "Epoch 3| Batch 5400/9812 | Loss 0.0001 | Accuracy 904.83\n",
      "Epoch 3| Batch 5600/9812 | Loss 0.0034 | Accuracy 904.83\n",
      "Epoch 3| Batch 5800/9812 | Loss 0.0006 | Accuracy 904.83\n",
      "Epoch 3| Batch 6000/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 6200/9812 | Loss 0.9807 | Accuracy 904.83\n",
      "Epoch 3| Batch 6400/9812 | Loss 0.0003 | Accuracy 904.83\n",
      "Epoch 3| Batch 6600/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 6800/9812 | Loss 2.6070 | Accuracy 904.83\n",
      "Epoch 3| Batch 7000/9812 | Loss 0.0028 | Accuracy 904.83\n",
      "Epoch 3| Batch 7200/9812 | Loss 0.0000 | Accuracy 904.83\n",
      "Epoch 3| Batch 7400/9812 | Loss 0.4088 | Accuracy 904.83\n",
      "Epoch 3| Batch 7600/9812 | Loss 0.0237 | Accuracy 904.83\n",
      "Epoch 3| Batch 7800/9812 | Loss 0.0027 | Accuracy 904.83\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 200/9812 | Loss 1.4881 | Accuracy 942.80\n",
      "Epoch 4| Batch 400/9812 | Loss 0.0003 | Accuracy 942.80\n",
      "Epoch 4| Batch 600/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 800/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 1000/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 1200/9812 | Loss 0.0458 | Accuracy 942.80\n",
      "Epoch 4| Batch 1400/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 1600/9812 | Loss 0.0011 | Accuracy 942.80\n",
      "Epoch 4| Batch 1800/9812 | Loss 0.0952 | Accuracy 942.80\n",
      "Epoch 4| Batch 2000/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 2200/9812 | Loss 0.0001 | Accuracy 942.80\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.0012 | Accuracy 942.80\n",
      "Epoch 4| Batch 2600/9812 | Loss 0.0044 | Accuracy 942.80\n",
      "Epoch 4| Batch 2800/9812 | Loss 0.0029 | Accuracy 942.80\n",
      "Epoch 4| Batch 3000/9812 | Loss 0.2552 | Accuracy 942.80\n",
      "Epoch 4| Batch 3200/9812 | Loss 0.0023 | Accuracy 942.80\n",
      "Epoch 4| Batch 3400/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 3600/9812 | Loss 0.0287 | Accuracy 942.80\n",
      "Epoch 4| Batch 3800/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 4000/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 4200/9812 | Loss 0.5074 | Accuracy 942.80\n",
      "Epoch 4| Batch 4400/9812 | Loss 0.0001 | Accuracy 942.80\n",
      "Epoch 4| Batch 4600/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 4800/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 5000/9812 | Loss 0.2774 | Accuracy 942.80\n",
      "Epoch 4| Batch 5200/9812 | Loss 0.0474 | Accuracy 942.80\n",
      "Epoch 4| Batch 5400/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.0030 | Accuracy 942.80\n",
      "Epoch 4| Batch 5800/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.0512 | Accuracy 942.80\n",
      "Epoch 4| Batch 6200/9812 | Loss 0.0034 | Accuracy 942.80\n",
      "Epoch 4| Batch 6400/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 6600/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 6800/9812 | Loss 0.0050 | Accuracy 942.80\n",
      "Epoch 4| Batch 7000/9812 | Loss 0.0001 | Accuracy 942.80\n",
      "Epoch 4| Batch 7200/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 7400/9812 | Loss 0.0033 | Accuracy 942.80\n",
      "Epoch 4| Batch 7600/9812 | Loss 0.0000 | Accuracy 942.80\n",
      "Epoch 4| Batch 7800/9812 | Loss 0.0006 | Accuracy 942.80\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 200/9812 | Loss 0.0007 | Accuracy 960.50\n",
      "Epoch 5| Batch 400/9812 | Loss 0.0001 | Accuracy 960.50\n",
      "Epoch 5| Batch 600/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 800/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 1000/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 1200/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 1400/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 1600/9812 | Loss 0.0002 | Accuracy 960.50\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.0180 | Accuracy 960.50\n",
      "Epoch 5| Batch 2000/9812 | Loss 0.0101 | Accuracy 960.50\n",
      "Epoch 5| Batch 2200/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 2400/9812 | Loss 0.0006 | Accuracy 960.50\n",
      "Epoch 5| Batch 2600/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 2800/9812 | Loss 4.2251 | Accuracy 960.50\n",
      "Epoch 5| Batch 3000/9812 | Loss 0.7834 | Accuracy 960.50\n",
      "Epoch 5| Batch 3200/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 3800/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 4000/9812 | Loss 0.0001 | Accuracy 960.50\n",
      "Epoch 5| Batch 4200/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 4400/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 4600/9812 | Loss 0.0009 | Accuracy 960.50\n",
      "Epoch 5| Batch 4800/9812 | Loss 0.0117 | Accuracy 960.50\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.0001 | Accuracy 960.50\n",
      "Epoch 5| Batch 5200/9812 | Loss 0.0001 | Accuracy 960.50\n",
      "Epoch 5| Batch 5400/9812 | Loss 0.0191 | Accuracy 960.50\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 5800/9812 | Loss 0.0037 | Accuracy 960.50\n",
      "Epoch 5| Batch 6000/9812 | Loss 0.4953 | Accuracy 960.50\n",
      "Epoch 5| Batch 6200/9812 | Loss 0.1261 | Accuracy 960.50\n",
      "Epoch 5| Batch 6400/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 6600/9812 | Loss 0.0001 | Accuracy 960.50\n",
      "Epoch 5| Batch 6800/9812 | Loss 0.0000 | Accuracy 960.50\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.0199 | Accuracy 960.50\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.0018 | Accuracy 960.50\n",
      "Epoch 5| Batch 7400/9812 | Loss 0.0103 | Accuracy 960.50\n",
      "Epoch 5| Batch 7600/9812 | Loss 0.0041 | Accuracy 960.50\n",
      "Epoch 5| Batch 7800/9812 | Loss 0.0004 | Accuracy 960.50\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 200/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 400/9812 | Loss 0.0027 | Accuracy 975.54\n",
      "Epoch 6| Batch 600/9812 | Loss 0.0029 | Accuracy 975.54\n",
      "Epoch 6| Batch 800/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 1000/9812 | Loss 0.0018 | Accuracy 975.54\n",
      "Epoch 6| Batch 1200/9812 | Loss 2.3110 | Accuracy 975.54\n",
      "Epoch 6| Batch 1400/9812 | Loss 0.0002 | Accuracy 975.54\n",
      "Epoch 6| Batch 1600/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 1800/9812 | Loss 0.0102 | Accuracy 975.54\n",
      "Epoch 6| Batch 2000/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 2200/9812 | Loss 0.0146 | Accuracy 975.54\n",
      "Epoch 6| Batch 2400/9812 | Loss 0.0001 | Accuracy 975.54\n",
      "Epoch 6| Batch 2600/9812 | Loss 0.0022 | Accuracy 975.54\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.0003 | Accuracy 975.54\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 3200/9812 | Loss 0.0002 | Accuracy 975.54\n",
      "Epoch 6| Batch 3400/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 3600/9812 | Loss 0.0032 | Accuracy 975.54\n",
      "Epoch 6| Batch 3800/9812 | Loss 0.0004 | Accuracy 975.54\n",
      "Epoch 6| Batch 4000/9812 | Loss 0.0001 | Accuracy 975.54\n",
      "Epoch 6| Batch 4200/9812 | Loss 0.0010 | Accuracy 975.54\n",
      "Epoch 6| Batch 4400/9812 | Loss 0.0002 | Accuracy 975.54\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.0794 | Accuracy 975.54\n",
      "Epoch 6| Batch 4800/9812 | Loss 0.0001 | Accuracy 975.54\n",
      "Epoch 6| Batch 5000/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 5200/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.0265 | Accuracy 975.54\n",
      "Epoch 6| Batch 5600/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 5800/9812 | Loss 0.0305 | Accuracy 975.54\n",
      "Epoch 6| Batch 6000/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 6200/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 6400/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 6800/9812 | Loss 0.0001 | Accuracy 975.54\n",
      "Epoch 6| Batch 7000/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 7200/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 7600/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "Epoch 6| Batch 7800/9812 | Loss 0.0000 | Accuracy 975.54\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 0.0002 | Accuracy 980.25\n",
      "Epoch 7| Batch 200/9812 | Loss 0.0003 | Accuracy 980.25\n",
      "Epoch 7| Batch 400/9812 | Loss 0.0006 | Accuracy 980.25\n",
      "Epoch 7| Batch 600/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 800/9812 | Loss 0.0008 | Accuracy 980.25\n",
      "Epoch 7| Batch 1000/9812 | Loss 0.0003 | Accuracy 980.25\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.0025 | Accuracy 980.25\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.0001 | Accuracy 980.25\n",
      "Epoch 7| Batch 1800/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0188 | Accuracy 980.25\n",
      "Epoch 7| Batch 2200/9812 | Loss 0.0005 | Accuracy 980.25\n",
      "Epoch 7| Batch 2400/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 2600/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.0002 | Accuracy 980.25\n",
      "Epoch 7| Batch 3000/9812 | Loss 0.0389 | Accuracy 980.25\n",
      "Epoch 7| Batch 3200/9812 | Loss 1.5861 | Accuracy 980.25\n",
      "Epoch 7| Batch 3400/9812 | Loss 0.0013 | Accuracy 980.25\n",
      "Epoch 7| Batch 3600/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 3800/9812 | Loss 0.0016 | Accuracy 980.25\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.0007 | Accuracy 980.25\n",
      "Epoch 7| Batch 4200/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.0002 | Accuracy 980.25\n",
      "Epoch 7| Batch 4600/9812 | Loss 0.0029 | Accuracy 980.25\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.1818 | Accuracy 980.25\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 5600/9812 | Loss 0.1017 | Accuracy 980.25\n",
      "Epoch 7| Batch 5800/9812 | Loss 0.0011 | Accuracy 980.25\n",
      "Epoch 7| Batch 6000/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 6400/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.0001 | Accuracy 980.25\n",
      "Epoch 7| Batch 6800/9812 | Loss 0.0005 | Accuracy 980.25\n",
      "Epoch 7| Batch 7000/9812 | Loss 0.0001 | Accuracy 980.25\n",
      "Epoch 7| Batch 7200/9812 | Loss 0.0000 | Accuracy 980.25\n",
      "Epoch 7| Batch 7400/9812 | Loss 0.0211 | Accuracy 980.25\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.0006 | Accuracy 980.25\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.0002 | Accuracy 980.25\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 0.0008 | Accuracy 981.14\n",
      "Epoch 8| Batch 200/9812 | Loss 0.0002 | Accuracy 981.14\n",
      "Epoch 8| Batch 400/9812 | Loss 0.0003 | Accuracy 981.14\n",
      "Epoch 8| Batch 600/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 800/9812 | Loss 0.0004 | Accuracy 981.14\n",
      "Epoch 8| Batch 1000/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 1200/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.0227 | Accuracy 981.14\n",
      "Epoch 8| Batch 1800/9812 | Loss 0.0002 | Accuracy 981.14\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 2200/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0002 | Accuracy 981.14\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 3000/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 3200/9812 | Loss 0.0038 | Accuracy 981.14\n",
      "Epoch 8| Batch 3400/9812 | Loss 0.0017 | Accuracy 981.14\n",
      "Epoch 8| Batch 3600/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 3800/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.0001 | Accuracy 981.14\n",
      "Epoch 8| Batch 4200/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 4400/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 4600/9812 | Loss 0.0048 | Accuracy 981.14\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.0001 | Accuracy 981.14\n",
      "Epoch 8| Batch 5000/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 5200/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 5600/9812 | Loss 0.0166 | Accuracy 981.14\n",
      "Epoch 8| Batch 5800/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 6200/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 6400/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 6800/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 7000/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 7200/9812 | Loss 0.0000 | Accuracy 981.14\n",
      "Epoch 8| Batch 7400/9812 | Loss 0.0234 | Accuracy 981.14\n",
      "Epoch 8| Batch 7600/9812 | Loss 0.0373 | Accuracy 981.14\n",
      "Epoch 8| Batch 7800/9812 | Loss 0.0002 | Accuracy 981.14\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 0.0022 | Accuracy 986.37\n",
      "Epoch 9| Batch 200/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 400/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 600/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 800/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 1000/9812 | Loss 0.0081 | Accuracy 986.37\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 1400/9812 | Loss 0.0119 | Accuracy 986.37\n",
      "Epoch 9| Batch 1600/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.0001 | Accuracy 986.37\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 2400/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.0778 | Accuracy 986.37\n",
      "Epoch 9| Batch 2800/9812 | Loss 0.0041 | Accuracy 986.37\n",
      "Epoch 9| Batch 3000/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 3200/9812 | Loss 0.0276 | Accuracy 986.37\n",
      "Epoch 9| Batch 3400/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.0026 | Accuracy 986.37\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.0005 | Accuracy 986.37\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 4400/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 4600/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 4800/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 5200/9812 | Loss 0.0013 | Accuracy 986.37\n",
      "Epoch 9| Batch 5400/9812 | Loss 0.0001 | Accuracy 986.37\n",
      "Epoch 9| Batch 5600/9812 | Loss 0.0879 | Accuracy 986.37\n",
      "Epoch 9| Batch 5800/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.0018 | Accuracy 986.37\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.0002 | Accuracy 986.37\n",
      "Epoch 9| Batch 6600/9812 | Loss 0.0208 | Accuracy 986.37\n",
      "Epoch 9| Batch 6800/9812 | Loss 0.0007 | Accuracy 986.37\n",
      "Epoch 9| Batch 7000/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "Epoch 9| Batch 7400/9812 | Loss 0.0048 | Accuracy 986.37\n",
      "Epoch 9| Batch 7600/9812 | Loss 0.0002 | Accuracy 986.37\n",
      "Epoch 9| Batch 7800/9812 | Loss 0.0000 | Accuracy 986.37\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.0000 | Accuracy 985.35\n"
     ]
    }
   ],
   "source": [
    "## UCF_CNN1D ##\n",
    "h_in, h_out = 30, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 256, 128, 64, 1000, 500\n",
    "CNN_1D = UCF_CNN1D(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion_1D = nn.CrossEntropyLoss()\n",
    "optimizer_1D = optim.Adam(CNN_1D.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "print(\"\\n\\n\\{}\\n\".format(CNN_1D.__class__.__name__ ))\n",
    "\n",
    "## Training the DNN ##\n",
    "CNN_1D.train()\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        optimizer_1D.zero_grad()\n",
    "\n",
    "        y_pred = CNN_1D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = criterion_1D(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer_1D.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), train_acc))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 86.75%\n"
     ]
    }
   ],
   "source": [
    "CNN_1D.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = CNN_1D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_CNN2D\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.6533 | Accuracy 985.35\n",
      "Epoch 0| Batch 400/9812 | Loss 4.3223 | Accuracy 985.35\n",
      "Epoch 0| Batch 600/9812 | Loss 4.6003 | Accuracy 985.35\n",
      "Epoch 0| Batch 800/9812 | Loss 4.5542 | Accuracy 985.35\n",
      "Epoch 0| Batch 1000/9812 | Loss 3.8608 | Accuracy 985.35\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.3825 | Accuracy 985.35\n",
      "Epoch 0| Batch 1400/9812 | Loss 5.8491 | Accuracy 985.35\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.4751 | Accuracy 985.35\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.2232 | Accuracy 985.35\n",
      "Epoch 0| Batch 2000/9812 | Loss 4.4989 | Accuracy 985.35\n",
      "Epoch 0| Batch 2200/9812 | Loss 4.5987 | Accuracy 985.35\n",
      "Epoch 0| Batch 2400/9812 | Loss 4.6552 | Accuracy 985.35\n",
      "Epoch 0| Batch 2600/9812 | Loss 5.0445 | Accuracy 985.35\n",
      "Epoch 0| Batch 2800/9812 | Loss 4.6829 | Accuracy 985.35\n",
      "Epoch 0| Batch 3000/9812 | Loss 4.3347 | Accuracy 985.35\n",
      "Epoch 0| Batch 3200/9812 | Loss 4.6692 | Accuracy 985.35\n",
      "Epoch 0| Batch 3400/9812 | Loss 4.3913 | Accuracy 985.35\n",
      "Epoch 0| Batch 3600/9812 | Loss 4.4610 | Accuracy 985.35\n",
      "Epoch 0| Batch 3800/9812 | Loss 1.5220 | Accuracy 985.35\n",
      "Epoch 0| Batch 4000/9812 | Loss 4.9432 | Accuracy 985.35\n",
      "Epoch 0| Batch 4200/9812 | Loss 4.0942 | Accuracy 985.35\n",
      "Epoch 0| Batch 4400/9812 | Loss 3.0808 | Accuracy 985.35\n",
      "Epoch 0| Batch 4600/9812 | Loss 4.6647 | Accuracy 985.35\n",
      "Epoch 0| Batch 4800/9812 | Loss 4.2719 | Accuracy 985.35\n",
      "Epoch 0| Batch 5000/9812 | Loss 3.9018 | Accuracy 985.35\n",
      "Epoch 0| Batch 5200/9812 | Loss 3.9956 | Accuracy 985.35\n",
      "Epoch 0| Batch 5400/9812 | Loss 3.8455 | Accuracy 985.35\n",
      "Epoch 0| Batch 5600/9812 | Loss 4.6443 | Accuracy 985.35\n",
      "Epoch 0| Batch 5800/9812 | Loss 2.9741 | Accuracy 985.35\n",
      "Epoch 0| Batch 6000/9812 | Loss 1.8299 | Accuracy 985.35\n",
      "Epoch 0| Batch 6200/9812 | Loss 2.9299 | Accuracy 985.35\n",
      "Epoch 0| Batch 6400/9812 | Loss 1.8776 | Accuracy 985.35\n",
      "Epoch 0| Batch 6600/9812 | Loss 2.7755 | Accuracy 985.35\n",
      "Epoch 0| Batch 6800/9812 | Loss 4.1019 | Accuracy 985.35\n",
      "Epoch 0| Batch 7000/9812 | Loss 3.1585 | Accuracy 985.35\n",
      "Epoch 0| Batch 7200/9812 | Loss 3.1007 | Accuracy 985.35\n",
      "Epoch 0| Batch 7400/9812 | Loss 4.0176 | Accuracy 985.35\n",
      "Epoch 0| Batch 7600/9812 | Loss 4.5010 | Accuracy 985.35\n",
      "Epoch 0| Batch 7800/9812 | Loss 4.9097 | Accuracy 985.35\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 3.2609 | Accuracy 7.22\n",
      "Epoch 1| Batch 200/9812 | Loss 5.1784 | Accuracy 7.22\n",
      "Epoch 1| Batch 400/9812 | Loss 2.2914 | Accuracy 7.22\n",
      "Epoch 1| Batch 600/9812 | Loss 4.4594 | Accuracy 7.22\n",
      "Epoch 1| Batch 800/9812 | Loss 2.7099 | Accuracy 7.22\n",
      "Epoch 1| Batch 1000/9812 | Loss 2.7891 | Accuracy 7.22\n",
      "Epoch 1| Batch 1200/9812 | Loss 0.8685 | Accuracy 7.22\n",
      "Epoch 1| Batch 1400/9812 | Loss 3.7093 | Accuracy 7.22\n",
      "Epoch 1| Batch 1600/9812 | Loss 2.5840 | Accuracy 7.22\n",
      "Epoch 1| Batch 1800/9812 | Loss 4.0046 | Accuracy 7.22\n",
      "Epoch 1| Batch 2000/9812 | Loss 2.4376 | Accuracy 7.22\n",
      "Epoch 1| Batch 2200/9812 | Loss 0.8375 | Accuracy 7.22\n",
      "Epoch 1| Batch 2400/9812 | Loss 0.8622 | Accuracy 7.22\n",
      "Epoch 1| Batch 2600/9812 | Loss 2.7983 | Accuracy 7.22\n",
      "Epoch 1| Batch 2800/9812 | Loss 0.9338 | Accuracy 7.22\n",
      "Epoch 1| Batch 3000/9812 | Loss 2.0765 | Accuracy 7.22\n",
      "Epoch 1| Batch 3200/9812 | Loss 2.9070 | Accuracy 7.22\n",
      "Epoch 1| Batch 3400/9812 | Loss 2.7046 | Accuracy 7.22\n",
      "Epoch 1| Batch 3600/9812 | Loss 4.9481 | Accuracy 7.22\n",
      "Epoch 1| Batch 3800/9812 | Loss 0.2919 | Accuracy 7.22\n",
      "Epoch 1| Batch 4000/9812 | Loss 1.5668 | Accuracy 7.22\n",
      "Epoch 1| Batch 4200/9812 | Loss 1.4864 | Accuracy 7.22\n",
      "Epoch 1| Batch 4400/9812 | Loss 6.1736 | Accuracy 7.22\n",
      "Epoch 1| Batch 4600/9812 | Loss 2.0427 | Accuracy 7.22\n",
      "Epoch 1| Batch 4800/9812 | Loss 2.2940 | Accuracy 7.22\n",
      "Epoch 1| Batch 5000/9812 | Loss 0.6224 | Accuracy 7.22\n",
      "Epoch 1| Batch 5200/9812 | Loss 3.6522 | Accuracy 7.22\n",
      "Epoch 1| Batch 5400/9812 | Loss 2.7048 | Accuracy 7.22\n",
      "Epoch 1| Batch 5600/9812 | Loss 1.7351 | Accuracy 7.22\n",
      "Epoch 1| Batch 5800/9812 | Loss 2.6276 | Accuracy 7.22\n",
      "Epoch 1| Batch 6000/9812 | Loss 3.7524 | Accuracy 7.22\n",
      "Epoch 1| Batch 6200/9812 | Loss 2.3369 | Accuracy 7.22\n",
      "Epoch 1| Batch 6400/9812 | Loss 2.5195 | Accuracy 7.22\n",
      "Epoch 1| Batch 6600/9812 | Loss 0.3315 | Accuracy 7.22\n",
      "Epoch 1| Batch 6800/9812 | Loss 1.5303 | Accuracy 7.22\n",
      "Epoch 1| Batch 7000/9812 | Loss 3.7218 | Accuracy 7.22\n",
      "Epoch 1| Batch 7200/9812 | Loss 0.1302 | Accuracy 7.22\n",
      "Epoch 1| Batch 7400/9812 | Loss 0.0864 | Accuracy 7.22\n",
      "Epoch 1| Batch 7600/9812 | Loss 4.5780 | Accuracy 7.22\n",
      "Epoch 1| Batch 7800/9812 | Loss 1.4297 | Accuracy 7.22\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 2.5965 | Accuracy 29.58\n",
      "Epoch 2| Batch 200/9812 | Loss 0.7573 | Accuracy 29.58\n",
      "Epoch 2| Batch 400/9812 | Loss 2.0435 | Accuracy 29.58\n",
      "Epoch 2| Batch 600/9812 | Loss 2.0261 | Accuracy 29.58\n",
      "Epoch 2| Batch 800/9812 | Loss 0.4667 | Accuracy 29.58\n",
      "Epoch 2| Batch 1000/9812 | Loss 0.2512 | Accuracy 29.58\n",
      "Epoch 2| Batch 1200/9812 | Loss 0.4110 | Accuracy 29.58\n",
      "Epoch 2| Batch 1400/9812 | Loss 1.6199 | Accuracy 29.58\n",
      "Epoch 2| Batch 1600/9812 | Loss 3.4324 | Accuracy 29.58\n",
      "Epoch 2| Batch 1800/9812 | Loss 1.4285 | Accuracy 29.58\n",
      "Epoch 2| Batch 2000/9812 | Loss 5.3999 | Accuracy 29.58\n",
      "Epoch 2| Batch 2200/9812 | Loss 0.9633 | Accuracy 29.58\n",
      "Epoch 2| Batch 2400/9812 | Loss 4.3362 | Accuracy 29.58\n",
      "Epoch 2| Batch 2600/9812 | Loss 2.8566 | Accuracy 29.58\n",
      "Epoch 2| Batch 2800/9812 | Loss 2.6168 | Accuracy 29.58\n",
      "Epoch 2| Batch 3000/9812 | Loss 5.3763 | Accuracy 29.58\n",
      "Epoch 2| Batch 3200/9812 | Loss 3.6584 | Accuracy 29.58\n",
      "Epoch 2| Batch 3400/9812 | Loss 3.7933 | Accuracy 29.58\n",
      "Epoch 2| Batch 3600/9812 | Loss 4.9639 | Accuracy 29.58\n",
      "Epoch 2| Batch 3800/9812 | Loss 1.0858 | Accuracy 29.58\n",
      "Epoch 2| Batch 4000/9812 | Loss 4.1675 | Accuracy 29.58\n",
      "Epoch 2| Batch 4200/9812 | Loss 3.4774 | Accuracy 29.58\n",
      "Epoch 2| Batch 4400/9812 | Loss 2.7985 | Accuracy 29.58\n",
      "Epoch 2| Batch 4600/9812 | Loss 4.5095 | Accuracy 29.58\n",
      "Epoch 2| Batch 4800/9812 | Loss 4.6693 | Accuracy 29.58\n",
      "Epoch 2| Batch 5000/9812 | Loss 4.2303 | Accuracy 29.58\n",
      "Epoch 2| Batch 5200/9812 | Loss 0.2938 | Accuracy 29.58\n",
      "Epoch 2| Batch 5400/9812 | Loss 5.1356 | Accuracy 29.58\n",
      "Epoch 2| Batch 5600/9812 | Loss 5.8181 | Accuracy 29.58\n",
      "Epoch 2| Batch 5800/9812 | Loss 0.0001 | Accuracy 29.58\n",
      "Epoch 2| Batch 6000/9812 | Loss 1.8064 | Accuracy 29.58\n",
      "Epoch 2| Batch 6200/9812 | Loss 0.2571 | Accuracy 29.58\n",
      "Epoch 2| Batch 6400/9812 | Loss 2.0790 | Accuracy 29.58\n",
      "Epoch 2| Batch 6600/9812 | Loss 0.9442 | Accuracy 29.58\n",
      "Epoch 2| Batch 6800/9812 | Loss 2.3097 | Accuracy 29.58\n",
      "Epoch 2| Batch 7000/9812 | Loss 0.8141 | Accuracy 29.58\n",
      "Epoch 2| Batch 7200/9812 | Loss 1.1438 | Accuracy 29.58\n",
      "Epoch 2| Batch 7400/9812 | Loss 1.7479 | Accuracy 29.58\n",
      "Epoch 2| Batch 7600/9812 | Loss 5.1521 | Accuracy 29.58\n",
      "Epoch 2| Batch 7800/9812 | Loss 0.1958 | Accuracy 29.58\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 0.3570 | Accuracy 41.58\n",
      "Epoch 3| Batch 200/9812 | Loss 1.3256 | Accuracy 41.58\n",
      "Epoch 3| Batch 400/9812 | Loss 0.0244 | Accuracy 41.58\n",
      "Epoch 3| Batch 600/9812 | Loss 6.0502 | Accuracy 41.58\n",
      "Epoch 3| Batch 800/9812 | Loss 2.6742 | Accuracy 41.58\n",
      "Epoch 3| Batch 1000/9812 | Loss 2.7696 | Accuracy 41.58\n",
      "Epoch 3| Batch 1200/9812 | Loss 0.0044 | Accuracy 41.58\n",
      "Epoch 3| Batch 1400/9812 | Loss 2.6580 | Accuracy 41.58\n",
      "Epoch 3| Batch 1600/9812 | Loss 0.6370 | Accuracy 41.58\n",
      "Epoch 3| Batch 1800/9812 | Loss 0.1695 | Accuracy 41.58\n",
      "Epoch 3| Batch 2000/9812 | Loss 0.7089 | Accuracy 41.58\n",
      "Epoch 3| Batch 2200/9812 | Loss 2.0773 | Accuracy 41.58\n",
      "Epoch 3| Batch 2400/9812 | Loss 3.0530 | Accuracy 41.58\n",
      "Epoch 3| Batch 2600/9812 | Loss 0.1964 | Accuracy 41.58\n",
      "Epoch 3| Batch 2800/9812 | Loss 1.9165 | Accuracy 41.58\n",
      "Epoch 3| Batch 3000/9812 | Loss 5.0111 | Accuracy 41.58\n",
      "Epoch 3| Batch 3200/9812 | Loss 0.7138 | Accuracy 41.58\n",
      "Epoch 3| Batch 3400/9812 | Loss 6.2909 | Accuracy 41.58\n",
      "Epoch 3| Batch 3600/9812 | Loss 0.8673 | Accuracy 41.58\n",
      "Epoch 3| Batch 3800/9812 | Loss 0.9339 | Accuracy 41.58\n",
      "Epoch 3| Batch 4000/9812 | Loss 4.1825 | Accuracy 41.58\n",
      "Epoch 3| Batch 4200/9812 | Loss 1.1393 | Accuracy 41.58\n",
      "Epoch 3| Batch 4400/9812 | Loss 0.1191 | Accuracy 41.58\n",
      "Epoch 3| Batch 4600/9812 | Loss 1.4955 | Accuracy 41.58\n",
      "Epoch 3| Batch 4800/9812 | Loss 0.0005 | Accuracy 41.58\n",
      "Epoch 3| Batch 5000/9812 | Loss 1.9036 | Accuracy 41.58\n",
      "Epoch 3| Batch 5200/9812 | Loss 0.0002 | Accuracy 41.58\n",
      "Epoch 3| Batch 5400/9812 | Loss 1.9229 | Accuracy 41.58\n",
      "Epoch 3| Batch 5600/9812 | Loss 2.2614 | Accuracy 41.58\n",
      "Epoch 3| Batch 5800/9812 | Loss 3.6855 | Accuracy 41.58\n",
      "Epoch 3| Batch 6000/9812 | Loss 0.4489 | Accuracy 41.58\n",
      "Epoch 3| Batch 6200/9812 | Loss 2.3283 | Accuracy 41.58\n",
      "Epoch 3| Batch 6400/9812 | Loss 4.3886 | Accuracy 41.58\n",
      "Epoch 3| Batch 6600/9812 | Loss 0.1934 | Accuracy 41.58\n",
      "Epoch 3| Batch 6800/9812 | Loss 0.8193 | Accuracy 41.58\n",
      "Epoch 3| Batch 7000/9812 | Loss 0.6398 | Accuracy 41.58\n",
      "Epoch 3| Batch 7200/9812 | Loss 2.3726 | Accuracy 41.58\n",
      "Epoch 3| Batch 7400/9812 | Loss 1.8533 | Accuracy 41.58\n",
      "Epoch 3| Batch 7600/9812 | Loss 0.4194 | Accuracy 41.58\n",
      "Epoch 3| Batch 7800/9812 | Loss 1.0480 | Accuracy 41.58\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 0.0376 | Accuracy 50.32\n",
      "Epoch 4| Batch 200/9812 | Loss 0.0046 | Accuracy 50.32\n",
      "Epoch 4| Batch 400/9812 | Loss 4.9104 | Accuracy 50.32\n",
      "Epoch 4| Batch 600/9812 | Loss 4.1030 | Accuracy 50.32\n",
      "Epoch 4| Batch 800/9812 | Loss 0.7872 | Accuracy 50.32\n",
      "Epoch 4| Batch 1000/9812 | Loss 0.7266 | Accuracy 50.32\n",
      "Epoch 4| Batch 1200/9812 | Loss 2.4987 | Accuracy 50.32\n",
      "Epoch 4| Batch 1400/9812 | Loss 3.8259 | Accuracy 50.32\n",
      "Epoch 4| Batch 1600/9812 | Loss 0.0179 | Accuracy 50.32\n",
      "Epoch 4| Batch 1800/9812 | Loss 5.8749 | Accuracy 50.32\n",
      "Epoch 4| Batch 2000/9812 | Loss 6.8961 | Accuracy 50.32\n",
      "Epoch 4| Batch 2200/9812 | Loss 0.0872 | Accuracy 50.32\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.3119 | Accuracy 50.32\n",
      "Epoch 4| Batch 2600/9812 | Loss 1.3936 | Accuracy 50.32\n",
      "Epoch 4| Batch 2800/9812 | Loss 0.9617 | Accuracy 50.32\n",
      "Epoch 4| Batch 3000/9812 | Loss 1.0178 | Accuracy 50.32\n",
      "Epoch 4| Batch 3200/9812 | Loss 2.1770 | Accuracy 50.32\n",
      "Epoch 4| Batch 3400/9812 | Loss 0.4396 | Accuracy 50.32\n",
      "Epoch 4| Batch 3600/9812 | Loss 0.0027 | Accuracy 50.32\n",
      "Epoch 4| Batch 3800/9812 | Loss 0.4791 | Accuracy 50.32\n",
      "Epoch 4| Batch 4000/9812 | Loss 0.9717 | Accuracy 50.32\n",
      "Epoch 4| Batch 4200/9812 | Loss 1.1665 | Accuracy 50.32\n",
      "Epoch 4| Batch 4400/9812 | Loss 0.1392 | Accuracy 50.32\n",
      "Epoch 4| Batch 4600/9812 | Loss 1.2926 | Accuracy 50.32\n",
      "Epoch 4| Batch 4800/9812 | Loss 0.1410 | Accuracy 50.32\n",
      "Epoch 4| Batch 5000/9812 | Loss 0.0347 | Accuracy 50.32\n",
      "Epoch 4| Batch 5200/9812 | Loss 2.3532 | Accuracy 50.32\n",
      "Epoch 4| Batch 5400/9812 | Loss 1.6149 | Accuracy 50.32\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.5721 | Accuracy 50.32\n",
      "Epoch 4| Batch 5800/9812 | Loss 4.9458 | Accuracy 50.32\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.0259 | Accuracy 50.32\n",
      "Epoch 4| Batch 6200/9812 | Loss 3.1917 | Accuracy 50.32\n",
      "Epoch 4| Batch 6400/9812 | Loss 1.4141 | Accuracy 50.32\n",
      "Epoch 4| Batch 6600/9812 | Loss 0.2659 | Accuracy 50.32\n",
      "Epoch 4| Batch 6800/9812 | Loss 0.1359 | Accuracy 50.32\n",
      "Epoch 4| Batch 7000/9812 | Loss 3.3490 | Accuracy 50.32\n",
      "Epoch 4| Batch 7200/9812 | Loss 1.5108 | Accuracy 50.32\n",
      "Epoch 4| Batch 7400/9812 | Loss 1.4915 | Accuracy 50.32\n",
      "Epoch 4| Batch 7600/9812 | Loss 0.4111 | Accuracy 50.32\n",
      "Epoch 4| Batch 7800/9812 | Loss 1.0161 | Accuracy 50.32\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 1.0700 | Accuracy 56.10\n",
      "Epoch 5| Batch 200/9812 | Loss 4.2089 | Accuracy 56.10\n",
      "Epoch 5| Batch 400/9812 | Loss 0.3296 | Accuracy 56.10\n",
      "Epoch 5| Batch 600/9812 | Loss 0.2006 | Accuracy 56.10\n",
      "Epoch 5| Batch 800/9812 | Loss 0.0000 | Accuracy 56.10\n",
      "Epoch 5| Batch 1000/9812 | Loss 0.5281 | Accuracy 56.10\n",
      "Epoch 5| Batch 1200/9812 | Loss 3.5981 | Accuracy 56.10\n",
      "Epoch 5| Batch 1400/9812 | Loss 1.4772 | Accuracy 56.10\n",
      "Epoch 5| Batch 1600/9812 | Loss 1.5222 | Accuracy 56.10\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.0213 | Accuracy 56.10\n",
      "Epoch 5| Batch 2000/9812 | Loss 0.1422 | Accuracy 56.10\n",
      "Epoch 5| Batch 2200/9812 | Loss 0.4247 | Accuracy 56.10\n",
      "Epoch 5| Batch 2400/9812 | Loss 3.1745 | Accuracy 56.10\n",
      "Epoch 5| Batch 2600/9812 | Loss 1.5240 | Accuracy 56.10\n",
      "Epoch 5| Batch 2800/9812 | Loss 0.2396 | Accuracy 56.10\n",
      "Epoch 5| Batch 3000/9812 | Loss 1.7417 | Accuracy 56.10\n",
      "Epoch 5| Batch 3200/9812 | Loss 0.0918 | Accuracy 56.10\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.8364 | Accuracy 56.10\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.1533 | Accuracy 56.10\n",
      "Epoch 5| Batch 3800/9812 | Loss 2.5061 | Accuracy 56.10\n",
      "Epoch 5| Batch 4000/9812 | Loss 0.1848 | Accuracy 56.10\n",
      "Epoch 5| Batch 4200/9812 | Loss 3.2904 | Accuracy 56.10\n",
      "Epoch 5| Batch 4400/9812 | Loss 1.2051 | Accuracy 56.10\n",
      "Epoch 5| Batch 4600/9812 | Loss 0.3249 | Accuracy 56.10\n",
      "Epoch 5| Batch 4800/9812 | Loss 0.3753 | Accuracy 56.10\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.2881 | Accuracy 56.10\n",
      "Epoch 5| Batch 5200/9812 | Loss 1.5469 | Accuracy 56.10\n",
      "Epoch 5| Batch 5400/9812 | Loss 0.0070 | Accuracy 56.10\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.3217 | Accuracy 56.10\n",
      "Epoch 5| Batch 5800/9812 | Loss 0.9964 | Accuracy 56.10\n",
      "Epoch 5| Batch 6000/9812 | Loss 0.1571 | Accuracy 56.10\n",
      "Epoch 5| Batch 6200/9812 | Loss 0.1487 | Accuracy 56.10\n",
      "Epoch 5| Batch 6400/9812 | Loss 0.0032 | Accuracy 56.10\n",
      "Epoch 5| Batch 6600/9812 | Loss 2.7968 | Accuracy 56.10\n",
      "Epoch 5| Batch 6800/9812 | Loss 0.1062 | Accuracy 56.10\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.5423 | Accuracy 56.10\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.2275 | Accuracy 56.10\n",
      "Epoch 5| Batch 7400/9812 | Loss 2.4537 | Accuracy 56.10\n",
      "Epoch 5| Batch 7600/9812 | Loss 0.0022 | Accuracy 56.10\n",
      "Epoch 5| Batch 7800/9812 | Loss 0.0153 | Accuracy 56.10\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 0.0584 | Accuracy 61.35\n",
      "Epoch 6| Batch 200/9812 | Loss 0.0041 | Accuracy 61.35\n",
      "Epoch 6| Batch 400/9812 | Loss 0.0483 | Accuracy 61.35\n",
      "Epoch 6| Batch 600/9812 | Loss 0.9995 | Accuracy 61.35\n",
      "Epoch 6| Batch 800/9812 | Loss 1.3535 | Accuracy 61.35\n",
      "Epoch 6| Batch 1000/9812 | Loss 0.0077 | Accuracy 61.35\n",
      "Epoch 6| Batch 1200/9812 | Loss 1.1205 | Accuracy 61.35\n",
      "Epoch 6| Batch 1400/9812 | Loss 0.9041 | Accuracy 61.35\n",
      "Epoch 6| Batch 1600/9812 | Loss 0.0124 | Accuracy 61.35\n",
      "Epoch 6| Batch 1800/9812 | Loss 2.2044 | Accuracy 61.35\n",
      "Epoch 6| Batch 2000/9812 | Loss 1.4239 | Accuracy 61.35\n",
      "Epoch 6| Batch 2200/9812 | Loss 1.2711 | Accuracy 61.35\n",
      "Epoch 6| Batch 2400/9812 | Loss 1.8578 | Accuracy 61.35\n",
      "Epoch 6| Batch 2600/9812 | Loss 0.0211 | Accuracy 61.35\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.0231 | Accuracy 61.35\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.0047 | Accuracy 61.35\n",
      "Epoch 6| Batch 3200/9812 | Loss 0.4544 | Accuracy 61.35\n",
      "Epoch 6| Batch 3400/9812 | Loss 0.0016 | Accuracy 61.35\n",
      "Epoch 6| Batch 3600/9812 | Loss 1.1896 | Accuracy 61.35\n",
      "Epoch 6| Batch 3800/9812 | Loss 2.3156 | Accuracy 61.35\n",
      "Epoch 6| Batch 4000/9812 | Loss 2.0456 | Accuracy 61.35\n",
      "Epoch 6| Batch 4200/9812 | Loss 0.4197 | Accuracy 61.35\n",
      "Epoch 6| Batch 4400/9812 | Loss 0.2979 | Accuracy 61.35\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.4412 | Accuracy 61.35\n",
      "Epoch 6| Batch 4800/9812 | Loss 2.0889 | Accuracy 61.35\n",
      "Epoch 6| Batch 5000/9812 | Loss 0.4761 | Accuracy 61.35\n",
      "Epoch 6| Batch 5200/9812 | Loss 0.0163 | Accuracy 61.35\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.9573 | Accuracy 61.35\n",
      "Epoch 6| Batch 5600/9812 | Loss 0.0221 | Accuracy 61.35\n",
      "Epoch 6| Batch 5800/9812 | Loss 2.5918 | Accuracy 61.35\n",
      "Epoch 6| Batch 6000/9812 | Loss 0.1376 | Accuracy 61.35\n",
      "Epoch 6| Batch 6200/9812 | Loss 0.6645 | Accuracy 61.35\n",
      "Epoch 6| Batch 6400/9812 | Loss 0.5704 | Accuracy 61.35\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.1159 | Accuracy 61.35\n",
      "Epoch 6| Batch 6800/9812 | Loss 0.1874 | Accuracy 61.35\n",
      "Epoch 6| Batch 7000/9812 | Loss 1.5699 | Accuracy 61.35\n",
      "Epoch 6| Batch 7200/9812 | Loss 1.0100 | Accuracy 61.35\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.6722 | Accuracy 61.35\n",
      "Epoch 6| Batch 7600/9812 | Loss 0.0392 | Accuracy 61.35\n",
      "Epoch 6| Batch 7800/9812 | Loss 1.1155 | Accuracy 61.35\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 1.1397 | Accuracy 65.47\n",
      "Epoch 7| Batch 200/9812 | Loss 0.0150 | Accuracy 65.47\n",
      "Epoch 7| Batch 400/9812 | Loss 0.2472 | Accuracy 65.47\n",
      "Epoch 7| Batch 600/9812 | Loss 1.7044 | Accuracy 65.47\n",
      "Epoch 7| Batch 800/9812 | Loss 2.6064 | Accuracy 65.47\n",
      "Epoch 7| Batch 1000/9812 | Loss 0.3560 | Accuracy 65.47\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.1158 | Accuracy 65.47\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.0153 | Accuracy 65.47\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.7536 | Accuracy 65.47\n",
      "Epoch 7| Batch 1800/9812 | Loss 1.6945 | Accuracy 65.47\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0000 | Accuracy 65.47\n",
      "Epoch 7| Batch 2200/9812 | Loss 1.6760 | Accuracy 65.47\n",
      "Epoch 7| Batch 2400/9812 | Loss 0.0144 | Accuracy 65.47\n",
      "Epoch 7| Batch 2600/9812 | Loss 3.3668 | Accuracy 65.47\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.1770 | Accuracy 65.47\n",
      "Epoch 7| Batch 3000/9812 | Loss 0.3471 | Accuracy 65.47\n",
      "Epoch 7| Batch 3200/9812 | Loss 2.0253 | Accuracy 65.47\n",
      "Epoch 7| Batch 3400/9812 | Loss 1.6601 | Accuracy 65.47\n",
      "Epoch 7| Batch 3600/9812 | Loss 3.1896 | Accuracy 65.47\n",
      "Epoch 7| Batch 3800/9812 | Loss 0.0006 | Accuracy 65.47\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.0000 | Accuracy 65.47\n",
      "Epoch 7| Batch 4200/9812 | Loss 0.0000 | Accuracy 65.47\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.0186 | Accuracy 65.47\n",
      "Epoch 7| Batch 4600/9812 | Loss 0.1903 | Accuracy 65.47\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.2550 | Accuracy 65.47\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.7741 | Accuracy 65.47\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.2912 | Accuracy 65.47\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.5326 | Accuracy 65.47\n",
      "Epoch 7| Batch 5600/9812 | Loss 3.7489 | Accuracy 65.47\n",
      "Epoch 7| Batch 5800/9812 | Loss 1.0431 | Accuracy 65.47\n",
      "Epoch 7| Batch 6000/9812 | Loss 0.5354 | Accuracy 65.47\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.1643 | Accuracy 65.47\n",
      "Epoch 7| Batch 6400/9812 | Loss 1.0037 | Accuracy 65.47\n",
      "Epoch 7| Batch 6600/9812 | Loss 3.1300 | Accuracy 65.47\n",
      "Epoch 7| Batch 6800/9812 | Loss 0.0178 | Accuracy 65.47\n",
      "Epoch 7| Batch 7000/9812 | Loss 0.3073 | Accuracy 65.47\n",
      "Epoch 7| Batch 7200/9812 | Loss 0.0593 | Accuracy 65.47\n",
      "Epoch 7| Batch 7400/9812 | Loss 0.2198 | Accuracy 65.47\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.8881 | Accuracy 65.47\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.0042 | Accuracy 65.47\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 0.4695 | Accuracy 69.54\n",
      "Epoch 8| Batch 200/9812 | Loss 3.2188 | Accuracy 69.54\n",
      "Epoch 8| Batch 400/9812 | Loss 0.7602 | Accuracy 69.54\n",
      "Epoch 8| Batch 600/9812 | Loss 2.6099 | Accuracy 69.54\n",
      "Epoch 8| Batch 800/9812 | Loss 2.1642 | Accuracy 69.54\n",
      "Epoch 8| Batch 1000/9812 | Loss 3.6015 | Accuracy 69.54\n",
      "Epoch 8| Batch 1200/9812 | Loss 0.3834 | Accuracy 69.54\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.3509 | Accuracy 69.54\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.0099 | Accuracy 69.54\n",
      "Epoch 8| Batch 1800/9812 | Loss 0.0000 | Accuracy 69.54\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.0004 | Accuracy 69.54\n",
      "Epoch 8| Batch 2200/9812 | Loss 0.0000 | Accuracy 69.54\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0300 | Accuracy 69.54\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.0124 | Accuracy 69.54\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.0073 | Accuracy 69.54\n",
      "Epoch 8| Batch 3000/9812 | Loss 0.6051 | Accuracy 69.54\n",
      "Epoch 8| Batch 3200/9812 | Loss 0.0173 | Accuracy 69.54\n",
      "Epoch 8| Batch 3400/9812 | Loss 0.9284 | Accuracy 69.54\n",
      "Epoch 8| Batch 3600/9812 | Loss 2.0611 | Accuracy 69.54\n",
      "Epoch 8| Batch 3800/9812 | Loss 0.1724 | Accuracy 69.54\n",
      "Epoch 8| Batch 4000/9812 | Loss 1.4315 | Accuracy 69.54\n",
      "Epoch 8| Batch 4200/9812 | Loss 0.0255 | Accuracy 69.54\n",
      "Epoch 8| Batch 4400/9812 | Loss 0.3568 | Accuracy 69.54\n",
      "Epoch 8| Batch 4600/9812 | Loss 2.4393 | Accuracy 69.54\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.0087 | Accuracy 69.54\n",
      "Epoch 8| Batch 5000/9812 | Loss 0.0001 | Accuracy 69.54\n",
      "Epoch 8| Batch 5200/9812 | Loss 0.0001 | Accuracy 69.54\n",
      "Epoch 8| Batch 5400/9812 | Loss 3.9124 | Accuracy 69.54\n",
      "Epoch 8| Batch 5600/9812 | Loss 0.6154 | Accuracy 69.54\n",
      "Epoch 8| Batch 5800/9812 | Loss 2.5554 | Accuracy 69.54\n",
      "Epoch 8| Batch 6000/9812 | Loss 3.3198 | Accuracy 69.54\n",
      "Epoch 8| Batch 6200/9812 | Loss 0.3662 | Accuracy 69.54\n",
      "Epoch 8| Batch 6400/9812 | Loss 0.7152 | Accuracy 69.54\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.6952 | Accuracy 69.54\n",
      "Epoch 8| Batch 6800/9812 | Loss 0.2319 | Accuracy 69.54\n",
      "Epoch 8| Batch 7000/9812 | Loss 0.1190 | Accuracy 69.54\n",
      "Epoch 8| Batch 7200/9812 | Loss 0.4154 | Accuracy 69.54\n",
      "Epoch 8| Batch 7400/9812 | Loss 0.7684 | Accuracy 69.54\n",
      "Epoch 8| Batch 7600/9812 | Loss 0.3041 | Accuracy 69.54\n",
      "Epoch 8| Batch 7800/9812 | Loss 4.0897 | Accuracy 69.54\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 1.7963 | Accuracy 71.51\n",
      "Epoch 9| Batch 200/9812 | Loss 1.0833 | Accuracy 71.51\n",
      "Epoch 9| Batch 400/9812 | Loss 2.6590 | Accuracy 71.51\n",
      "Epoch 9| Batch 600/9812 | Loss 0.5697 | Accuracy 71.51\n",
      "Epoch 9| Batch 800/9812 | Loss 0.6881 | Accuracy 71.51\n",
      "Epoch 9| Batch 1000/9812 | Loss 0.2546 | Accuracy 71.51\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.6228 | Accuracy 71.51\n",
      "Epoch 9| Batch 1400/9812 | Loss 0.0507 | Accuracy 71.51\n",
      "Epoch 9| Batch 1600/9812 | Loss 0.2831 | Accuracy 71.51\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.1039 | Accuracy 71.51\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.0293 | Accuracy 71.51\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.9032 | Accuracy 71.51\n",
      "Epoch 9| Batch 2400/9812 | Loss 0.0965 | Accuracy 71.51\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.0685 | Accuracy 71.51\n",
      "Epoch 9| Batch 2800/9812 | Loss 0.5266 | Accuracy 71.51\n",
      "Epoch 9| Batch 3000/9812 | Loss 0.0115 | Accuracy 71.51\n",
      "Epoch 9| Batch 3200/9812 | Loss 0.3167 | Accuracy 71.51\n",
      "Epoch 9| Batch 3400/9812 | Loss 5.1573 | Accuracy 71.51\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0003 | Accuracy 71.51\n",
      "Epoch 9| Batch 3800/9812 | Loss 4.0477 | Accuracy 71.51\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.0480 | Accuracy 71.51\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.0771 | Accuracy 71.51\n",
      "Epoch 9| Batch 4400/9812 | Loss 1.9148 | Accuracy 71.51\n",
      "Epoch 9| Batch 4600/9812 | Loss 0.0862 | Accuracy 71.51\n",
      "Epoch 9| Batch 4800/9812 | Loss 1.0371 | Accuracy 71.51\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.0189 | Accuracy 71.51\n",
      "Epoch 9| Batch 5200/9812 | Loss 3.9615 | Accuracy 71.51\n",
      "Epoch 9| Batch 5400/9812 | Loss 1.8666 | Accuracy 71.51\n",
      "Epoch 9| Batch 5600/9812 | Loss 2.0853 | Accuracy 71.51\n",
      "Epoch 9| Batch 5800/9812 | Loss 3.2332 | Accuracy 71.51\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.1159 | Accuracy 71.51\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0020 | Accuracy 71.51\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.3986 | Accuracy 71.51\n",
      "Epoch 9| Batch 6600/9812 | Loss 2.4732 | Accuracy 71.51\n",
      "Epoch 9| Batch 6800/9812 | Loss 1.5432 | Accuracy 71.51\n",
      "Epoch 9| Batch 7000/9812 | Loss 2.4766 | Accuracy 71.51\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.0739 | Accuracy 71.51\n",
      "Epoch 9| Batch 7400/9812 | Loss 2.7158 | Accuracy 71.51\n",
      "Epoch 9| Batch 7600/9812 | Loss 0.0271 | Accuracy 71.51\n",
      "Epoch 9| Batch 7800/9812 | Loss 3.8782 | Accuracy 71.51\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.0009 | Accuracy 74.93\n"
     ]
    }
   ],
   "source": [
    "## UCF_CNN2D ##\n",
    "num_of_classes = 101\n",
    "h_in, h_out = 30, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 256, 128, 64, 1000, 500\n",
    "CNN_2D = UCF_CNN2D(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "\n",
    "criterion_2D = nn.CrossEntropyLoss()\n",
    "optimizer_2D = optim.Adam(CNN_2D.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "print(\"\\n\\n\\{}\\n\".format(CNN_2D.__class__.__name__ ))\n",
    "\n",
    "## Training the 2DCNN ##\n",
    "CNN_2D.train()\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        optimizer_2D.zero_grad()\n",
    "\n",
    "        y_pred = CNN_2D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = criterion_2D(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer_2D.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), train_acc))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 75.65%\n"
     ]
    }
   ],
   "source": [
    "CNN_2D.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = CNN_2D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UCFdataset(data.Dataset):\n",
    "#     def __init__(self, upper, lower, dir_path): # 'train', 'validation'\n",
    "#         super(UCFdataset, self).__init__()\n",
    "        \n",
    "#         self.file_list, self.y, self.video_names = self.file_load(upper, lower, dir_path)\n",
    "        \n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         x = jpg2np(self.file_list[index]) / 255. # (30, 3, 240, 320)\n",
    "#         self.x_data = torch.from_numpy(x).float()\n",
    "#         self.y_data = torch.from_numpy(self.y[index]).float()\n",
    "#         return self.x_data, self.y_data, self.video_names[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.y.shape[0]\n",
    "    \n",
    "#     def file_load(self, upper, lower, dir_path):\n",
    "#         \"\"\"\n",
    "#         return the input file path list\n",
    "#         \"\"\"\n",
    "#         data_path = []\n",
    "#         video_imgs_path = os.path.join(os.getcwd(), dir_path)\n",
    "#         folders = os.listdir(video_imgs_path)\n",
    "\n",
    "#         frames = {}\n",
    "#         for folder in folders:\n",
    "#             path = os.path.join(video_imgs_path, folder)\n",
    "#             frames[folder] = len(os.listdir(path))\n",
    "\n",
    "#         video_names = []\n",
    "#         for video_name, num_of_frames in zip(list(frames.keys()), list(frames.values())):\n",
    "#             if upper <= num_of_frames and num_of_frames <= lower:\n",
    "#                 video_names.append(video_name)\n",
    "#         video_names = natsort.natsorted(video_names)\n",
    "                \n",
    "#         print(\"Select The number of frames between [%d, %d] of UCF101 Dataset\" %(upper, lower))\n",
    "#         print('The number of selected videos is', len(video_names))\n",
    "\n",
    "\n",
    "#         data_path = [os.path.join(video_imgs_path, video_name) for video_name in video_names]\n",
    "\n",
    "#         labels = []\n",
    "#         for label in video_names:\n",
    "#             loc1 = label.find('_')\n",
    "#             loc2 = loc1 + label[loc1+1:].find('_')\n",
    "#             labels.append(label[loc1+1:loc2+1])\n",
    "#         y, _ = encoder(labels)\n",
    "# #         y = torch.tensor(y, dtype=torch.float) # [N(13320, 1)\n",
    "\n",
    "#         return data_path, y, video_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-tcn",
   "language": "python",
   "name": "ms-tcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
