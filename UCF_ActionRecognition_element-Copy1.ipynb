{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import natsort\n",
    "import bezier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from dataset import encoder\n",
    "from model import get_pretrained_model\n",
    "from dataset import jpg2np, get_loader\n",
    "from torchvision import models\n",
    "from bezier.hazmat.curve_helpers import evaluate_hodograph, get_curvature\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils import data\n",
    "from UCF_dataset import UCFdataset\n",
    "from model import UCF_DNN, UCF_CNN1D, UCF_CNN2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import UCF_DNN, UCF_CNN1D, UCF_CNN2D\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  1\n"
     ]
    }
   ],
   "source": [
    "cut = 9812\n",
    "upper, lower = 150, 10000\n",
    "GPU_NUM = 1\n",
    "bs=1\n",
    "upper, lower, dir_path = 150, 10000, './ucf_image'\n",
    "datastyle ='std' # 'embeddings', 'elementwise', 'minmax', 'std'\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select The number of frames between [150, 10000] of UCF101 Dataset\n",
      "The number of selected videos is 9812\n"
     ]
    }
   ],
   "source": [
    "dataset = UCFdataset(upper, lower, dir_path)\n",
    "loader = data.DataLoader(dataset=dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "num_of_classes = 101\n",
    "epochs=10\n",
    "\n",
    "\n",
    "## Embedding ##\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg_embedding = nn.Sequential(vgg16.features,\n",
    "#                    nn.AdaptiveAvgPool2d((1,1))).to(device)\n",
    "\n",
    "# bs_step = 0\n",
    "# for x, y, video_name in loader:\n",
    "#     if '{}.npy'.format(list(video_name)[0]) in  os.listdir('./ucf_embeddings/'): \n",
    "#         bs_step += bs\n",
    "#         continue\n",
    "#     x, y = x.to(device).squeeze(), y.squeeze().long().to(device)\n",
    "\n",
    "\n",
    "#     ## VGG Embedding ##\n",
    "#     embedding = vgg_embedding(x).squeeze().cpu().detach().numpy() # [30, 512]\n",
    "#     np.save('./ucf_embeddings/{}.npy'.format(list(video_name)[0]), embedding)    \n",
    "#     bs_step += bs\n",
    "#     if bs_step % 10 == 0: \n",
    "#         print(\"Batch {}/{} | {}\".format(bs_step, 9812, video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bezier Approximation and Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9812\n",
      "1000 9812\n",
      "2000 9812\n",
      "3000 9812\n",
      "4000 9812\n",
      "5000 9812\n",
      "6000 9812\n",
      "7000 9812\n",
      "8000 9812\n",
      "9000 9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9812, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load vgg16 embedding data ##\n",
    "video_names = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "for embedding in os.listdir('./ucf_embeddings'):\n",
    "    if embedding == '.ipynb_checkpoints': continue\n",
    "    loc1 = embedding.find('_')\n",
    "    loc2 = loc1 + embedding[loc1+1:].find('_')\n",
    "    labels.append(embedding[loc1+1:loc2+1])\n",
    "    video_names.append(embedding[:-4])\n",
    "  \n",
    "\n",
    "    embeddings.append(np.load('./ucf_embeddings/{}'.format(embedding)))\n",
    "\n",
    "\n",
    "y, _ = encoder(labels)\n",
    "\n",
    "embeddings = np.stack(embeddings)\n",
    "embeddings_pca = embeddings.reshape(embeddings.shape[0]*embeddings.shape[1],-1)\n",
    "\n",
    "## PCA ##\n",
    "pca = PCA(3)\n",
    "embeddings_pca = pca.fit_transform(embeddings_pca).reshape(embeddings.shape[0], embeddings.shape[1],-1)\n",
    "embeddings_pca.shape\n",
    "\n",
    "## Bezier Curve and Curvature ##\n",
    "k = dict()\n",
    "curves = []\n",
    "for i, embedding in enumerate(embeddings_pca):\n",
    "#     print(embedding.shape)\n",
    "    curves.append(bezier.Curve.from_nodes(embedding.T))\n",
    "    kappa = []\n",
    "    for s in range(30):\n",
    "        t = s / 30\n",
    "        tangent_vec = curves[i].evaluate_hodograph(t)\n",
    "        kappa.append(get_curvature(embedding.T, tangent_vec, t))\n",
    "    k[video_names[i]] = kappa\n",
    "    if i % 1000 == 0:\n",
    "        print(i, len(embeddings_pca))\n",
    "\n",
    "K = np.stack(list(k.values())) # [N, 30]\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the kappa vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard norm and operation\n",
      "[1000 / 9812] video processing!\n",
      "[2000 / 9812] video processing!\n",
      "[3000 / 9812] video processing!\n",
      "[4000 / 9812] video processing!\n",
      "[5000 / 9812] video processing!\n",
      "[6000 / 9812] video processing!\n",
      "[7000 / 9812] video processing!\n",
      "[8000 / 9812] video processing!\n",
      "[9000 / 9812] video processing!\n"
     ]
    }
   ],
   "source": [
    "## Elementwise operation ##\n",
    "if datastyle == 'elementwise':\n",
    "    print('just elementwise operation')\n",
    "    result = list()\n",
    "    for video in range(len(embeddings)):\n",
    "        elementwise = list()\n",
    "        for frame in range(30):\n",
    "            elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "        result.append(np.stack(elementwise))\n",
    "        if (video+1) % 1000 == 0: \n",
    "            print(\"[%d / %d] video processing!\" %(video+1, len(embeddings)))\n",
    "    result = torch.from_numpy(np.stack(result))\n",
    "#     print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)\n",
    "if datastyle == 'minmax': \n",
    "    print('minmax scaling and operation')\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    K = min_max_scaler.fit_transform(K.T).T\n",
    "    result = list()\n",
    "    for video in range(len(embeddings)):\n",
    "        elementwise = list()\n",
    "        for frame in range(30):\n",
    "            elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "        result.append(np.stack(elementwise))\n",
    "        if (video+1) % 1000 == 0: \n",
    "            print(\"[%d / %d] video processing!\" %(video+1, len(embeddings)))\n",
    "    result = torch.from_numpy(np.stack(result))\n",
    "    print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)\n",
    "if datastyle == 'std':\n",
    "    print('standard norm and operation')\n",
    "    standard_scaler = StandardScaler()\n",
    "    K = standard_scaler.fit_transform(K.T).T\n",
    "    result = list()\n",
    "    for video in range(len(embeddings)):\n",
    "        elementwise = list()\n",
    "        for frame in range(30):\n",
    "            elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "        result.append(np.stack(elementwise))\n",
    "        if (video+1) % 1000 == 0: \n",
    "            print(\"[%d / %d] video processing!\" %(video+1, len(embeddings)))\n",
    "    result = torch.from_numpy(np.stack(result))\n",
    "#     print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data with Train and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std\n",
      "torch.Size([9812, 30, 512]) torch.Size([9812, 1])\n"
     ]
    }
   ],
   "source": [
    "## Data Setting for Action Recognition(Prediction) ##\n",
    "\n",
    "print(datastyle)\n",
    "if datastyle =='embeddings':\n",
    "    X = torch.from_numpy(embeddings)\n",
    "if datastyle == 'elementwise' or 'minmax' or 'std':\n",
    "    X = result\n",
    "Y = torch.from_numpy(y)\n",
    "\n",
    "## train / test split ##\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, shuffle=True, random_state=123)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_DNN\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.7225 | Accuracy 0.00\n",
      "Epoch 0| Batch 400/9812 | Loss 4.8616 | Accuracy 0.25\n",
      "Epoch 0| Batch 600/9812 | Loss 4.7972 | Accuracy 0.67\n",
      "Epoch 0| Batch 800/9812 | Loss 4.7185 | Accuracy 0.88\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.7808 | Accuracy 1.10\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.4807 | Accuracy 1.33\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.7006 | Accuracy 1.43\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.4621 | Accuracy 1.50\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.4294 | Accuracy 1.50\n",
      "Epoch 0| Batch 2000/9812 | Loss 4.1868 | Accuracy 1.40\n",
      "Epoch 0| Batch 2200/9812 | Loss 4.3210 | Accuracy 1.32\n",
      "Epoch 0| Batch 2400/9812 | Loss 4.7901 | Accuracy 1.29\n",
      "Epoch 0| Batch 2600/9812 | Loss 4.7330 | Accuracy 1.31\n",
      "Epoch 0| Batch 2800/9812 | Loss 4.4767 | Accuracy 1.32\n",
      "Epoch 0| Batch 3000/9812 | Loss 4.6540 | Accuracy 1.33\n",
      "Epoch 0| Batch 3200/9812 | Loss 4.8509 | Accuracy 1.41\n",
      "Epoch 0| Batch 3400/9812 | Loss 4.6134 | Accuracy 1.35\n",
      "Epoch 0| Batch 3600/9812 | Loss 4.5825 | Accuracy 1.36\n",
      "Epoch 0| Batch 3800/9812 | Loss 4.3923 | Accuracy 1.34\n",
      "Epoch 0| Batch 4000/9812 | Loss 4.7993 | Accuracy 1.35\n",
      "Epoch 0| Batch 4200/9812 | Loss 3.9291 | Accuracy 1.38\n",
      "Epoch 0| Batch 4400/9812 | Loss 3.8904 | Accuracy 1.43\n",
      "Epoch 0| Batch 4600/9812 | Loss 3.0495 | Accuracy 1.43\n",
      "Epoch 0| Batch 4800/9812 | Loss 4.4047 | Accuracy 1.46\n",
      "Epoch 0| Batch 5000/9812 | Loss 4.4010 | Accuracy 1.52\n",
      "Epoch 0| Batch 5200/9812 | Loss 5.4854 | Accuracy 1.52\n",
      "Epoch 0| Batch 5400/9812 | Loss 3.9885 | Accuracy 1.50\n",
      "Epoch 0| Batch 5600/9812 | Loss 5.6215 | Accuracy 1.48\n",
      "Epoch 0| Batch 5800/9812 | Loss 4.3942 | Accuracy 1.55\n",
      "Epoch 0| Batch 6000/9812 | Loss 4.3375 | Accuracy 1.55\n",
      "Epoch 0| Batch 6200/9812 | Loss 4.6927 | Accuracy 1.53\n",
      "Epoch 0| Batch 6400/9812 | Loss 4.5162 | Accuracy 1.53\n",
      "Epoch 0| Batch 6600/9812 | Loss 4.2407 | Accuracy 1.58\n",
      "Epoch 0| Batch 6800/9812 | Loss 4.7419 | Accuracy 1.56\n",
      "Epoch 0| Batch 7000/9812 | Loss 4.1213 | Accuracy 1.57\n",
      "Epoch 0| Batch 7200/9812 | Loss 4.2122 | Accuracy 1.57\n",
      "Epoch 0| Batch 7400/9812 | Loss 3.7941 | Accuracy 1.55\n",
      "Epoch 0| Batch 7600/9812 | Loss 4.5722 | Accuracy 1.55\n",
      "Epoch 0| Batch 7800/9812 | Loss 4.4011 | Accuracy 1.55\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 3.8940 | Accuracy 1.54\n",
      "Epoch 1| Batch 200/9812 | Loss 3.9690 | Accuracy 1.50\n",
      "Epoch 1| Batch 400/9812 | Loss 3.7018 | Accuracy 2.25\n",
      "Epoch 1| Batch 600/9812 | Loss 4.1784 | Accuracy 2.33\n",
      "Epoch 1| Batch 800/9812 | Loss 4.7566 | Accuracy 2.00\n",
      "Epoch 1| Batch 1000/9812 | Loss 4.3657 | Accuracy 1.80\n",
      "Epoch 1| Batch 1200/9812 | Loss 4.6541 | Accuracy 1.75\n",
      "Epoch 1| Batch 1400/9812 | Loss 4.4996 | Accuracy 1.93\n",
      "Epoch 1| Batch 1600/9812 | Loss 3.3741 | Accuracy 2.00\n",
      "Epoch 1| Batch 1800/9812 | Loss 4.5932 | Accuracy 2.06\n",
      "Epoch 1| Batch 2000/9812 | Loss 5.8313 | Accuracy 1.95\n",
      "Epoch 1| Batch 2200/9812 | Loss 4.1700 | Accuracy 1.86\n",
      "Epoch 1| Batch 2400/9812 | Loss 4.2336 | Accuracy 2.00\n",
      "Epoch 1| Batch 2600/9812 | Loss 4.4953 | Accuracy 1.92\n",
      "Epoch 1| Batch 2800/9812 | Loss 4.4569 | Accuracy 2.00\n",
      "Epoch 1| Batch 3000/9812 | Loss 4.7137 | Accuracy 2.20\n",
      "Epoch 1| Batch 3200/9812 | Loss 4.4631 | Accuracy 2.22\n",
      "Epoch 1| Batch 3400/9812 | Loss 4.6622 | Accuracy 2.26\n",
      "Epoch 1| Batch 3600/9812 | Loss 3.7967 | Accuracy 2.31\n",
      "Epoch 1| Batch 3800/9812 | Loss 4.5143 | Accuracy 2.32\n",
      "Epoch 1| Batch 4000/9812 | Loss 3.7960 | Accuracy 2.25\n",
      "Epoch 1| Batch 4200/9812 | Loss 4.7882 | Accuracy 2.36\n",
      "Epoch 1| Batch 4400/9812 | Loss 4.1347 | Accuracy 2.34\n",
      "Epoch 1| Batch 4600/9812 | Loss 4.1065 | Accuracy 2.33\n",
      "Epoch 1| Batch 4800/9812 | Loss 4.5963 | Accuracy 2.35\n",
      "Epoch 1| Batch 5000/9812 | Loss 4.5605 | Accuracy 2.36\n",
      "Epoch 1| Batch 5200/9812 | Loss 4.9502 | Accuracy 2.35\n",
      "Epoch 1| Batch 5400/9812 | Loss 4.6593 | Accuracy 2.30\n",
      "Epoch 1| Batch 5600/9812 | Loss 4.3662 | Accuracy 2.30\n",
      "Epoch 1| Batch 5800/9812 | Loss 4.6683 | Accuracy 2.31\n",
      "Epoch 1| Batch 6000/9812 | Loss 4.5428 | Accuracy 2.33\n",
      "Epoch 1| Batch 6200/9812 | Loss 4.8277 | Accuracy 2.35\n",
      "Epoch 1| Batch 6400/9812 | Loss 5.1062 | Accuracy 2.39\n",
      "Epoch 1| Batch 6600/9812 | Loss 4.7843 | Accuracy 2.44\n",
      "Epoch 1| Batch 6800/9812 | Loss 4.1649 | Accuracy 2.49\n",
      "Epoch 1| Batch 7000/9812 | Loss 4.3573 | Accuracy 2.53\n",
      "Epoch 1| Batch 7200/9812 | Loss 4.0221 | Accuracy 2.61\n",
      "Epoch 1| Batch 7400/9812 | Loss 3.3471 | Accuracy 2.68\n",
      "Epoch 1| Batch 7600/9812 | Loss 4.3446 | Accuracy 2.71\n",
      "Epoch 1| Batch 7800/9812 | Loss 4.0237 | Accuracy 2.71\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 4.1749 | Accuracy 2.73\n",
      "Epoch 2| Batch 200/9812 | Loss 3.9290 | Accuracy 5.00\n",
      "Epoch 2| Batch 400/9812 | Loss 4.3603 | Accuracy 6.00\n",
      "Epoch 2| Batch 600/9812 | Loss 4.4664 | Accuracy 5.67\n",
      "Epoch 2| Batch 800/9812 | Loss 4.5045 | Accuracy 5.88\n",
      "Epoch 2| Batch 1000/9812 | Loss 4.8359 | Accuracy 5.20\n",
      "Epoch 2| Batch 1200/9812 | Loss 3.6678 | Accuracy 5.17\n",
      "Epoch 2| Batch 1400/9812 | Loss 4.0844 | Accuracy 5.00\n",
      "Epoch 2| Batch 1600/9812 | Loss 4.2119 | Accuracy 5.00\n",
      "Epoch 2| Batch 1800/9812 | Loss 4.9136 | Accuracy 4.83\n",
      "Epoch 2| Batch 2000/9812 | Loss 2.0569 | Accuracy 5.00\n",
      "Epoch 2| Batch 2200/9812 | Loss 4.4970 | Accuracy 5.00\n",
      "Epoch 2| Batch 2400/9812 | Loss 3.2204 | Accuracy 4.88\n",
      "Epoch 2| Batch 2600/9812 | Loss 3.4303 | Accuracy 4.88\n",
      "Epoch 2| Batch 2800/9812 | Loss 4.4736 | Accuracy 4.79\n",
      "Epoch 2| Batch 3000/9812 | Loss 3.8977 | Accuracy 4.67\n",
      "Epoch 2| Batch 3200/9812 | Loss 3.6684 | Accuracy 4.50\n",
      "Epoch 2| Batch 3400/9812 | Loss 4.4182 | Accuracy 4.41\n",
      "Epoch 2| Batch 3600/9812 | Loss 3.9848 | Accuracy 4.28\n",
      "Epoch 2| Batch 3800/9812 | Loss 3.8081 | Accuracy 4.21\n",
      "Epoch 2| Batch 4000/9812 | Loss 3.7201 | Accuracy 4.10\n",
      "Epoch 2| Batch 4200/9812 | Loss 3.7683 | Accuracy 4.14\n",
      "Epoch 2| Batch 4400/9812 | Loss 3.7245 | Accuracy 4.20\n",
      "Epoch 2| Batch 4600/9812 | Loss 3.9478 | Accuracy 4.13\n",
      "Epoch 2| Batch 4800/9812 | Loss 4.2923 | Accuracy 4.12\n",
      "Epoch 2| Batch 5000/9812 | Loss 2.6312 | Accuracy 4.22\n",
      "Epoch 2| Batch 5200/9812 | Loss 5.0636 | Accuracy 4.19\n",
      "Epoch 2| Batch 5400/9812 | Loss 4.5637 | Accuracy 4.15\n",
      "Epoch 2| Batch 5600/9812 | Loss 3.9521 | Accuracy 4.12\n",
      "Epoch 2| Batch 5800/9812 | Loss 4.2704 | Accuracy 4.09\n",
      "Epoch 2| Batch 6000/9812 | Loss 5.1831 | Accuracy 4.03\n",
      "Epoch 2| Batch 6200/9812 | Loss 8.2607 | Accuracy 4.00\n",
      "Epoch 2| Batch 6400/9812 | Loss 4.4793 | Accuracy 4.09\n",
      "Epoch 2| Batch 6600/9812 | Loss 3.9696 | Accuracy 4.09\n",
      "Epoch 2| Batch 6800/9812 | Loss 4.5405 | Accuracy 4.06\n",
      "Epoch 2| Batch 7000/9812 | Loss 3.8330 | Accuracy 4.00\n",
      "Epoch 2| Batch 7200/9812 | Loss 4.2064 | Accuracy 4.01\n",
      "Epoch 2| Batch 7400/9812 | Loss 4.3303 | Accuracy 4.00\n",
      "Epoch 2| Batch 7600/9812 | Loss 3.9762 | Accuracy 4.04\n",
      "Epoch 2| Batch 7800/9812 | Loss 4.6681 | Accuracy 4.06\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 3.0629 | Accuracy 4.06\n",
      "Epoch 3| Batch 200/9812 | Loss 4.2408 | Accuracy 7.00\n",
      "Epoch 3| Batch 400/9812 | Loss 4.0584 | Accuracy 7.00\n",
      "Epoch 3| Batch 600/9812 | Loss 4.1306 | Accuracy 6.67\n",
      "Epoch 3| Batch 800/9812 | Loss 3.9639 | Accuracy 6.62\n",
      "Epoch 3| Batch 1000/9812 | Loss 3.3087 | Accuracy 6.40\n",
      "Epoch 3| Batch 1200/9812 | Loss 3.7815 | Accuracy 6.50\n",
      "Epoch 3| Batch 1400/9812 | Loss 3.6793 | Accuracy 6.86\n",
      "Epoch 3| Batch 1600/9812 | Loss 3.3553 | Accuracy 6.50\n",
      "Epoch 3| Batch 1800/9812 | Loss 4.4004 | Accuracy 6.39\n",
      "Epoch 3| Batch 2000/9812 | Loss 3.4363 | Accuracy 6.45\n",
      "Epoch 3| Batch 2200/9812 | Loss 3.9359 | Accuracy 6.55\n",
      "Epoch 3| Batch 2400/9812 | Loss 4.0795 | Accuracy 6.50\n",
      "Epoch 3| Batch 2600/9812 | Loss 3.3592 | Accuracy 6.31\n",
      "Epoch 3| Batch 2800/9812 | Loss 3.2660 | Accuracy 6.29\n",
      "Epoch 3| Batch 3000/9812 | Loss 2.9109 | Accuracy 6.07\n",
      "Epoch 3| Batch 3200/9812 | Loss 3.4006 | Accuracy 6.06\n",
      "Epoch 3| Batch 3400/9812 | Loss 3.7820 | Accuracy 6.03\n",
      "Epoch 3| Batch 3600/9812 | Loss 3.8744 | Accuracy 6.00\n",
      "Epoch 3| Batch 3800/9812 | Loss 4.0840 | Accuracy 6.00\n",
      "Epoch 3| Batch 4000/9812 | Loss 6.0213 | Accuracy 5.97\n",
      "Epoch 3| Batch 4200/9812 | Loss 3.6116 | Accuracy 5.95\n",
      "Epoch 3| Batch 4400/9812 | Loss 4.4922 | Accuracy 5.91\n",
      "Epoch 3| Batch 4600/9812 | Loss 0.2902 | Accuracy 5.89\n",
      "Epoch 3| Batch 4800/9812 | Loss 2.9553 | Accuracy 5.90\n",
      "Epoch 3| Batch 5000/9812 | Loss 4.3819 | Accuracy 5.92\n",
      "Epoch 3| Batch 5200/9812 | Loss 3.8589 | Accuracy 5.92\n",
      "Epoch 3| Batch 5400/9812 | Loss 4.3491 | Accuracy 5.76\n",
      "Epoch 3| Batch 5600/9812 | Loss 2.1778 | Accuracy 5.73\n",
      "Epoch 3| Batch 5800/9812 | Loss 2.7227 | Accuracy 5.74\n",
      "Epoch 3| Batch 6000/9812 | Loss 4.2964 | Accuracy 5.72\n",
      "Epoch 3| Batch 6200/9812 | Loss 3.8516 | Accuracy 5.74\n",
      "Epoch 3| Batch 6400/9812 | Loss 3.9121 | Accuracy 5.81\n",
      "Epoch 3| Batch 6600/9812 | Loss 3.6215 | Accuracy 5.76\n",
      "Epoch 3| Batch 6800/9812 | Loss 3.6224 | Accuracy 5.78\n",
      "Epoch 3| Batch 7000/9812 | Loss 6.4474 | Accuracy 5.73\n",
      "Epoch 3| Batch 7200/9812 | Loss 3.1855 | Accuracy 5.86\n",
      "Epoch 3| Batch 7400/9812 | Loss 5.5094 | Accuracy 5.88\n",
      "Epoch 3| Batch 7600/9812 | Loss 2.7201 | Accuracy 5.87\n",
      "Epoch 3| Batch 7800/9812 | Loss 4.1973 | Accuracy 5.85\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 3.9455 | Accuracy 5.85\n",
      "Epoch 4| Batch 200/9812 | Loss 2.2831 | Accuracy 6.50\n",
      "Epoch 4| Batch 400/9812 | Loss 5.0107 | Accuracy 5.50\n",
      "Epoch 4| Batch 600/9812 | Loss 2.6723 | Accuracy 7.33\n",
      "Epoch 4| Batch 800/9812 | Loss 5.0922 | Accuracy 6.75\n",
      "Epoch 4| Batch 1000/9812 | Loss 2.1549 | Accuracy 7.20\n",
      "Epoch 4| Batch 1200/9812 | Loss 2.4314 | Accuracy 7.42\n",
      "Epoch 4| Batch 1400/9812 | Loss 3.5065 | Accuracy 7.36\n",
      "Epoch 4| Batch 1600/9812 | Loss 2.6349 | Accuracy 8.12\n",
      "Epoch 4| Batch 1800/9812 | Loss 4.5827 | Accuracy 8.33\n",
      "Epoch 4| Batch 2000/9812 | Loss 2.8932 | Accuracy 8.05\n",
      "Epoch 4| Batch 2200/9812 | Loss 4.0588 | Accuracy 7.77\n",
      "Epoch 4| Batch 2400/9812 | Loss 3.7751 | Accuracy 7.50\n",
      "Epoch 4| Batch 2600/9812 | Loss 1.6189 | Accuracy 7.62\n",
      "Epoch 4| Batch 2800/9812 | Loss 3.3851 | Accuracy 7.64\n",
      "Epoch 4| Batch 3000/9812 | Loss 4.0940 | Accuracy 7.57\n",
      "Epoch 4| Batch 3200/9812 | Loss 3.6891 | Accuracy 7.72\n",
      "Epoch 4| Batch 3400/9812 | Loss 2.6617 | Accuracy 7.68\n",
      "Epoch 4| Batch 3600/9812 | Loss 2.6022 | Accuracy 7.44\n",
      "Epoch 4| Batch 3800/9812 | Loss 4.2463 | Accuracy 7.53\n",
      "Epoch 4| Batch 4000/9812 | Loss 3.5802 | Accuracy 7.47\n",
      "Epoch 4| Batch 4200/9812 | Loss 3.9110 | Accuracy 7.45\n",
      "Epoch 4| Batch 4400/9812 | Loss 6.8133 | Accuracy 7.36\n",
      "Epoch 4| Batch 4600/9812 | Loss 3.9578 | Accuracy 7.30\n",
      "Epoch 4| Batch 4800/9812 | Loss 4.3303 | Accuracy 7.29\n",
      "Epoch 4| Batch 5000/9812 | Loss 4.0368 | Accuracy 7.44\n",
      "Epoch 4| Batch 5200/9812 | Loss 3.2287 | Accuracy 7.37\n",
      "Epoch 4| Batch 5400/9812 | Loss 4.1424 | Accuracy 7.39\n",
      "Epoch 4| Batch 5600/9812 | Loss 3.5404 | Accuracy 7.45\n",
      "Epoch 4| Batch 5800/9812 | Loss 1.9038 | Accuracy 7.52\n",
      "Epoch 4| Batch 6000/9812 | Loss 3.6794 | Accuracy 7.62\n",
      "Epoch 4| Batch 6200/9812 | Loss 3.6817 | Accuracy 7.58\n",
      "Epoch 4| Batch 6400/9812 | Loss 4.6777 | Accuracy 7.62\n",
      "Epoch 4| Batch 6600/9812 | Loss 3.0105 | Accuracy 7.65\n",
      "Epoch 4| Batch 6800/9812 | Loss 4.3663 | Accuracy 7.71\n",
      "Epoch 4| Batch 7000/9812 | Loss 3.6091 | Accuracy 7.77\n",
      "Epoch 4| Batch 7200/9812 | Loss 1.4223 | Accuracy 7.78\n",
      "Epoch 4| Batch 7400/9812 | Loss 3.5130 | Accuracy 7.84\n",
      "Epoch 4| Batch 7600/9812 | Loss 3.2187 | Accuracy 7.99\n",
      "Epoch 4| Batch 7800/9812 | Loss 2.7647 | Accuracy 7.96\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 4.3434 | Accuracy 7.94\n",
      "Epoch 5| Batch 200/9812 | Loss 3.3457 | Accuracy 11.00\n",
      "Epoch 5| Batch 400/9812 | Loss 3.6961 | Accuracy 10.50\n",
      "Epoch 5| Batch 600/9812 | Loss 4.6363 | Accuracy 11.17\n",
      "Epoch 5| Batch 800/9812 | Loss 2.3705 | Accuracy 10.88\n",
      "Epoch 5| Batch 1000/9812 | Loss 3.9143 | Accuracy 11.10\n",
      "Epoch 5| Batch 1200/9812 | Loss 0.2667 | Accuracy 11.17\n",
      "Epoch 5| Batch 1400/9812 | Loss 2.6854 | Accuracy 11.00\n",
      "Epoch 5| Batch 1600/9812 | Loss 3.2888 | Accuracy 10.56\n",
      "Epoch 5| Batch 1800/9812 | Loss 3.7894 | Accuracy 10.44\n",
      "Epoch 5| Batch 2000/9812 | Loss 4.0089 | Accuracy 10.15\n",
      "Epoch 5| Batch 2200/9812 | Loss 3.6649 | Accuracy 9.95\n",
      "Epoch 5| Batch 2400/9812 | Loss 4.3078 | Accuracy 9.75\n",
      "Epoch 5| Batch 2600/9812 | Loss 5.3564 | Accuracy 10.08\n",
      "Epoch 5| Batch 2800/9812 | Loss 1.6794 | Accuracy 10.04\n",
      "Epoch 5| Batch 3000/9812 | Loss 4.6293 | Accuracy 9.87\n",
      "Epoch 5| Batch 3200/9812 | Loss 3.8051 | Accuracy 10.09\n",
      "Epoch 5| Batch 3400/9812 | Loss 3.3376 | Accuracy 9.94\n",
      "Epoch 5| Batch 3600/9812 | Loss 3.4641 | Accuracy 9.81\n",
      "Epoch 5| Batch 3800/9812 | Loss 5.0543 | Accuracy 9.84\n",
      "Epoch 5| Batch 4000/9812 | Loss 9.2160 | Accuracy 9.75\n",
      "Epoch 5| Batch 4200/9812 | Loss 4.0182 | Accuracy 9.83\n",
      "Epoch 5| Batch 4400/9812 | Loss 3.9504 | Accuracy 9.77\n",
      "Epoch 5| Batch 4600/9812 | Loss 3.3741 | Accuracy 9.78\n",
      "Epoch 5| Batch 4800/9812 | Loss 3.5546 | Accuracy 9.83\n",
      "Epoch 5| Batch 5000/9812 | Loss 3.9342 | Accuracy 9.94\n",
      "Epoch 5| Batch 5200/9812 | Loss 3.7538 | Accuracy 9.98\n",
      "Epoch 5| Batch 5400/9812 | Loss 3.6651 | Accuracy 10.06\n",
      "Epoch 5| Batch 5600/9812 | Loss 3.4158 | Accuracy 10.02\n",
      "Epoch 5| Batch 5800/9812 | Loss 2.4438 | Accuracy 9.98\n",
      "Epoch 5| Batch 6000/9812 | Loss 4.5187 | Accuracy 9.98\n",
      "Epoch 5| Batch 6200/9812 | Loss 1.4446 | Accuracy 9.94\n",
      "Epoch 5| Batch 6400/9812 | Loss 1.4052 | Accuracy 9.97\n",
      "Epoch 5| Batch 6600/9812 | Loss 2.9323 | Accuracy 9.95\n",
      "Epoch 5| Batch 6800/9812 | Loss 3.4230 | Accuracy 10.03\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.0173 | Accuracy 10.19\n",
      "Epoch 5| Batch 7200/9812 | Loss 3.2540 | Accuracy 10.29\n",
      "Epoch 5| Batch 7400/9812 | Loss 4.6068 | Accuracy 10.20\n",
      "Epoch 5| Batch 7600/9812 | Loss 3.7867 | Accuracy 10.12\n",
      "Epoch 5| Batch 7800/9812 | Loss 2.9726 | Accuracy 10.10\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 2.3373 | Accuracy 10.14\n",
      "Epoch 6| Batch 200/9812 | Loss 0.0042 | Accuracy 14.00\n",
      "Epoch 6| Batch 400/9812 | Loss 2.3150 | Accuracy 13.50\n",
      "Epoch 6| Batch 600/9812 | Loss 5.1239 | Accuracy 13.33\n",
      "Epoch 6| Batch 800/9812 | Loss 4.1821 | Accuracy 13.38\n",
      "Epoch 6| Batch 1000/9812 | Loss 3.6866 | Accuracy 13.80\n",
      "Epoch 6| Batch 1200/9812 | Loss 3.4123 | Accuracy 13.17\n",
      "Epoch 6| Batch 1400/9812 | Loss 3.7984 | Accuracy 12.71\n",
      "Epoch 6| Batch 1600/9812 | Loss 4.0423 | Accuracy 12.31\n",
      "Epoch 6| Batch 1800/9812 | Loss 3.1949 | Accuracy 12.61\n",
      "Epoch 6| Batch 2000/9812 | Loss 3.2878 | Accuracy 12.60\n",
      "Epoch 6| Batch 2200/9812 | Loss 2.3500 | Accuracy 12.73\n",
      "Epoch 6| Batch 2400/9812 | Loss 3.6531 | Accuracy 12.71\n",
      "Epoch 6| Batch 2600/9812 | Loss 2.1562 | Accuracy 12.38\n",
      "Epoch 6| Batch 2800/9812 | Loss 3.9107 | Accuracy 12.32\n",
      "Epoch 6| Batch 3000/9812 | Loss 3.0584 | Accuracy 12.20\n",
      "Epoch 6| Batch 3200/9812 | Loss 2.4444 | Accuracy 12.22\n",
      "Epoch 6| Batch 3400/9812 | Loss 2.3970 | Accuracy 12.35\n",
      "Epoch 6| Batch 3600/9812 | Loss 4.3546 | Accuracy 12.47\n",
      "Epoch 6| Batch 3800/9812 | Loss 3.1490 | Accuracy 12.76\n",
      "Epoch 6| Batch 4000/9812 | Loss 3.4360 | Accuracy 12.68\n",
      "Epoch 6| Batch 4200/9812 | Loss 2.5933 | Accuracy 12.79\n",
      "Epoch 6| Batch 4400/9812 | Loss 1.5865 | Accuracy 12.75\n",
      "Epoch 6| Batch 4600/9812 | Loss 3.8422 | Accuracy 12.76\n",
      "Epoch 6| Batch 4800/9812 | Loss 1.2498 | Accuracy 12.73\n",
      "Epoch 6| Batch 5000/9812 | Loss 3.5797 | Accuracy 12.62\n",
      "Epoch 6| Batch 5200/9812 | Loss 6.3851 | Accuracy 12.65\n",
      "Epoch 6| Batch 5400/9812 | Loss 3.9807 | Accuracy 12.63\n",
      "Epoch 6| Batch 5600/9812 | Loss 1.8472 | Accuracy 12.62\n",
      "Epoch 6| Batch 5800/9812 | Loss 3.2116 | Accuracy 12.72\n",
      "Epoch 6| Batch 6000/9812 | Loss 2.5643 | Accuracy 12.62\n",
      "Epoch 6| Batch 6200/9812 | Loss 2.3616 | Accuracy 12.68\n",
      "Epoch 6| Batch 6400/9812 | Loss 3.9178 | Accuracy 12.70\n",
      "Epoch 6| Batch 6600/9812 | Loss 3.4338 | Accuracy 12.76\n",
      "Epoch 6| Batch 6800/9812 | Loss 3.4727 | Accuracy 12.72\n",
      "Epoch 6| Batch 7000/9812 | Loss 2.7343 | Accuracy 12.81\n",
      "Epoch 6| Batch 7200/9812 | Loss 5.3043 | Accuracy 12.85\n",
      "Epoch 6| Batch 7400/9812 | Loss 3.1447 | Accuracy 12.84\n",
      "Epoch 6| Batch 7600/9812 | Loss 2.5800 | Accuracy 12.84\n",
      "Epoch 6| Batch 7800/9812 | Loss 3.2101 | Accuracy 12.91\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 2.4906 | Accuracy 12.93\n",
      "Epoch 7| Batch 200/9812 | Loss 3.8444 | Accuracy 16.00\n",
      "Epoch 7| Batch 400/9812 | Loss 2.5033 | Accuracy 17.75\n",
      "Epoch 7| Batch 600/9812 | Loss 3.2288 | Accuracy 18.33\n",
      "Epoch 7| Batch 800/9812 | Loss 2.9266 | Accuracy 18.12\n",
      "Epoch 7| Batch 1000/9812 | Loss 3.4846 | Accuracy 17.80\n",
      "Epoch 7| Batch 1200/9812 | Loss 3.1193 | Accuracy 17.00\n",
      "Epoch 7| Batch 1400/9812 | Loss 1.6074 | Accuracy 16.50\n",
      "Epoch 7| Batch 1600/9812 | Loss 3.3827 | Accuracy 16.56\n",
      "Epoch 7| Batch 1800/9812 | Loss 1.6723 | Accuracy 16.28\n",
      "Epoch 7| Batch 2000/9812 | Loss 4.4962 | Accuracy 16.30\n",
      "Epoch 7| Batch 2200/9812 | Loss 2.1165 | Accuracy 16.27\n",
      "Epoch 7| Batch 2400/9812 | Loss 3.6920 | Accuracy 16.12\n",
      "Epoch 7| Batch 2600/9812 | Loss 5.0857 | Accuracy 16.08\n",
      "Epoch 7| Batch 2800/9812 | Loss 3.9528 | Accuracy 15.79\n",
      "Epoch 7| Batch 3000/9812 | Loss 1.8252 | Accuracy 15.80\n",
      "Epoch 7| Batch 3200/9812 | Loss 3.6240 | Accuracy 15.91\n",
      "Epoch 7| Batch 3400/9812 | Loss 2.9340 | Accuracy 15.79\n",
      "Epoch 7| Batch 3600/9812 | Loss 3.2881 | Accuracy 15.78\n",
      "Epoch 7| Batch 3800/9812 | Loss 3.2519 | Accuracy 15.92\n",
      "Epoch 7| Batch 4000/9812 | Loss 3.8288 | Accuracy 16.05\n",
      "Epoch 7| Batch 4200/9812 | Loss 2.9403 | Accuracy 16.00\n",
      "Epoch 7| Batch 4400/9812 | Loss 3.6672 | Accuracy 15.95\n",
      "Epoch 7| Batch 4600/9812 | Loss 2.9983 | Accuracy 15.93\n",
      "Epoch 7| Batch 4800/9812 | Loss 2.0706 | Accuracy 15.92\n",
      "Epoch 7| Batch 5000/9812 | Loss 4.0918 | Accuracy 16.00\n",
      "Epoch 7| Batch 5200/9812 | Loss 3.0151 | Accuracy 16.00\n",
      "Epoch 7| Batch 5400/9812 | Loss 2.8329 | Accuracy 15.87\n",
      "Epoch 7| Batch 5600/9812 | Loss 5.1108 | Accuracy 15.82\n",
      "Epoch 7| Batch 5800/9812 | Loss 4.2514 | Accuracy 15.81\n",
      "Epoch 7| Batch 6000/9812 | Loss 1.4269 | Accuracy 15.68\n",
      "Epoch 7| Batch 6200/9812 | Loss 5.9902 | Accuracy 15.63\n",
      "Epoch 7| Batch 6400/9812 | Loss 3.4835 | Accuracy 15.55\n",
      "Epoch 7| Batch 6600/9812 | Loss 2.3019 | Accuracy 15.53\n",
      "Epoch 7| Batch 6800/9812 | Loss 3.5397 | Accuracy 15.51\n",
      "Epoch 7| Batch 7000/9812 | Loss 4.5218 | Accuracy 15.56\n",
      "Epoch 7| Batch 7200/9812 | Loss 1.5808 | Accuracy 15.62\n",
      "Epoch 7| Batch 7400/9812 | Loss 1.5202 | Accuracy 15.68\n",
      "Epoch 7| Batch 7600/9812 | Loss 2.2465 | Accuracy 15.74\n",
      "Epoch 7| Batch 7800/9812 | Loss 3.9162 | Accuracy 15.63\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 4.3889 | Accuracy 15.65\n",
      "Epoch 8| Batch 200/9812 | Loss 3.7726 | Accuracy 20.50\n",
      "Epoch 8| Batch 400/9812 | Loss 2.0950 | Accuracy 17.75\n",
      "Epoch 8| Batch 600/9812 | Loss 3.2607 | Accuracy 19.17\n",
      "Epoch 8| Batch 800/9812 | Loss 0.3609 | Accuracy 19.25\n",
      "Epoch 8| Batch 1000/9812 | Loss 0.0034 | Accuracy 18.50\n",
      "Epoch 8| Batch 1200/9812 | Loss 3.8367 | Accuracy 17.67\n",
      "Epoch 8| Batch 1400/9812 | Loss 3.0974 | Accuracy 17.14\n",
      "Epoch 8| Batch 1600/9812 | Loss 3.1563 | Accuracy 17.38\n",
      "Epoch 8| Batch 1800/9812 | Loss 2.6704 | Accuracy 17.39\n",
      "Epoch 8| Batch 2000/9812 | Loss 1.9699 | Accuracy 17.50\n",
      "Epoch 8| Batch 2200/9812 | Loss 3.3437 | Accuracy 17.45\n",
      "Epoch 8| Batch 2400/9812 | Loss 2.8207 | Accuracy 17.92\n",
      "Epoch 8| Batch 2600/9812 | Loss 1.6731 | Accuracy 18.12\n",
      "Epoch 8| Batch 2800/9812 | Loss 2.6606 | Accuracy 18.18\n",
      "Epoch 8| Batch 3000/9812 | Loss 2.7092 | Accuracy 18.20\n",
      "Epoch 8| Batch 3200/9812 | Loss 2.9244 | Accuracy 18.12\n",
      "Epoch 8| Batch 3400/9812 | Loss 3.9128 | Accuracy 18.26\n",
      "Epoch 8| Batch 3600/9812 | Loss 4.2386 | Accuracy 18.36\n",
      "Epoch 8| Batch 3800/9812 | Loss 2.8302 | Accuracy 18.39\n",
      "Epoch 8| Batch 4000/9812 | Loss 4.1615 | Accuracy 18.23\n",
      "Epoch 8| Batch 4200/9812 | Loss 3.4472 | Accuracy 18.21\n",
      "Epoch 8| Batch 4400/9812 | Loss 4.7193 | Accuracy 18.25\n",
      "Epoch 8| Batch 4600/9812 | Loss 5.0563 | Accuracy 18.50\n",
      "Epoch 8| Batch 4800/9812 | Loss 3.2314 | Accuracy 18.42\n",
      "Epoch 8| Batch 5000/9812 | Loss 2.8636 | Accuracy 18.28\n",
      "Epoch 8| Batch 5200/9812 | Loss 3.2065 | Accuracy 18.31\n",
      "Epoch 8| Batch 5400/9812 | Loss 2.0390 | Accuracy 18.44\n",
      "Epoch 8| Batch 5600/9812 | Loss 2.8052 | Accuracy 18.48\n",
      "Epoch 8| Batch 5800/9812 | Loss 4.9302 | Accuracy 18.36\n",
      "Epoch 8| Batch 6000/9812 | Loss 3.8553 | Accuracy 18.40\n",
      "Epoch 8| Batch 6200/9812 | Loss 2.5279 | Accuracy 18.52\n",
      "Epoch 8| Batch 6400/9812 | Loss 3.0661 | Accuracy 18.62\n",
      "Epoch 8| Batch 6600/9812 | Loss 3.9515 | Accuracy 18.68\n",
      "Epoch 8| Batch 6800/9812 | Loss 3.4029 | Accuracy 18.68\n",
      "Epoch 8| Batch 7000/9812 | Loss 2.9704 | Accuracy 18.74\n",
      "Epoch 8| Batch 7200/9812 | Loss 3.3665 | Accuracy 18.67\n",
      "Epoch 8| Batch 7400/9812 | Loss 5.8672 | Accuracy 18.69\n",
      "Epoch 8| Batch 7600/9812 | Loss 4.3795 | Accuracy 18.55\n",
      "Epoch 8| Batch 7800/9812 | Loss 1.5338 | Accuracy 18.59\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 2.7004 | Accuracy 18.63\n",
      "Epoch 9| Batch 200/9812 | Loss 2.5061 | Accuracy 26.00\n",
      "Epoch 9| Batch 400/9812 | Loss 2.5654 | Accuracy 25.50\n",
      "Epoch 9| Batch 600/9812 | Loss 1.7791 | Accuracy 24.00\n",
      "Epoch 9| Batch 800/9812 | Loss 3.6207 | Accuracy 24.00\n",
      "Epoch 9| Batch 1000/9812 | Loss 3.4736 | Accuracy 24.00\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.8487 | Accuracy 23.50\n",
      "Epoch 9| Batch 1400/9812 | Loss 4.5142 | Accuracy 23.43\n",
      "Epoch 9| Batch 1600/9812 | Loss 4.2290 | Accuracy 24.12\n",
      "Epoch 9| Batch 1800/9812 | Loss 1.3793 | Accuracy 23.22\n",
      "Epoch 9| Batch 2000/9812 | Loss 5.8961 | Accuracy 23.10\n",
      "Epoch 9| Batch 2200/9812 | Loss 2.2059 | Accuracy 22.77\n",
      "Epoch 9| Batch 2400/9812 | Loss 5.2925 | Accuracy 22.83\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.9187 | Accuracy 22.81\n",
      "Epoch 9| Batch 2800/9812 | Loss 2.2082 | Accuracy 22.79\n",
      "Epoch 9| Batch 3000/9812 | Loss 3.9415 | Accuracy 22.83\n",
      "Epoch 9| Batch 3200/9812 | Loss 2.4384 | Accuracy 23.03\n",
      "Epoch 9| Batch 3400/9812 | Loss 1.2392 | Accuracy 22.82\n",
      "Epoch 9| Batch 3600/9812 | Loss 1.5159 | Accuracy 22.61\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.8419 | Accuracy 22.61\n",
      "Epoch 9| Batch 4000/9812 | Loss 4.0106 | Accuracy 22.65\n",
      "Epoch 9| Batch 4200/9812 | Loss 3.3019 | Accuracy 22.67\n",
      "Epoch 9| Batch 4400/9812 | Loss 2.1797 | Accuracy 22.64\n",
      "Epoch 9| Batch 4600/9812 | Loss 2.2498 | Accuracy 22.65\n",
      "Epoch 9| Batch 4800/9812 | Loss 8.7536 | Accuracy 22.48\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.1894 | Accuracy 22.44\n",
      "Epoch 9| Batch 5200/9812 | Loss 2.0378 | Accuracy 22.23\n",
      "Epoch 9| Batch 5400/9812 | Loss 4.5829 | Accuracy 22.30\n",
      "Epoch 9| Batch 5600/9812 | Loss 3.4910 | Accuracy 22.27\n",
      "Epoch 9| Batch 5800/9812 | Loss 3.2903 | Accuracy 22.38\n",
      "Epoch 9| Batch 6000/9812 | Loss 2.8632 | Accuracy 22.40\n",
      "Epoch 9| Batch 6200/9812 | Loss 1.6455 | Accuracy 22.39\n",
      "Epoch 9| Batch 6400/9812 | Loss 2.9053 | Accuracy 22.52\n",
      "Epoch 9| Batch 6600/9812 | Loss 3.6444 | Accuracy 22.38\n",
      "Epoch 9| Batch 6800/9812 | Loss 6.2985 | Accuracy 22.26\n",
      "Epoch 9| Batch 7000/9812 | Loss 2.4754 | Accuracy 22.19\n",
      "Epoch 9| Batch 7200/9812 | Loss 1.5194 | Accuracy 22.17\n",
      "Epoch 9| Batch 7400/9812 | Loss 2.4257 | Accuracy 22.27\n",
      "Epoch 9| Batch 7600/9812 | Loss 1.1134 | Accuracy 22.42\n",
      "Epoch 9| Batch 7800/9812 | Loss 3.6257 | Accuracy 22.33\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 2.2120 | Accuracy 22.32\n"
     ]
    }
   ],
   "source": [
    "## DNN ##\n",
    "h_in, h_out =  30*512, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 512, 256, 128, 128, 100\n",
    "DNN = UCF_DNN(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "DNN_criterion = nn.CrossEntropyLoss()\n",
    "DNN_optimizer = optim.Adam(DNN.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "## Training the DNN ##\n",
    "DNN.train()\n",
    "print(\"\\n\\n\\{}\\n\".format(DNN.__class__.__name__ ))\n",
    "epochs=10\n",
    "DNN.train()\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        DNN_optimizer.zero_grad()\n",
    "\n",
    "        y_pred = DNN(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = DNN_criterion(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        DNN_optimizer.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 14.72%\n"
     ]
    }
   ],
   "source": [
    "DNN.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = DNN(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_CNN1D\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.5827 | Accuracy 1.50\n",
      "Epoch 0| Batch 400/9812 | Loss 4.3081 | Accuracy 0.75\n",
      "Epoch 0| Batch 600/9812 | Loss 4.3630 | Accuracy 1.00\n",
      "Epoch 0| Batch 800/9812 | Loss 4.6849 | Accuracy 1.00\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.5445 | Accuracy 1.10\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.4805 | Accuracy 1.00\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.4598 | Accuracy 1.36\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.4295 | Accuracy 1.69\n",
      "Epoch 0| Batch 1800/9812 | Loss 3.3189 | Accuracy 2.39\n",
      "Epoch 0| Batch 2000/9812 | Loss 3.9807 | Accuracy 3.45\n",
      "Epoch 0| Batch 2200/9812 | Loss 3.9307 | Accuracy 4.41\n",
      "Epoch 0| Batch 2400/9812 | Loss 7.4732 | Accuracy 5.50\n",
      "Epoch 0| Batch 2600/9812 | Loss 3.7324 | Accuracy 6.69\n",
      "Epoch 0| Batch 2800/9812 | Loss 2.0401 | Accuracy 7.79\n",
      "Epoch 0| Batch 3000/9812 | Loss 4.2827 | Accuracy 8.70\n",
      "Epoch 0| Batch 3200/9812 | Loss 1.9187 | Accuracy 10.06\n",
      "Epoch 0| Batch 3400/9812 | Loss 4.6383 | Accuracy 11.35\n",
      "Epoch 0| Batch 3600/9812 | Loss 1.5163 | Accuracy 12.36\n",
      "Epoch 0| Batch 3800/9812 | Loss 0.2552 | Accuracy 13.76\n",
      "Epoch 0| Batch 4000/9812 | Loss 2.6674 | Accuracy 14.75\n",
      "Epoch 0| Batch 4200/9812 | Loss 0.0002 | Accuracy 16.33\n",
      "Epoch 0| Batch 4400/9812 | Loss 0.0488 | Accuracy 17.27\n",
      "Epoch 0| Batch 4600/9812 | Loss 0.9828 | Accuracy 18.26\n",
      "Epoch 0| Batch 4800/9812 | Loss 0.0659 | Accuracy 19.25\n",
      "Epoch 0| Batch 5000/9812 | Loss 0.2503 | Accuracy 20.34\n",
      "Epoch 0| Batch 5200/9812 | Loss 1.6617 | Accuracy 21.08\n",
      "Epoch 0| Batch 5400/9812 | Loss 0.0817 | Accuracy 21.98\n",
      "Epoch 0| Batch 5600/9812 | Loss 4.1216 | Accuracy 22.91\n",
      "Epoch 0| Batch 5800/9812 | Loss 1.7266 | Accuracy 23.74\n",
      "Epoch 0| Batch 6000/9812 | Loss 1.8232 | Accuracy 24.37\n",
      "Epoch 0| Batch 6200/9812 | Loss 0.3684 | Accuracy 25.26\n",
      "Epoch 0| Batch 6400/9812 | Loss 2.1988 | Accuracy 26.08\n",
      "Epoch 0| Batch 6600/9812 | Loss 0.1267 | Accuracy 26.88\n",
      "Epoch 0| Batch 6800/9812 | Loss 0.5605 | Accuracy 27.56\n",
      "Epoch 0| Batch 7000/9812 | Loss 1.5977 | Accuracy 28.34\n",
      "Epoch 0| Batch 7200/9812 | Loss 3.5513 | Accuracy 28.96\n",
      "Epoch 0| Batch 7400/9812 | Loss 2.5855 | Accuracy 29.78\n",
      "Epoch 0| Batch 7600/9812 | Loss 1.7381 | Accuracy 30.45\n",
      "Epoch 0| Batch 7800/9812 | Loss 1.1607 | Accuracy 30.97\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 1.9672 | Accuracy 31.11\n",
      "Epoch 1| Batch 200/9812 | Loss 0.0025 | Accuracy 58.00\n",
      "Epoch 1| Batch 400/9812 | Loss 0.5803 | Accuracy 60.25\n",
      "Epoch 1| Batch 600/9812 | Loss 2.8453 | Accuracy 55.83\n",
      "Epoch 1| Batch 800/9812 | Loss 5.0504 | Accuracy 58.38\n",
      "Epoch 1| Batch 1000/9812 | Loss 0.8100 | Accuracy 59.30\n",
      "Epoch 1| Batch 1200/9812 | Loss 3.8637 | Accuracy 59.42\n",
      "Epoch 1| Batch 1400/9812 | Loss 0.0894 | Accuracy 60.00\n",
      "Epoch 1| Batch 1600/9812 | Loss 0.0018 | Accuracy 60.44\n",
      "Epoch 1| Batch 1800/9812 | Loss 0.0248 | Accuracy 61.50\n",
      "Epoch 1| Batch 2000/9812 | Loss 0.0364 | Accuracy 61.20\n",
      "Epoch 1| Batch 2200/9812 | Loss 0.2769 | Accuracy 60.73\n",
      "Epoch 1| Batch 2400/9812 | Loss 0.0002 | Accuracy 61.04\n",
      "Epoch 1| Batch 2600/9812 | Loss 0.0347 | Accuracy 61.15\n",
      "Epoch 1| Batch 2800/9812 | Loss 1.3238 | Accuracy 61.25\n",
      "Epoch 1| Batch 3000/9812 | Loss 1.9348 | Accuracy 61.57\n",
      "Epoch 1| Batch 3200/9812 | Loss 0.0263 | Accuracy 61.91\n",
      "Epoch 1| Batch 3400/9812 | Loss 0.0325 | Accuracy 62.09\n",
      "Epoch 1| Batch 3600/9812 | Loss 1.3126 | Accuracy 62.25\n",
      "Epoch 1| Batch 3800/9812 | Loss 1.2230 | Accuracy 62.42\n",
      "Epoch 1| Batch 4000/9812 | Loss 4.3586 | Accuracy 62.75\n",
      "Epoch 1| Batch 4200/9812 | Loss 1.2277 | Accuracy 63.10\n",
      "Epoch 1| Batch 4400/9812 | Loss 3.1622 | Accuracy 63.30\n",
      "Epoch 1| Batch 4600/9812 | Loss 2.1368 | Accuracy 63.50\n",
      "Epoch 1| Batch 4800/9812 | Loss 4.4028 | Accuracy 63.90\n",
      "Epoch 1| Batch 5000/9812 | Loss 0.0685 | Accuracy 64.02\n",
      "Epoch 1| Batch 5200/9812 | Loss 0.0022 | Accuracy 64.40\n",
      "Epoch 1| Batch 5400/9812 | Loss 0.7741 | Accuracy 64.56\n",
      "Epoch 1| Batch 5600/9812 | Loss 0.3911 | Accuracy 64.68\n",
      "Epoch 1| Batch 5800/9812 | Loss 2.4101 | Accuracy 64.78\n",
      "Epoch 1| Batch 6000/9812 | Loss 1.3759 | Accuracy 65.02\n",
      "Epoch 1| Batch 6200/9812 | Loss 0.0222 | Accuracy 65.32\n",
      "Epoch 1| Batch 6400/9812 | Loss 0.2802 | Accuracy 65.41\n",
      "Epoch 1| Batch 6600/9812 | Loss 0.0191 | Accuracy 65.59\n",
      "Epoch 1| Batch 6800/9812 | Loss 1.7084 | Accuracy 65.74\n",
      "Epoch 1| Batch 7000/9812 | Loss 0.0004 | Accuracy 65.96\n",
      "Epoch 1| Batch 7200/9812 | Loss 0.0610 | Accuracy 66.00\n",
      "Epoch 1| Batch 7400/9812 | Loss 0.0371 | Accuracy 66.00\n",
      "Epoch 1| Batch 7600/9812 | Loss 0.0653 | Accuracy 66.25\n",
      "Epoch 1| Batch 7800/9812 | Loss 1.1285 | Accuracy 66.44\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 2.4905 | Accuracy 66.39\n",
      "Epoch 2| Batch 200/9812 | Loss 0.4147 | Accuracy 83.50\n",
      "Epoch 2| Batch 400/9812 | Loss 2.8836 | Accuracy 82.50\n",
      "Epoch 2| Batch 600/9812 | Loss 0.5184 | Accuracy 80.33\n",
      "Epoch 2| Batch 800/9812 | Loss 1.4979 | Accuracy 80.38\n",
      "Epoch 2| Batch 1000/9812 | Loss 0.0030 | Accuracy 80.50\n",
      "Epoch 2| Batch 1200/9812 | Loss 0.1379 | Accuracy 81.50\n",
      "Epoch 2| Batch 1400/9812 | Loss 0.0216 | Accuracy 81.07\n",
      "Epoch 2| Batch 1600/9812 | Loss 0.0224 | Accuracy 81.00\n",
      "Epoch 2| Batch 1800/9812 | Loss 0.1203 | Accuracy 80.78\n",
      "Epoch 2| Batch 2000/9812 | Loss 0.1110 | Accuracy 80.90\n",
      "Epoch 2| Batch 2200/9812 | Loss 0.1642 | Accuracy 80.91\n",
      "Epoch 2| Batch 2400/9812 | Loss 0.0028 | Accuracy 80.75\n",
      "Epoch 2| Batch 2600/9812 | Loss 0.0001 | Accuracy 80.58\n",
      "Epoch 2| Batch 2800/9812 | Loss 0.9054 | Accuracy 80.75\n",
      "Epoch 2| Batch 3000/9812 | Loss 0.0435 | Accuracy 80.73\n",
      "Epoch 2| Batch 3200/9812 | Loss 0.0000 | Accuracy 80.75\n",
      "Epoch 2| Batch 3400/9812 | Loss 0.1764 | Accuracy 80.68\n",
      "Epoch 2| Batch 3600/9812 | Loss 0.0396 | Accuracy 80.61\n",
      "Epoch 2| Batch 3800/9812 | Loss 0.1136 | Accuracy 80.63\n",
      "Epoch 2| Batch 4000/9812 | Loss 0.0017 | Accuracy 80.53\n",
      "Epoch 2| Batch 4200/9812 | Loss 0.6508 | Accuracy 80.31\n",
      "Epoch 2| Batch 4400/9812 | Loss 0.0773 | Accuracy 80.36\n",
      "Epoch 2| Batch 4600/9812 | Loss 0.0007 | Accuracy 80.46\n",
      "Epoch 2| Batch 4800/9812 | Loss 0.0254 | Accuracy 80.79\n",
      "Epoch 2| Batch 5000/9812 | Loss 0.1011 | Accuracy 80.68\n",
      "Epoch 2| Batch 5200/9812 | Loss 0.0883 | Accuracy 80.56\n",
      "Epoch 2| Batch 5400/9812 | Loss 1.0532 | Accuracy 80.57\n",
      "Epoch 2| Batch 5600/9812 | Loss 0.7908 | Accuracy 80.71\n",
      "Epoch 2| Batch 5800/9812 | Loss 0.9581 | Accuracy 80.88\n",
      "Epoch 2| Batch 6000/9812 | Loss 0.7883 | Accuracy 81.02\n",
      "Epoch 2| Batch 6200/9812 | Loss 0.0001 | Accuracy 81.06\n",
      "Epoch 2| Batch 6400/9812 | Loss 0.0180 | Accuracy 81.05\n",
      "Epoch 2| Batch 6600/9812 | Loss 0.0004 | Accuracy 81.18\n",
      "Epoch 2| Batch 6800/9812 | Loss 0.0068 | Accuracy 81.24\n",
      "Epoch 2| Batch 7000/9812 | Loss 0.0219 | Accuracy 81.39\n",
      "Epoch 2| Batch 7200/9812 | Loss 3.2600 | Accuracy 81.24\n",
      "Epoch 2| Batch 7400/9812 | Loss 0.2813 | Accuracy 81.31\n",
      "Epoch 2| Batch 7600/9812 | Loss 0.2560 | Accuracy 81.57\n",
      "Epoch 2| Batch 7800/9812 | Loss 0.0052 | Accuracy 81.46\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 0.0184 | Accuracy 81.41\n",
      "Epoch 3| Batch 200/9812 | Loss 0.0008 | Accuracy 90.00\n",
      "Epoch 3| Batch 400/9812 | Loss 0.0422 | Accuracy 90.00\n",
      "Epoch 3| Batch 600/9812 | Loss 0.0192 | Accuracy 90.67\n",
      "Epoch 3| Batch 800/9812 | Loss 0.0001 | Accuracy 91.00\n",
      "Epoch 3| Batch 1000/9812 | Loss 3.4536 | Accuracy 90.40\n",
      "Epoch 3| Batch 1200/9812 | Loss 0.0209 | Accuracy 90.50\n",
      "Epoch 3| Batch 1400/9812 | Loss 0.4814 | Accuracy 90.36\n",
      "Epoch 3| Batch 1600/9812 | Loss 0.0010 | Accuracy 89.75\n",
      "Epoch 3| Batch 1800/9812 | Loss 0.0150 | Accuracy 89.78\n",
      "Epoch 3| Batch 2000/9812 | Loss 2.0805 | Accuracy 89.90\n",
      "Epoch 3| Batch 2200/9812 | Loss 0.1236 | Accuracy 89.55\n",
      "Epoch 3| Batch 2400/9812 | Loss 0.0014 | Accuracy 89.62\n",
      "Epoch 3| Batch 2600/9812 | Loss 0.0182 | Accuracy 89.31\n",
      "Epoch 3| Batch 2800/9812 | Loss 0.0000 | Accuracy 89.32\n",
      "Epoch 3| Batch 3000/9812 | Loss 0.1820 | Accuracy 89.17\n",
      "Epoch 3| Batch 3200/9812 | Loss 0.0893 | Accuracy 89.16\n",
      "Epoch 3| Batch 3400/9812 | Loss 0.0269 | Accuracy 89.06\n",
      "Epoch 3| Batch 3600/9812 | Loss 0.3817 | Accuracy 89.08\n",
      "Epoch 3| Batch 3800/9812 | Loss 0.0000 | Accuracy 89.05\n",
      "Epoch 3| Batch 4000/9812 | Loss 0.6645 | Accuracy 89.08\n",
      "Epoch 3| Batch 4200/9812 | Loss 0.0081 | Accuracy 88.98\n",
      "Epoch 3| Batch 4400/9812 | Loss 0.0594 | Accuracy 89.07\n",
      "Epoch 3| Batch 4600/9812 | Loss 0.0003 | Accuracy 89.22\n",
      "Epoch 3| Batch 4800/9812 | Loss 0.0415 | Accuracy 89.23\n",
      "Epoch 3| Batch 5000/9812 | Loss 0.0000 | Accuracy 89.38\n",
      "Epoch 3| Batch 5200/9812 | Loss 0.0174 | Accuracy 89.37\n",
      "Epoch 3| Batch 5400/9812 | Loss 0.0067 | Accuracy 89.37\n",
      "Epoch 3| Batch 5600/9812 | Loss 2.5843 | Accuracy 89.25\n",
      "Epoch 3| Batch 5800/9812 | Loss 1.3531 | Accuracy 89.12\n",
      "Epoch 3| Batch 6000/9812 | Loss 0.0024 | Accuracy 89.20\n",
      "Epoch 3| Batch 6200/9812 | Loss 0.0000 | Accuracy 89.21\n",
      "Epoch 3| Batch 6400/9812 | Loss 1.0022 | Accuracy 89.25\n",
      "Epoch 3| Batch 6600/9812 | Loss 0.6603 | Accuracy 89.20\n",
      "Epoch 3| Batch 6800/9812 | Loss 0.0299 | Accuracy 89.16\n",
      "Epoch 3| Batch 7000/9812 | Loss 0.0260 | Accuracy 89.13\n",
      "Epoch 3| Batch 7200/9812 | Loss 0.0000 | Accuracy 89.14\n",
      "Epoch 3| Batch 7400/9812 | Loss 0.6948 | Accuracy 89.12\n",
      "Epoch 3| Batch 7600/9812 | Loss 3.6652 | Accuracy 89.12\n",
      "Epoch 3| Batch 7800/9812 | Loss 4.8178 | Accuracy 89.09\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 0.0002 | Accuracy 89.08\n",
      "Epoch 4| Batch 200/9812 | Loss 0.0257 | Accuracy 94.00\n",
      "Epoch 4| Batch 400/9812 | Loss 0.0001 | Accuracy 94.50\n",
      "Epoch 4| Batch 600/9812 | Loss 0.0000 | Accuracy 94.33\n",
      "Epoch 4| Batch 800/9812 | Loss 0.0000 | Accuracy 93.38\n",
      "Epoch 4| Batch 1000/9812 | Loss 0.0011 | Accuracy 93.80\n",
      "Epoch 4| Batch 1200/9812 | Loss 0.1510 | Accuracy 93.83\n",
      "Epoch 4| Batch 1400/9812 | Loss 0.0001 | Accuracy 93.29\n",
      "Epoch 4| Batch 1600/9812 | Loss 0.0301 | Accuracy 93.56\n",
      "Epoch 4| Batch 1800/9812 | Loss 0.0000 | Accuracy 93.67\n",
      "Epoch 4| Batch 2000/9812 | Loss 0.0000 | Accuracy 93.80\n",
      "Epoch 4| Batch 2200/9812 | Loss 0.0002 | Accuracy 93.41\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.0024 | Accuracy 93.54\n",
      "Epoch 4| Batch 2600/9812 | Loss 0.0004 | Accuracy 93.54\n",
      "Epoch 4| Batch 2800/9812 | Loss 0.1048 | Accuracy 93.50\n",
      "Epoch 4| Batch 3000/9812 | Loss 0.0003 | Accuracy 93.40\n",
      "Epoch 4| Batch 3200/9812 | Loss 0.0026 | Accuracy 93.50\n",
      "Epoch 4| Batch 3400/9812 | Loss 0.0002 | Accuracy 93.24\n",
      "Epoch 4| Batch 3600/9812 | Loss 0.0006 | Accuracy 93.19\n",
      "Epoch 4| Batch 3800/9812 | Loss 0.0631 | Accuracy 93.29\n",
      "Epoch 4| Batch 4000/9812 | Loss 0.0007 | Accuracy 93.35\n",
      "Epoch 4| Batch 4200/9812 | Loss 0.0073 | Accuracy 93.29\n",
      "Epoch 4| Batch 4400/9812 | Loss 0.0000 | Accuracy 93.23\n",
      "Epoch 4| Batch 4600/9812 | Loss 0.0000 | Accuracy 93.22\n",
      "Epoch 4| Batch 4800/9812 | Loss 0.0023 | Accuracy 93.06\n",
      "Epoch 4| Batch 5000/9812 | Loss 0.0000 | Accuracy 93.14\n",
      "Epoch 4| Batch 5200/9812 | Loss 0.0000 | Accuracy 93.04\n",
      "Epoch 4| Batch 5400/9812 | Loss 0.0318 | Accuracy 93.09\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.0000 | Accuracy 93.00\n",
      "Epoch 4| Batch 5800/9812 | Loss 0.0007 | Accuracy 93.00\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.0001 | Accuracy 92.95\n",
      "Epoch 4| Batch 6200/9812 | Loss 0.0002 | Accuracy 93.02\n",
      "Epoch 4| Batch 6400/9812 | Loss 0.0305 | Accuracy 93.06\n",
      "Epoch 4| Batch 6600/9812 | Loss 0.0001 | Accuracy 93.03\n",
      "Epoch 4| Batch 6800/9812 | Loss 0.0001 | Accuracy 93.06\n",
      "Epoch 4| Batch 7000/9812 | Loss 0.0004 | Accuracy 93.07\n",
      "Epoch 4| Batch 7200/9812 | Loss 0.2753 | Accuracy 92.99\n",
      "Epoch 4| Batch 7400/9812 | Loss 2.7431 | Accuracy 93.00\n",
      "Epoch 4| Batch 7600/9812 | Loss 0.0013 | Accuracy 93.07\n",
      "Epoch 4| Batch 7800/9812 | Loss 0.0012 | Accuracy 93.14\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 0.0001 | Accuracy 93.15\n",
      "Epoch 5| Batch 200/9812 | Loss 0.0000 | Accuracy 97.50\n",
      "Epoch 5| Batch 400/9812 | Loss 0.0052 | Accuracy 97.25\n",
      "Epoch 5| Batch 600/9812 | Loss 0.8727 | Accuracy 97.00\n",
      "Epoch 5| Batch 800/9812 | Loss 0.0029 | Accuracy 96.50\n",
      "Epoch 5| Batch 1000/9812 | Loss 0.0041 | Accuracy 96.60\n",
      "Epoch 5| Batch 1200/9812 | Loss 0.0000 | Accuracy 96.17\n",
      "Epoch 5| Batch 1400/9812 | Loss 0.0161 | Accuracy 95.43\n",
      "Epoch 5| Batch 1600/9812 | Loss 0.0004 | Accuracy 95.62\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.0002 | Accuracy 96.00\n",
      "Epoch 5| Batch 2000/9812 | Loss 0.0000 | Accuracy 96.05\n",
      "Epoch 5| Batch 2200/9812 | Loss 0.9852 | Accuracy 96.09\n",
      "Epoch 5| Batch 2400/9812 | Loss 0.0000 | Accuracy 95.92\n",
      "Epoch 5| Batch 2600/9812 | Loss 0.0985 | Accuracy 96.00\n",
      "Epoch 5| Batch 2800/9812 | Loss 0.0071 | Accuracy 95.93\n",
      "Epoch 5| Batch 3000/9812 | Loss 0.0000 | Accuracy 95.90\n",
      "Epoch 5| Batch 3200/9812 | Loss 0.0217 | Accuracy 95.72\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.0002 | Accuracy 95.59\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.0000 | Accuracy 95.47\n",
      "Epoch 5| Batch 3800/9812 | Loss 0.0000 | Accuracy 95.50\n",
      "Epoch 5| Batch 4000/9812 | Loss 0.0080 | Accuracy 95.47\n",
      "Epoch 5| Batch 4200/9812 | Loss 0.0002 | Accuracy 95.57\n",
      "Epoch 5| Batch 4400/9812 | Loss 4.1035 | Accuracy 95.61\n",
      "Epoch 5| Batch 4600/9812 | Loss 0.0083 | Accuracy 95.54\n",
      "Epoch 5| Batch 4800/9812 | Loss 0.0066 | Accuracy 95.46\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.0000 | Accuracy 95.38\n",
      "Epoch 5| Batch 5200/9812 | Loss 0.0000 | Accuracy 95.42\n",
      "Epoch 5| Batch 5400/9812 | Loss 0.0009 | Accuracy 95.46\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.0000 | Accuracy 95.45\n",
      "Epoch 5| Batch 5800/9812 | Loss 0.0000 | Accuracy 95.43\n",
      "Epoch 5| Batch 6000/9812 | Loss 0.0009 | Accuracy 95.48\n",
      "Epoch 5| Batch 6200/9812 | Loss 0.0287 | Accuracy 95.50\n",
      "Epoch 5| Batch 6400/9812 | Loss 0.0004 | Accuracy 95.45\n",
      "Epoch 5| Batch 6600/9812 | Loss 0.0000 | Accuracy 95.38\n",
      "Epoch 5| Batch 6800/9812 | Loss 0.0000 | Accuracy 95.43\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.0000 | Accuracy 95.46\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.0063 | Accuracy 95.42\n",
      "Epoch 5| Batch 7400/9812 | Loss 0.0000 | Accuracy 95.42\n",
      "Epoch 5| Batch 7600/9812 | Loss 0.2526 | Accuracy 95.47\n",
      "Epoch 5| Batch 7800/9812 | Loss 0.0005 | Accuracy 95.45\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 0.0000 | Accuracy 95.43\n",
      "Epoch 6| Batch 200/9812 | Loss 0.0002 | Accuracy 96.00\n",
      "Epoch 6| Batch 400/9812 | Loss 0.0000 | Accuracy 97.00\n",
      "Epoch 6| Batch 600/9812 | Loss 2.2217 | Accuracy 96.83\n",
      "Epoch 6| Batch 800/9812 | Loss 0.0000 | Accuracy 97.12\n",
      "Epoch 6| Batch 1000/9812 | Loss 0.0001 | Accuracy 96.90\n",
      "Epoch 6| Batch 1200/9812 | Loss 0.0398 | Accuracy 96.67\n",
      "Epoch 6| Batch 1400/9812 | Loss 4.4993 | Accuracy 96.07\n",
      "Epoch 6| Batch 1600/9812 | Loss 0.0004 | Accuracy 96.31\n",
      "Epoch 6| Batch 1800/9812 | Loss 0.0011 | Accuracy 96.28\n",
      "Epoch 6| Batch 2000/9812 | Loss 0.0000 | Accuracy 96.50\n",
      "Epoch 6| Batch 2200/9812 | Loss 0.0214 | Accuracy 96.23\n",
      "Epoch 6| Batch 2400/9812 | Loss 0.0069 | Accuracy 96.21\n",
      "Epoch 6| Batch 2600/9812 | Loss 0.0008 | Accuracy 96.27\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.1342 | Accuracy 96.29\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.0005 | Accuracy 96.37\n",
      "Epoch 6| Batch 3200/9812 | Loss 0.0000 | Accuracy 96.31\n",
      "Epoch 6| Batch 3400/9812 | Loss 0.0000 | Accuracy 96.32\n",
      "Epoch 6| Batch 3600/9812 | Loss 0.0002 | Accuracy 96.25\n",
      "Epoch 6| Batch 3800/9812 | Loss 0.0090 | Accuracy 96.32\n",
      "Epoch 6| Batch 4000/9812 | Loss 0.0001 | Accuracy 96.25\n",
      "Epoch 6| Batch 4200/9812 | Loss 0.0000 | Accuracy 96.21\n",
      "Epoch 6| Batch 4400/9812 | Loss 0.0000 | Accuracy 96.16\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.0010 | Accuracy 96.26\n",
      "Epoch 6| Batch 4800/9812 | Loss 0.0000 | Accuracy 96.17\n",
      "Epoch 6| Batch 5000/9812 | Loss 0.0000 | Accuracy 96.28\n",
      "Epoch 6| Batch 5200/9812 | Loss 0.0165 | Accuracy 96.21\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.0000 | Accuracy 96.20\n",
      "Epoch 6| Batch 5600/9812 | Loss 0.0000 | Accuracy 96.18\n",
      "Epoch 6| Batch 5800/9812 | Loss 0.0000 | Accuracy 96.24\n",
      "Epoch 6| Batch 6000/9812 | Loss 0.0130 | Accuracy 96.25\n",
      "Epoch 6| Batch 6200/9812 | Loss 0.0000 | Accuracy 96.15\n",
      "Epoch 6| Batch 6400/9812 | Loss 0.0228 | Accuracy 96.16\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.0000 | Accuracy 96.14\n",
      "Epoch 6| Batch 6800/9812 | Loss 0.0076 | Accuracy 96.15\n",
      "Epoch 6| Batch 7000/9812 | Loss 0.6524 | Accuracy 96.16\n",
      "Epoch 6| Batch 7200/9812 | Loss 0.2727 | Accuracy 96.22\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.0001 | Accuracy 96.27\n",
      "Epoch 6| Batch 7600/9812 | Loss 0.0000 | Accuracy 96.33\n",
      "Epoch 6| Batch 7800/9812 | Loss 0.0000 | Accuracy 96.28\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 0.2138 | Accuracy 96.27\n",
      "Epoch 7| Batch 200/9812 | Loss 0.0000 | Accuracy 99.50\n",
      "Epoch 7| Batch 400/9812 | Loss 0.0000 | Accuracy 98.50\n",
      "Epoch 7| Batch 600/9812 | Loss 0.0000 | Accuracy 98.33\n",
      "Epoch 7| Batch 800/9812 | Loss 0.0000 | Accuracy 97.62\n",
      "Epoch 7| Batch 1000/9812 | Loss 0.0000 | Accuracy 97.20\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.0010 | Accuracy 97.25\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.6192 | Accuracy 97.36\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.0005 | Accuracy 97.44\n",
      "Epoch 7| Batch 1800/9812 | Loss 0.0116 | Accuracy 97.17\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0001 | Accuracy 97.10\n",
      "Epoch 7| Batch 2200/9812 | Loss 0.0228 | Accuracy 97.18\n",
      "Epoch 7| Batch 2400/9812 | Loss 0.0000 | Accuracy 97.21\n",
      "Epoch 7| Batch 2600/9812 | Loss 1.1234 | Accuracy 97.08\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.0000 | Accuracy 96.93\n",
      "Epoch 7| Batch 3000/9812 | Loss 0.0000 | Accuracy 96.97\n",
      "Epoch 7| Batch 3200/9812 | Loss 0.0013 | Accuracy 96.97\n",
      "Epoch 7| Batch 3400/9812 | Loss 0.0020 | Accuracy 96.97\n",
      "Epoch 7| Batch 3600/9812 | Loss 0.0000 | Accuracy 97.06\n",
      "Epoch 7| Batch 3800/9812 | Loss 0.0008 | Accuracy 97.08\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.0109 | Accuracy 96.97\n",
      "Epoch 7| Batch 4200/9812 | Loss 0.0209 | Accuracy 96.88\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.0157 | Accuracy 96.93\n",
      "Epoch 7| Batch 4600/9812 | Loss 0.0000 | Accuracy 96.93\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.2065 | Accuracy 97.00\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.0000 | Accuracy 96.92\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.0000 | Accuracy 96.88\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.0000 | Accuracy 96.91\n",
      "Epoch 7| Batch 5600/9812 | Loss 0.0000 | Accuracy 96.82\n",
      "Epoch 7| Batch 5800/9812 | Loss 0.0007 | Accuracy 96.81\n",
      "Epoch 7| Batch 6000/9812 | Loss 0.0000 | Accuracy 96.78\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.0005 | Accuracy 96.73\n",
      "Epoch 7| Batch 6400/9812 | Loss 0.0098 | Accuracy 96.66\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.0002 | Accuracy 96.70\n",
      "Epoch 7| Batch 6800/9812 | Loss 0.0000 | Accuracy 96.69\n",
      "Epoch 7| Batch 7000/9812 | Loss 0.0012 | Accuracy 96.64\n",
      "Epoch 7| Batch 7200/9812 | Loss 0.0101 | Accuracy 96.67\n",
      "Epoch 7| Batch 7400/9812 | Loss 0.0000 | Accuracy 96.69\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.0000 | Accuracy 96.75\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.0138 | Accuracy 96.78\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 2.3841 | Accuracy 96.78\n",
      "Epoch 8| Batch 200/9812 | Loss 0.0016 | Accuracy 98.50\n",
      "Epoch 8| Batch 400/9812 | Loss 0.0001 | Accuracy 99.00\n",
      "Epoch 8| Batch 600/9812 | Loss 0.0000 | Accuracy 99.33\n",
      "Epoch 8| Batch 800/9812 | Loss 0.0000 | Accuracy 98.88\n",
      "Epoch 8| Batch 1000/9812 | Loss 0.0022 | Accuracy 98.70\n",
      "Epoch 8| Batch 1200/9812 | Loss 0.0000 | Accuracy 98.67\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.0006 | Accuracy 98.79\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.0050 | Accuracy 98.88\n",
      "Epoch 8| Batch 1800/9812 | Loss 0.0000 | Accuracy 98.67\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.0000 | Accuracy 98.30\n",
      "Epoch 8| Batch 2200/9812 | Loss 0.0016 | Accuracy 98.14\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0000 | Accuracy 98.17\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.0001 | Accuracy 98.12\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.0000 | Accuracy 98.11\n",
      "Epoch 8| Batch 3000/9812 | Loss 0.0000 | Accuracy 98.03\n",
      "Epoch 8| Batch 3200/9812 | Loss 0.0331 | Accuracy 97.97\n",
      "Epoch 8| Batch 3400/9812 | Loss 0.0000 | Accuracy 98.00\n",
      "Epoch 8| Batch 3600/9812 | Loss 0.0007 | Accuracy 98.11\n",
      "Epoch 8| Batch 3800/9812 | Loss 0.0187 | Accuracy 98.03\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.0575 | Accuracy 98.03\n",
      "Epoch 8| Batch 4200/9812 | Loss 0.0000 | Accuracy 97.93\n",
      "Epoch 8| Batch 4400/9812 | Loss 0.1295 | Accuracy 97.95\n",
      "Epoch 8| Batch 4600/9812 | Loss 0.0010 | Accuracy 97.89\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.3442 | Accuracy 97.85\n",
      "Epoch 8| Batch 5000/9812 | Loss 0.0023 | Accuracy 97.80\n",
      "Epoch 8| Batch 5200/9812 | Loss 0.0001 | Accuracy 97.75\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.0000 | Accuracy 97.72\n",
      "Epoch 8| Batch 5600/9812 | Loss 0.0269 | Accuracy 97.61\n",
      "Epoch 8| Batch 5800/9812 | Loss 0.0000 | Accuracy 97.59\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.0000 | Accuracy 97.60\n",
      "Epoch 8| Batch 6200/9812 | Loss 0.0000 | Accuracy 97.58\n",
      "Epoch 8| Batch 6400/9812 | Loss 0.0020 | Accuracy 97.55\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.9279 | Accuracy 97.50\n",
      "Epoch 8| Batch 6800/9812 | Loss 0.0890 | Accuracy 97.41\n",
      "Epoch 8| Batch 7000/9812 | Loss 0.0055 | Accuracy 97.40\n",
      "Epoch 8| Batch 7200/9812 | Loss 0.0004 | Accuracy 97.39\n",
      "Epoch 8| Batch 7400/9812 | Loss 0.0000 | Accuracy 97.43\n",
      "Epoch 8| Batch 7600/9812 | Loss 0.0000 | Accuracy 97.45\n",
      "Epoch 8| Batch 7800/9812 | Loss 0.0000 | Accuracy 97.49\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 0.3959 | Accuracy 97.49\n",
      "Epoch 9| Batch 200/9812 | Loss 0.0000 | Accuracy 100.00\n",
      "Epoch 9| Batch 400/9812 | Loss 0.0000 | Accuracy 99.25\n",
      "Epoch 9| Batch 600/9812 | Loss 0.0000 | Accuracy 99.33\n",
      "Epoch 9| Batch 800/9812 | Loss 0.0000 | Accuracy 99.38\n",
      "Epoch 9| Batch 1000/9812 | Loss 0.0000 | Accuracy 99.30\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.2465 | Accuracy 99.33\n",
      "Epoch 9| Batch 1400/9812 | Loss 0.0004 | Accuracy 99.43\n",
      "Epoch 9| Batch 1600/9812 | Loss 0.0000 | Accuracy 99.44\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.0000 | Accuracy 99.17\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.0784 | Accuracy 99.10\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.0000 | Accuracy 98.86\n",
      "Epoch 9| Batch 2400/9812 | Loss 0.0000 | Accuracy 98.83\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.0000 | Accuracy 98.92\n",
      "Epoch 9| Batch 2800/9812 | Loss 0.0000 | Accuracy 98.96\n",
      "Epoch 9| Batch 3000/9812 | Loss 0.0000 | Accuracy 98.90\n",
      "Epoch 9| Batch 3200/9812 | Loss 0.0000 | Accuracy 98.84\n",
      "Epoch 9| Batch 3400/9812 | Loss 0.0000 | Accuracy 98.71\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0004 | Accuracy 98.69\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.0000 | Accuracy 98.63\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.0002 | Accuracy 98.70\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.0000 | Accuracy 98.64\n",
      "Epoch 9| Batch 4400/9812 | Loss 0.0000 | Accuracy 98.55\n",
      "Epoch 9| Batch 4600/9812 | Loss 0.0000 | Accuracy 98.50\n",
      "Epoch 9| Batch 4800/9812 | Loss 0.0003 | Accuracy 98.48\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.0000 | Accuracy 98.54\n",
      "Epoch 9| Batch 5200/9812 | Loss 0.0026 | Accuracy 98.54\n",
      "Epoch 9| Batch 5400/9812 | Loss 0.0021 | Accuracy 98.46\n",
      "Epoch 9| Batch 5600/9812 | Loss 0.0004 | Accuracy 98.36\n",
      "Epoch 9| Batch 5800/9812 | Loss 0.0003 | Accuracy 98.17\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.0093 | Accuracy 98.13\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0004 | Accuracy 98.18\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.0009 | Accuracy 98.20\n",
      "Epoch 9| Batch 6600/9812 | Loss 0.0005 | Accuracy 98.15\n",
      "Epoch 9| Batch 6800/9812 | Loss 0.0000 | Accuracy 98.18\n",
      "Epoch 9| Batch 7000/9812 | Loss 0.0009 | Accuracy 98.13\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.0075 | Accuracy 98.14\n",
      "Epoch 9| Batch 7400/9812 | Loss 0.0000 | Accuracy 98.14\n",
      "Epoch 9| Batch 7600/9812 | Loss 0.0000 | Accuracy 98.13\n",
      "Epoch 9| Batch 7800/9812 | Loss 0.0000 | Accuracy 98.14\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.0001 | Accuracy 98.15\n"
     ]
    }
   ],
   "source": [
    "## UCF_CNN1D ##\n",
    "h_in, h_out = 30, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 256, 128, 64, 1000, 500\n",
    "CNN_1D = UCF_CNN1D(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion_1D = nn.CrossEntropyLoss()\n",
    "optimizer_1D = optim.Adam(CNN_1D.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "print(\"\\n\\n\\{}\\n\".format(CNN_1D.__class__.__name__ ))\n",
    "\n",
    "## Training the DNN ##\n",
    "CNN_1D.train()\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        optimizer_1D.zero_grad()\n",
    "\n",
    "        y_pred = CNN_1D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = criterion_1D(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer_1D.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 83.70%\n"
     ]
    }
   ],
   "source": [
    "CNN_1D.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = CNN_1D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_CNN2D\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.5393 | Accuracy 1.00\n",
      "Epoch 0| Batch 400/9812 | Loss 4.6083 | Accuracy 1.25\n",
      "Epoch 0| Batch 600/9812 | Loss 4.1054 | Accuracy 1.33\n",
      "Epoch 0| Batch 800/9812 | Loss 4.3296 | Accuracy 1.38\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.4556 | Accuracy 1.30\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.5529 | Accuracy 1.33\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.0666 | Accuracy 1.21\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.3370 | Accuracy 1.19\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.8201 | Accuracy 1.11\n",
      "Epoch 0| Batch 2000/9812 | Loss 4.5090 | Accuracy 1.25\n",
      "Epoch 0| Batch 2200/9812 | Loss 5.9918 | Accuracy 1.27\n",
      "Epoch 0| Batch 2400/9812 | Loss 4.9907 | Accuracy 1.25\n",
      "Epoch 0| Batch 2600/9812 | Loss 4.4234 | Accuracy 1.27\n",
      "Epoch 0| Batch 2800/9812 | Loss 4.8921 | Accuracy 1.32\n",
      "Epoch 0| Batch 3000/9812 | Loss 4.0373 | Accuracy 1.47\n",
      "Epoch 0| Batch 3200/9812 | Loss 4.4098 | Accuracy 1.56\n",
      "Epoch 0| Batch 3400/9812 | Loss 5.2937 | Accuracy 1.62\n",
      "Epoch 0| Batch 3600/9812 | Loss 4.5386 | Accuracy 1.61\n",
      "Epoch 0| Batch 3800/9812 | Loss 4.1071 | Accuracy 1.66\n",
      "Epoch 0| Batch 4000/9812 | Loss 4.9886 | Accuracy 1.68\n",
      "Epoch 0| Batch 4200/9812 | Loss 4.3563 | Accuracy 1.76\n",
      "Epoch 0| Batch 4400/9812 | Loss 4.3656 | Accuracy 1.80\n",
      "Epoch 0| Batch 4600/9812 | Loss 4.4860 | Accuracy 1.98\n",
      "Epoch 0| Batch 4800/9812 | Loss 4.1771 | Accuracy 2.10\n",
      "Epoch 0| Batch 5000/9812 | Loss 5.8062 | Accuracy 2.38\n",
      "Epoch 0| Batch 5200/9812 | Loss 4.0765 | Accuracy 2.65\n",
      "Epoch 0| Batch 5400/9812 | Loss 3.4442 | Accuracy 2.91\n",
      "Epoch 0| Batch 5600/9812 | Loss 1.6139 | Accuracy 3.12\n",
      "Epoch 0| Batch 5800/9812 | Loss 4.5936 | Accuracy 3.28\n",
      "Epoch 0| Batch 6000/9812 | Loss 4.5269 | Accuracy 3.65\n",
      "Epoch 0| Batch 6200/9812 | Loss 2.7016 | Accuracy 3.94\n",
      "Epoch 0| Batch 6400/9812 | Loss 3.7679 | Accuracy 4.17\n",
      "Epoch 0| Batch 6600/9812 | Loss 1.7693 | Accuracy 4.38\n",
      "Epoch 0| Batch 6800/9812 | Loss 4.5177 | Accuracy 4.57\n",
      "Epoch 0| Batch 7000/9812 | Loss 3.9221 | Accuracy 4.79\n",
      "Epoch 0| Batch 7200/9812 | Loss 4.7332 | Accuracy 5.00\n",
      "Epoch 0| Batch 7400/9812 | Loss 3.8327 | Accuracy 5.38\n",
      "Epoch 0| Batch 7600/9812 | Loss 3.0188 | Accuracy 5.66\n",
      "Epoch 0| Batch 7800/9812 | Loss 3.8206 | Accuracy 5.86\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 3.1872 | Accuracy 5.89\n",
      "Epoch 1| Batch 200/9812 | Loss 4.1317 | Accuracy 19.50\n",
      "Epoch 1| Batch 400/9812 | Loss 2.9623 | Accuracy 17.50\n",
      "Epoch 1| Batch 600/9812 | Loss 3.6436 | Accuracy 17.67\n",
      "Epoch 1| Batch 800/9812 | Loss 2.8311 | Accuracy 16.88\n",
      "Epoch 1| Batch 1000/9812 | Loss 4.1494 | Accuracy 17.30\n",
      "Epoch 1| Batch 1200/9812 | Loss 2.7116 | Accuracy 18.08\n",
      "Epoch 1| Batch 1400/9812 | Loss 2.2194 | Accuracy 19.07\n",
      "Epoch 1| Batch 1600/9812 | Loss 1.8415 | Accuracy 19.38\n",
      "Epoch 1| Batch 1800/9812 | Loss 5.0790 | Accuracy 20.11\n",
      "Epoch 1| Batch 2000/9812 | Loss 6.2309 | Accuracy 20.15\n",
      "Epoch 1| Batch 2200/9812 | Loss 7.0248 | Accuracy 20.41\n",
      "Epoch 1| Batch 2400/9812 | Loss 1.3906 | Accuracy 20.08\n",
      "Epoch 1| Batch 2600/9812 | Loss 5.1869 | Accuracy 20.42\n",
      "Epoch 1| Batch 2800/9812 | Loss 2.0158 | Accuracy 20.57\n",
      "Epoch 1| Batch 3000/9812 | Loss 9.3943 | Accuracy 20.67\n",
      "Epoch 1| Batch 3200/9812 | Loss 0.4239 | Accuracy 20.69\n",
      "Epoch 1| Batch 3400/9812 | Loss 2.6195 | Accuracy 20.65\n",
      "Epoch 1| Batch 3600/9812 | Loss 3.7791 | Accuracy 20.83\n",
      "Epoch 1| Batch 3800/9812 | Loss 6.5947 | Accuracy 20.87\n",
      "Epoch 1| Batch 4000/9812 | Loss 2.8360 | Accuracy 21.07\n",
      "Epoch 1| Batch 4200/9812 | Loss 4.1105 | Accuracy 21.24\n",
      "Epoch 1| Batch 4400/9812 | Loss 2.4617 | Accuracy 21.23\n",
      "Epoch 1| Batch 4600/9812 | Loss 1.5065 | Accuracy 21.67\n",
      "Epoch 1| Batch 4800/9812 | Loss 2.8789 | Accuracy 21.75\n",
      "Epoch 1| Batch 5000/9812 | Loss 2.7303 | Accuracy 21.92\n",
      "Epoch 1| Batch 5200/9812 | Loss 2.8479 | Accuracy 21.96\n",
      "Epoch 1| Batch 5400/9812 | Loss 1.0115 | Accuracy 21.98\n",
      "Epoch 1| Batch 5600/9812 | Loss 2.9920 | Accuracy 22.20\n",
      "Epoch 1| Batch 5800/9812 | Loss 2.4951 | Accuracy 22.69\n",
      "Epoch 1| Batch 6000/9812 | Loss 4.9557 | Accuracy 22.98\n",
      "Epoch 1| Batch 6200/9812 | Loss 4.2459 | Accuracy 23.02\n",
      "Epoch 1| Batch 6400/9812 | Loss 0.8665 | Accuracy 23.27\n",
      "Epoch 1| Batch 6600/9812 | Loss 1.5584 | Accuracy 23.24\n",
      "Epoch 1| Batch 6800/9812 | Loss 2.7463 | Accuracy 23.44\n",
      "Epoch 1| Batch 7000/9812 | Loss 1.4724 | Accuracy 23.39\n",
      "Epoch 1| Batch 7200/9812 | Loss 4.0490 | Accuracy 23.64\n",
      "Epoch 1| Batch 7400/9812 | Loss 3.9739 | Accuracy 23.88\n",
      "Epoch 1| Batch 7600/9812 | Loss 2.7855 | Accuracy 24.08\n",
      "Epoch 1| Batch 7800/9812 | Loss 2.1453 | Accuracy 24.12\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 4.4686 | Accuracy 24.14\n",
      "Epoch 2| Batch 200/9812 | Loss 2.3544 | Accuracy 29.00\n",
      "Epoch 2| Batch 400/9812 | Loss 0.6372 | Accuracy 33.50\n",
      "Epoch 2| Batch 600/9812 | Loss 1.5077 | Accuracy 31.50\n",
      "Epoch 2| Batch 800/9812 | Loss 1.4704 | Accuracy 31.00\n",
      "Epoch 2| Batch 1000/9812 | Loss 4.7805 | Accuracy 29.90\n",
      "Epoch 2| Batch 1200/9812 | Loss 5.6766 | Accuracy 30.33\n",
      "Epoch 2| Batch 1400/9812 | Loss 3.1676 | Accuracy 30.29\n",
      "Epoch 2| Batch 1600/9812 | Loss 2.9331 | Accuracy 29.88\n",
      "Epoch 2| Batch 1800/9812 | Loss 4.9984 | Accuracy 30.22\n",
      "Epoch 2| Batch 2000/9812 | Loss 3.4414 | Accuracy 30.70\n",
      "Epoch 2| Batch 2200/9812 | Loss 2.1467 | Accuracy 30.77\n",
      "Epoch 2| Batch 2400/9812 | Loss 0.4421 | Accuracy 30.96\n",
      "Epoch 2| Batch 2600/9812 | Loss 1.5627 | Accuracy 30.88\n",
      "Epoch 2| Batch 2800/9812 | Loss 3.1828 | Accuracy 31.04\n",
      "Epoch 2| Batch 3000/9812 | Loss 2.7148 | Accuracy 31.33\n",
      "Epoch 2| Batch 3200/9812 | Loss 9.8258 | Accuracy 31.47\n",
      "Epoch 2| Batch 3400/9812 | Loss 1.3245 | Accuracy 31.62\n",
      "Epoch 2| Batch 3600/9812 | Loss 1.8644 | Accuracy 31.72\n",
      "Epoch 2| Batch 3800/9812 | Loss 0.6454 | Accuracy 32.08\n",
      "Epoch 2| Batch 4000/9812 | Loss 0.0013 | Accuracy 32.40\n",
      "Epoch 2| Batch 4200/9812 | Loss 0.0381 | Accuracy 32.67\n",
      "Epoch 2| Batch 4400/9812 | Loss 1.7206 | Accuracy 32.57\n",
      "Epoch 2| Batch 4600/9812 | Loss 0.9011 | Accuracy 32.41\n",
      "Epoch 2| Batch 4800/9812 | Loss 1.5318 | Accuracy 32.54\n",
      "Epoch 2| Batch 5000/9812 | Loss 2.8932 | Accuracy 32.84\n",
      "Epoch 2| Batch 5200/9812 | Loss 0.9463 | Accuracy 32.88\n",
      "Epoch 2| Batch 5400/9812 | Loss 1.5459 | Accuracy 33.06\n",
      "Epoch 2| Batch 5600/9812 | Loss 0.4844 | Accuracy 33.04\n",
      "Epoch 2| Batch 5800/9812 | Loss 1.8991 | Accuracy 33.26\n",
      "Epoch 2| Batch 6000/9812 | Loss 3.1917 | Accuracy 33.22\n",
      "Epoch 2| Batch 6200/9812 | Loss 4.5342 | Accuracy 33.23\n",
      "Epoch 2| Batch 6400/9812 | Loss 4.1702 | Accuracy 33.42\n",
      "Epoch 2| Batch 6600/9812 | Loss 2.5648 | Accuracy 33.50\n",
      "Epoch 2| Batch 6800/9812 | Loss 2.2490 | Accuracy 33.65\n",
      "Epoch 2| Batch 7000/9812 | Loss 0.0008 | Accuracy 33.69\n",
      "Epoch 2| Batch 7200/9812 | Loss 4.5946 | Accuracy 33.79\n",
      "Epoch 2| Batch 7400/9812 | Loss 0.1000 | Accuracy 33.97\n",
      "Epoch 2| Batch 7600/9812 | Loss 0.4518 | Accuracy 33.97\n",
      "Epoch 2| Batch 7800/9812 | Loss 0.5516 | Accuracy 34.05\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 3.6958 | Accuracy 34.06\n",
      "Epoch 3| Batch 200/9812 | Loss 2.8250 | Accuracy 36.00\n",
      "Epoch 3| Batch 400/9812 | Loss 2.4186 | Accuracy 39.00\n",
      "Epoch 3| Batch 600/9812 | Loss 0.8083 | Accuracy 40.33\n",
      "Epoch 3| Batch 800/9812 | Loss 0.2769 | Accuracy 41.50\n",
      "Epoch 3| Batch 1000/9812 | Loss 6.0894 | Accuracy 42.50\n",
      "Epoch 3| Batch 1200/9812 | Loss 2.3086 | Accuracy 41.17\n",
      "Epoch 3| Batch 1400/9812 | Loss 0.2222 | Accuracy 41.14\n",
      "Epoch 3| Batch 1600/9812 | Loss 0.5398 | Accuracy 40.50\n",
      "Epoch 3| Batch 1800/9812 | Loss 5.1654 | Accuracy 40.28\n",
      "Epoch 3| Batch 2000/9812 | Loss 3.2450 | Accuracy 40.20\n",
      "Epoch 3| Batch 2200/9812 | Loss 4.0776 | Accuracy 40.36\n",
      "Epoch 3| Batch 2400/9812 | Loss 1.4953 | Accuracy 40.46\n",
      "Epoch 3| Batch 2600/9812 | Loss 4.7132 | Accuracy 40.65\n",
      "Epoch 3| Batch 2800/9812 | Loss 4.2037 | Accuracy 40.50\n",
      "Epoch 3| Batch 3000/9812 | Loss 3.2321 | Accuracy 40.37\n",
      "Epoch 3| Batch 3200/9812 | Loss 0.8933 | Accuracy 40.41\n",
      "Epoch 3| Batch 3400/9812 | Loss 0.1446 | Accuracy 40.56\n",
      "Epoch 3| Batch 3600/9812 | Loss 3.9254 | Accuracy 40.28\n",
      "Epoch 3| Batch 3800/9812 | Loss 2.4806 | Accuracy 40.16\n",
      "Epoch 3| Batch 4000/9812 | Loss 3.7045 | Accuracy 40.05\n",
      "Epoch 3| Batch 4200/9812 | Loss 1.5020 | Accuracy 40.36\n",
      "Epoch 3| Batch 4400/9812 | Loss 0.2574 | Accuracy 40.30\n",
      "Epoch 3| Batch 4600/9812 | Loss 0.0279 | Accuracy 40.20\n",
      "Epoch 3| Batch 4800/9812 | Loss 3.5672 | Accuracy 40.02\n",
      "Epoch 3| Batch 5000/9812 | Loss 2.8256 | Accuracy 40.06\n",
      "Epoch 3| Batch 5200/9812 | Loss 0.3334 | Accuracy 40.23\n",
      "Epoch 3| Batch 5400/9812 | Loss 3.7212 | Accuracy 40.13\n",
      "Epoch 3| Batch 5600/9812 | Loss 2.8691 | Accuracy 40.30\n",
      "Epoch 3| Batch 5800/9812 | Loss 2.3291 | Accuracy 40.16\n",
      "Epoch 3| Batch 6000/9812 | Loss 2.7329 | Accuracy 40.25\n",
      "Epoch 3| Batch 6200/9812 | Loss 3.2150 | Accuracy 40.45\n",
      "Epoch 3| Batch 6400/9812 | Loss 3.2761 | Accuracy 40.53\n",
      "Epoch 3| Batch 6600/9812 | Loss 1.6607 | Accuracy 40.58\n",
      "Epoch 3| Batch 6800/9812 | Loss 5.5237 | Accuracy 40.72\n",
      "Epoch 3| Batch 7000/9812 | Loss 1.3727 | Accuracy 40.84\n",
      "Epoch 3| Batch 7200/9812 | Loss 0.6079 | Accuracy 40.78\n",
      "Epoch 3| Batch 7400/9812 | Loss 2.1725 | Accuracy 40.88\n",
      "Epoch 3| Batch 7600/9812 | Loss 1.2134 | Accuracy 41.04\n",
      "Epoch 3| Batch 7800/9812 | Loss 5.6776 | Accuracy 41.13\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 3.0634 | Accuracy 41.13\n",
      "Epoch 4| Batch 200/9812 | Loss 0.0563 | Accuracy 52.00\n",
      "Epoch 4| Batch 400/9812 | Loss 0.9666 | Accuracy 50.25\n",
      "Epoch 4| Batch 600/9812 | Loss 2.1271 | Accuracy 48.50\n",
      "Epoch 4| Batch 800/9812 | Loss 0.3474 | Accuracy 48.25\n",
      "Epoch 4| Batch 1000/9812 | Loss 3.3291 | Accuracy 47.60\n",
      "Epoch 4| Batch 1200/9812 | Loss 0.8487 | Accuracy 48.33\n",
      "Epoch 4| Batch 1400/9812 | Loss 2.3554 | Accuracy 48.14\n",
      "Epoch 4| Batch 1600/9812 | Loss 1.9452 | Accuracy 47.25\n",
      "Epoch 4| Batch 1800/9812 | Loss 3.0776 | Accuracy 47.06\n",
      "Epoch 4| Batch 2000/9812 | Loss 3.0159 | Accuracy 46.80\n",
      "Epoch 4| Batch 2200/9812 | Loss 3.8893 | Accuracy 46.68\n",
      "Epoch 4| Batch 2400/9812 | Loss 3.1971 | Accuracy 46.75\n",
      "Epoch 4| Batch 2600/9812 | Loss 5.7913 | Accuracy 46.12\n",
      "Epoch 4| Batch 2800/9812 | Loss 2.5042 | Accuracy 45.86\n",
      "Epoch 4| Batch 3000/9812 | Loss 0.0001 | Accuracy 45.90\n",
      "Epoch 4| Batch 3200/9812 | Loss 3.1840 | Accuracy 46.03\n",
      "Epoch 4| Batch 3400/9812 | Loss 2.4079 | Accuracy 45.79\n",
      "Epoch 4| Batch 3600/9812 | Loss 3.1603 | Accuracy 46.03\n",
      "Epoch 4| Batch 3800/9812 | Loss 2.2235 | Accuracy 45.92\n",
      "Epoch 4| Batch 4000/9812 | Loss 0.8310 | Accuracy 46.08\n",
      "Epoch 4| Batch 4200/9812 | Loss 0.0137 | Accuracy 46.19\n",
      "Epoch 4| Batch 4400/9812 | Loss 0.9023 | Accuracy 46.11\n",
      "Epoch 4| Batch 4600/9812 | Loss 1.4630 | Accuracy 46.17\n",
      "Epoch 4| Batch 4800/9812 | Loss 4.3049 | Accuracy 46.33\n",
      "Epoch 4| Batch 5000/9812 | Loss 0.0003 | Accuracy 46.66\n",
      "Epoch 4| Batch 5200/9812 | Loss 1.7645 | Accuracy 46.69\n",
      "Epoch 4| Batch 5400/9812 | Loss 3.7762 | Accuracy 46.67\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.7494 | Accuracy 46.66\n",
      "Epoch 4| Batch 5800/9812 | Loss 2.8447 | Accuracy 46.57\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.5594 | Accuracy 46.70\n",
      "Epoch 4| Batch 6200/9812 | Loss 2.3469 | Accuracy 46.69\n",
      "Epoch 4| Batch 6400/9812 | Loss 1.1609 | Accuracy 46.58\n",
      "Epoch 4| Batch 6600/9812 | Loss 0.0502 | Accuracy 46.52\n",
      "Epoch 4| Batch 6800/9812 | Loss 1.4608 | Accuracy 46.51\n",
      "Epoch 4| Batch 7000/9812 | Loss 0.9627 | Accuracy 46.66\n",
      "Epoch 4| Batch 7200/9812 | Loss 1.9741 | Accuracy 46.40\n",
      "Epoch 4| Batch 7400/9812 | Loss 0.3141 | Accuracy 46.54\n",
      "Epoch 4| Batch 7600/9812 | Loss 0.1017 | Accuracy 46.64\n",
      "Epoch 4| Batch 7800/9812 | Loss 2.8940 | Accuracy 46.71\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 3.3067 | Accuracy 46.71\n",
      "Epoch 5| Batch 200/9812 | Loss 0.7449 | Accuracy 47.00\n",
      "Epoch 5| Batch 400/9812 | Loss 1.7334 | Accuracy 51.00\n",
      "Epoch 5| Batch 600/9812 | Loss 3.1435 | Accuracy 50.17\n",
      "Epoch 5| Batch 800/9812 | Loss 0.0054 | Accuracy 50.50\n",
      "Epoch 5| Batch 1000/9812 | Loss 1.0558 | Accuracy 50.20\n",
      "Epoch 5| Batch 1200/9812 | Loss 0.5883 | Accuracy 49.42\n",
      "Epoch 5| Batch 1400/9812 | Loss 0.0303 | Accuracy 49.50\n",
      "Epoch 5| Batch 1600/9812 | Loss 4.5298 | Accuracy 49.75\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.5193 | Accuracy 50.28\n",
      "Epoch 5| Batch 2000/9812 | Loss 1.0819 | Accuracy 50.20\n",
      "Epoch 5| Batch 2200/9812 | Loss 0.0634 | Accuracy 50.41\n",
      "Epoch 5| Batch 2400/9812 | Loss 1.4489 | Accuracy 50.38\n",
      "Epoch 5| Batch 2600/9812 | Loss 1.8860 | Accuracy 50.54\n",
      "Epoch 5| Batch 2800/9812 | Loss 1.6203 | Accuracy 50.57\n",
      "Epoch 5| Batch 3000/9812 | Loss 0.0008 | Accuracy 50.67\n",
      "Epoch 5| Batch 3200/9812 | Loss 1.1640 | Accuracy 50.75\n",
      "Epoch 5| Batch 3400/9812 | Loss 2.3498 | Accuracy 50.91\n",
      "Epoch 5| Batch 3600/9812 | Loss 2.7231 | Accuracy 50.97\n",
      "Epoch 5| Batch 3800/9812 | Loss 4.1625 | Accuracy 50.74\n",
      "Epoch 5| Batch 4000/9812 | Loss 1.0349 | Accuracy 50.60\n",
      "Epoch 5| Batch 4200/9812 | Loss 5.9285 | Accuracy 50.81\n",
      "Epoch 5| Batch 4400/9812 | Loss 2.5534 | Accuracy 50.84\n",
      "Epoch 5| Batch 4600/9812 | Loss 2.1310 | Accuracy 50.59\n",
      "Epoch 5| Batch 4800/9812 | Loss 0.5544 | Accuracy 50.46\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.4396 | Accuracy 50.56\n",
      "Epoch 5| Batch 5200/9812 | Loss 5.0675 | Accuracy 50.54\n",
      "Epoch 5| Batch 5400/9812 | Loss 6.7611 | Accuracy 50.56\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.3999 | Accuracy 50.46\n",
      "Epoch 5| Batch 5800/9812 | Loss 2.6626 | Accuracy 50.19\n",
      "Epoch 5| Batch 6000/9812 | Loss 0.5591 | Accuracy 50.13\n",
      "Epoch 5| Batch 6200/9812 | Loss 4.4095 | Accuracy 50.19\n",
      "Epoch 5| Batch 6400/9812 | Loss 1.3590 | Accuracy 50.31\n",
      "Epoch 5| Batch 6600/9812 | Loss 0.0895 | Accuracy 50.26\n",
      "Epoch 5| Batch 6800/9812 | Loss 1.1427 | Accuracy 50.19\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.9410 | Accuracy 50.36\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.0074 | Accuracy 50.51\n",
      "Epoch 5| Batch 7400/9812 | Loss 1.3979 | Accuracy 50.59\n",
      "Epoch 5| Batch 7600/9812 | Loss 2.9461 | Accuracy 50.76\n",
      "Epoch 5| Batch 7800/9812 | Loss 4.4612 | Accuracy 50.79\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 1.3017 | Accuracy 50.81\n",
      "Epoch 6| Batch 200/9812 | Loss 0.1839 | Accuracy 61.50\n",
      "Epoch 6| Batch 400/9812 | Loss 1.3504 | Accuracy 59.00\n",
      "Epoch 6| Batch 600/9812 | Loss 3.8009 | Accuracy 58.00\n",
      "Epoch 6| Batch 800/9812 | Loss 0.0058 | Accuracy 55.88\n",
      "Epoch 6| Batch 1000/9812 | Loss 4.1123 | Accuracy 56.40\n",
      "Epoch 6| Batch 1200/9812 | Loss 0.1650 | Accuracy 55.58\n",
      "Epoch 6| Batch 1400/9812 | Loss 2.0568 | Accuracy 56.14\n",
      "Epoch 6| Batch 1600/9812 | Loss 3.3628 | Accuracy 55.75\n",
      "Epoch 6| Batch 1800/9812 | Loss 1.3438 | Accuracy 55.72\n",
      "Epoch 6| Batch 2000/9812 | Loss 2.6453 | Accuracy 55.55\n",
      "Epoch 6| Batch 2200/9812 | Loss 0.0017 | Accuracy 55.45\n",
      "Epoch 6| Batch 2400/9812 | Loss 4.0859 | Accuracy 55.17\n",
      "Epoch 6| Batch 2600/9812 | Loss 0.0000 | Accuracy 54.77\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.0004 | Accuracy 55.43\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.6103 | Accuracy 55.23\n",
      "Epoch 6| Batch 3200/9812 | Loss 1.8343 | Accuracy 55.38\n",
      "Epoch 6| Batch 3400/9812 | Loss 0.9665 | Accuracy 55.74\n",
      "Epoch 6| Batch 3600/9812 | Loss 5.4385 | Accuracy 55.81\n",
      "Epoch 6| Batch 3800/9812 | Loss 0.0404 | Accuracy 55.53\n",
      "Epoch 6| Batch 4000/9812 | Loss 0.1299 | Accuracy 55.40\n",
      "Epoch 6| Batch 4200/9812 | Loss 2.3668 | Accuracy 55.69\n",
      "Epoch 6| Batch 4400/9812 | Loss 3.8017 | Accuracy 55.66\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.2955 | Accuracy 55.59\n",
      "Epoch 6| Batch 4800/9812 | Loss 0.3390 | Accuracy 55.77\n",
      "Epoch 6| Batch 5000/9812 | Loss 5.2884 | Accuracy 55.86\n",
      "Epoch 6| Batch 5200/9812 | Loss 1.8550 | Accuracy 55.81\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.2972 | Accuracy 55.48\n",
      "Epoch 6| Batch 5600/9812 | Loss 2.5277 | Accuracy 55.41\n",
      "Epoch 6| Batch 5800/9812 | Loss 4.0258 | Accuracy 55.52\n",
      "Epoch 6| Batch 6000/9812 | Loss 0.5827 | Accuracy 55.38\n",
      "Epoch 6| Batch 6200/9812 | Loss 0.9660 | Accuracy 55.27\n",
      "Epoch 6| Batch 6400/9812 | Loss 2.1690 | Accuracy 55.25\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.0565 | Accuracy 55.39\n",
      "Epoch 6| Batch 6800/9812 | Loss 0.0801 | Accuracy 55.51\n",
      "Epoch 6| Batch 7000/9812 | Loss 5.7973 | Accuracy 55.49\n",
      "Epoch 6| Batch 7200/9812 | Loss 0.0790 | Accuracy 55.32\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.0835 | Accuracy 55.36\n",
      "Epoch 6| Batch 7600/9812 | Loss 1.2068 | Accuracy 55.36\n",
      "Epoch 6| Batch 7800/9812 | Loss 1.4222 | Accuracy 55.47\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 0.0045 | Accuracy 55.48\n",
      "Epoch 7| Batch 200/9812 | Loss 0.8853 | Accuracy 62.00\n",
      "Epoch 7| Batch 400/9812 | Loss 0.6132 | Accuracy 64.25\n",
      "Epoch 7| Batch 600/9812 | Loss 3.3178 | Accuracy 63.17\n",
      "Epoch 7| Batch 800/9812 | Loss 0.2516 | Accuracy 63.00\n",
      "Epoch 7| Batch 1000/9812 | Loss 5.1516 | Accuracy 62.00\n",
      "Epoch 7| Batch 1200/9812 | Loss 2.0781 | Accuracy 60.67\n",
      "Epoch 7| Batch 1400/9812 | Loss 1.7295 | Accuracy 60.43\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.8825 | Accuracy 60.00\n",
      "Epoch 7| Batch 1800/9812 | Loss 1.3040 | Accuracy 59.39\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0066 | Accuracy 59.05\n",
      "Epoch 7| Batch 2200/9812 | Loss 0.6404 | Accuracy 59.23\n",
      "Epoch 7| Batch 2400/9812 | Loss 3.4927 | Accuracy 59.42\n",
      "Epoch 7| Batch 2600/9812 | Loss 2.2513 | Accuracy 59.31\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.9811 | Accuracy 59.21\n",
      "Epoch 7| Batch 3000/9812 | Loss 0.2646 | Accuracy 58.97\n",
      "Epoch 7| Batch 3200/9812 | Loss 0.0091 | Accuracy 59.25\n",
      "Epoch 7| Batch 3400/9812 | Loss 3.3859 | Accuracy 59.32\n",
      "Epoch 7| Batch 3600/9812 | Loss 3.0529 | Accuracy 59.42\n",
      "Epoch 7| Batch 3800/9812 | Loss 0.0080 | Accuracy 59.29\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.6246 | Accuracy 59.15\n",
      "Epoch 7| Batch 4200/9812 | Loss 0.2299 | Accuracy 59.14\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.0100 | Accuracy 59.39\n",
      "Epoch 7| Batch 4600/9812 | Loss 1.2053 | Accuracy 59.43\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.6240 | Accuracy 59.42\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.8024 | Accuracy 59.38\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.7822 | Accuracy 59.42\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.0525 | Accuracy 59.56\n",
      "Epoch 7| Batch 5600/9812 | Loss 0.9017 | Accuracy 59.48\n",
      "Epoch 7| Batch 5800/9812 | Loss 0.3200 | Accuracy 59.72\n",
      "Epoch 7| Batch 6000/9812 | Loss 0.0002 | Accuracy 59.95\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.8652 | Accuracy 59.98\n",
      "Epoch 7| Batch 6400/9812 | Loss 2.5900 | Accuracy 60.00\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.0349 | Accuracy 60.00\n",
      "Epoch 7| Batch 6800/9812 | Loss 1.8424 | Accuracy 59.78\n",
      "Epoch 7| Batch 7000/9812 | Loss 1.4164 | Accuracy 59.74\n",
      "Epoch 7| Batch 7200/9812 | Loss 2.2624 | Accuracy 59.64\n",
      "Epoch 7| Batch 7400/9812 | Loss 3.6592 | Accuracy 59.46\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.7662 | Accuracy 59.49\n",
      "Epoch 7| Batch 7800/9812 | Loss 4.3751 | Accuracy 59.45\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 0.0781 | Accuracy 59.49\n",
      "Epoch 8| Batch 200/9812 | Loss 0.0264 | Accuracy 66.50\n",
      "Epoch 8| Batch 400/9812 | Loss 0.3705 | Accuracy 64.75\n",
      "Epoch 8| Batch 600/9812 | Loss 0.8068 | Accuracy 64.67\n",
      "Epoch 8| Batch 800/9812 | Loss 0.8759 | Accuracy 63.62\n",
      "Epoch 8| Batch 1000/9812 | Loss 0.1929 | Accuracy 64.70\n",
      "Epoch 8| Batch 1200/9812 | Loss 0.0004 | Accuracy 64.58\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.6338 | Accuracy 63.50\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.0495 | Accuracy 63.00\n",
      "Epoch 8| Batch 1800/9812 | Loss 1.0321 | Accuracy 63.00\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.2945 | Accuracy 63.55\n",
      "Epoch 8| Batch 2200/9812 | Loss 2.3387 | Accuracy 63.59\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0368 | Accuracy 63.67\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.7929 | Accuracy 63.73\n",
      "Epoch 8| Batch 2800/9812 | Loss 3.5568 | Accuracy 63.21\n",
      "Epoch 8| Batch 3000/9812 | Loss 2.5942 | Accuracy 63.43\n",
      "Epoch 8| Batch 3200/9812 | Loss 0.6437 | Accuracy 63.59\n",
      "Epoch 8| Batch 3400/9812 | Loss 1.9667 | Accuracy 63.44\n",
      "Epoch 8| Batch 3600/9812 | Loss 5.6357 | Accuracy 63.19\n",
      "Epoch 8| Batch 3800/9812 | Loss 2.8985 | Accuracy 63.13\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.3908 | Accuracy 63.02\n",
      "Epoch 8| Batch 4200/9812 | Loss 0.1620 | Accuracy 63.05\n",
      "Epoch 8| Batch 4400/9812 | Loss 0.9777 | Accuracy 63.16\n",
      "Epoch 8| Batch 4600/9812 | Loss 0.1727 | Accuracy 63.24\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.4616 | Accuracy 63.29\n",
      "Epoch 8| Batch 5000/9812 | Loss 1.0169 | Accuracy 63.40\n",
      "Epoch 8| Batch 5200/9812 | Loss 0.5764 | Accuracy 63.17\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.3594 | Accuracy 62.96\n",
      "Epoch 8| Batch 5600/9812 | Loss 0.0003 | Accuracy 63.02\n",
      "Epoch 8| Batch 5800/9812 | Loss 0.4061 | Accuracy 63.03\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.5280 | Accuracy 63.02\n",
      "Epoch 8| Batch 6200/9812 | Loss 0.9231 | Accuracy 63.03\n",
      "Epoch 8| Batch 6400/9812 | Loss 0.3151 | Accuracy 63.02\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.0261 | Accuracy 63.00\n",
      "Epoch 8| Batch 6800/9812 | Loss 1.1838 | Accuracy 63.18\n",
      "Epoch 8| Batch 7000/9812 | Loss 3.5051 | Accuracy 63.11\n",
      "Epoch 8| Batch 7200/9812 | Loss 1.4508 | Accuracy 63.19\n",
      "Epoch 8| Batch 7400/9812 | Loss 0.0817 | Accuracy 63.26\n",
      "Epoch 8| Batch 7600/9812 | Loss 0.2889 | Accuracy 63.33\n",
      "Epoch 8| Batch 7800/9812 | Loss 0.1336 | Accuracy 63.22\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 1.3300 | Accuracy 63.21\n",
      "Epoch 9| Batch 200/9812 | Loss 0.7568 | Accuracy 67.00\n",
      "Epoch 9| Batch 400/9812 | Loss 1.2357 | Accuracy 69.00\n",
      "Epoch 9| Batch 600/9812 | Loss 4.2968 | Accuracy 68.33\n",
      "Epoch 9| Batch 800/9812 | Loss 2.6011 | Accuracy 68.88\n",
      "Epoch 9| Batch 1000/9812 | Loss 0.0901 | Accuracy 68.20\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.3154 | Accuracy 68.50\n",
      "Epoch 9| Batch 1400/9812 | Loss 0.7588 | Accuracy 68.21\n",
      "Epoch 9| Batch 1600/9812 | Loss 0.0312 | Accuracy 68.19\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.1375 | Accuracy 67.89\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.9286 | Accuracy 68.20\n",
      "Epoch 9| Batch 2200/9812 | Loss 1.9115 | Accuracy 68.73\n",
      "Epoch 9| Batch 2400/9812 | Loss 1.8241 | Accuracy 68.42\n",
      "Epoch 9| Batch 2600/9812 | Loss 1.6730 | Accuracy 67.85\n",
      "Epoch 9| Batch 2800/9812 | Loss 3.0830 | Accuracy 68.04\n",
      "Epoch 9| Batch 3000/9812 | Loss 1.2977 | Accuracy 68.33\n",
      "Epoch 9| Batch 3200/9812 | Loss 2.4332 | Accuracy 67.94\n",
      "Epoch 9| Batch 3400/9812 | Loss 0.0000 | Accuracy 67.74\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0181 | Accuracy 67.72\n",
      "Epoch 9| Batch 3800/9812 | Loss 1.4676 | Accuracy 67.87\n",
      "Epoch 9| Batch 4000/9812 | Loss 1.5760 | Accuracy 67.90\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.1365 | Accuracy 67.60\n",
      "Epoch 9| Batch 4400/9812 | Loss 1.7706 | Accuracy 67.45\n",
      "Epoch 9| Batch 4600/9812 | Loss 1.5898 | Accuracy 67.09\n",
      "Epoch 9| Batch 4800/9812 | Loss 0.0107 | Accuracy 66.81\n",
      "Epoch 9| Batch 5000/9812 | Loss 2.0304 | Accuracy 66.98\n",
      "Epoch 9| Batch 5200/9812 | Loss 2.7532 | Accuracy 67.15\n",
      "Epoch 9| Batch 5400/9812 | Loss 2.7064 | Accuracy 67.22\n",
      "Epoch 9| Batch 5600/9812 | Loss 1.2167 | Accuracy 67.00\n",
      "Epoch 9| Batch 5800/9812 | Loss 0.3092 | Accuracy 66.93\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.9005 | Accuracy 66.63\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0029 | Accuracy 66.60\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.0458 | Accuracy 66.67\n",
      "Epoch 9| Batch 6600/9812 | Loss 3.6948 | Accuracy 66.55\n",
      "Epoch 9| Batch 6800/9812 | Loss 0.2376 | Accuracy 66.54\n",
      "Epoch 9| Batch 7000/9812 | Loss 0.2886 | Accuracy 66.47\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.0585 | Accuracy 66.56\n",
      "Epoch 9| Batch 7400/9812 | Loss 0.6862 | Accuracy 66.62\n",
      "Epoch 9| Batch 7600/9812 | Loss 0.0099 | Accuracy 66.70\n",
      "Epoch 9| Batch 7800/9812 | Loss 0.2490 | Accuracy 66.72\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.0111 | Accuracy 66.72\n"
     ]
    }
   ],
   "source": [
    "## UCF_CNN2D ##\n",
    "num_of_classes = 101\n",
    "h_in, h_out = 30, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 256, 128, 64, 1000, 500\n",
    "CNN_2D = UCF_CNN2D(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "\n",
    "criterion_2D = nn.CrossEntropyLoss()\n",
    "optimizer_2D = optim.Adam(CNN_2D.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "print(\"\\n\\n\\{}\\n\".format(CNN_2D.__class__.__name__ ))\n",
    "\n",
    "## Training the 2DCNN ##\n",
    "CNN_2D.train()\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        optimizer_2D.zero_grad()\n",
    "\n",
    "        y_pred = CNN_2D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = criterion_2D(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer_2D.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 65.97%\n"
     ]
    }
   ],
   "source": [
    "CNN_2D.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = CNN_2D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UCFdataset(data.Dataset):\n",
    "#     def __init__(self, upper, lower, dir_path): # 'train', 'validation'\n",
    "#         super(UCFdataset, self).__init__()\n",
    "        \n",
    "#         self.file_list, self.y, self.video_names = self.file_load(upper, lower, dir_path)\n",
    "        \n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         x = jpg2np(self.file_list[index]) / 255. # (30, 3, 240, 320)\n",
    "#         self.x_data = torch.from_numpy(x).float()\n",
    "#         self.y_data = torch.from_numpy(self.y[index]).float()\n",
    "#         return self.x_data, self.y_data, self.video_names[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.y.shape[0]\n",
    "    \n",
    "#     def file_load(self, upper, lower, dir_path):\n",
    "#         \"\"\"\n",
    "#         return the input file path list\n",
    "#         \"\"\"\n",
    "#         data_path = []\n",
    "#         video_imgs_path = os.path.join(os.getcwd(), dir_path)\n",
    "#         folders = os.listdir(video_imgs_path)\n",
    "\n",
    "#         frames = {}\n",
    "#         for folder in folders:\n",
    "#             path = os.path.join(video_imgs_path, folder)\n",
    "#             frames[folder] = len(os.listdir(path))\n",
    "\n",
    "#         video_names = []\n",
    "#         for video_name, num_of_frames in zip(list(frames.keys()), list(frames.values())):\n",
    "#             if upper <= num_of_frames and num_of_frames <= lower:\n",
    "#                 video_names.append(video_name)\n",
    "#         video_names = natsort.natsorted(video_names)\n",
    "                \n",
    "#         print(\"Select The number of frames between [%d, %d] of UCF101 Dataset\" %(upper, lower))\n",
    "#         print('The number of selected videos is', len(video_names))\n",
    "\n",
    "\n",
    "#         data_path = [os.path.join(video_imgs_path, video_name) for video_name in video_names]\n",
    "\n",
    "#         labels = []\n",
    "#         for label in video_names:\n",
    "#             loc1 = label.find('_')\n",
    "#             loc2 = loc1 + label[loc1+1:].find('_')\n",
    "#             labels.append(label[loc1+1:loc2+1])\n",
    "#         y, _ = encoder(labels)\n",
    "# #         y = torch.tensor(y, dtype=torch.float) # [N(13320, 1)\n",
    "\n",
    "#         return data_path, y, video_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-tcn",
   "language": "python",
   "name": "ms-tcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
