{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import natsort\n",
    "import bezier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from dataset import encoder\n",
    "from model import get_pretrained_model\n",
    "from dataset import jpg2np, get_loader\n",
    "from torchvision import models\n",
    "from bezier.hazmat.curve_helpers import evaluate_hodograph, get_curvature\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils import data\n",
    "from UCF_dataset import UCFdataset\n",
    "from model import UCF_DNN, UCF_CNN1D, UCF_CNN2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model import UCF_DNN, UCF_CNN1D, UCF_CNN2D\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  2\n"
     ]
    }
   ],
   "source": [
    "cut = 9812\n",
    "upper, lower = 150, 10000\n",
    "GPU_NUM = 2\n",
    "bs=1\n",
    "upper, lower, dir_path = 150, 10000, './ucf_image'\n",
    "datastyle ='minmax' # 'embeddings', 'elementwise', 'minmax', 'std'\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select The number of frames between [150, 10000] of UCF101 Dataset\n",
      "The number of selected videos is 9812\n"
     ]
    }
   ],
   "source": [
    "dataset = UCFdataset(upper, lower, dir_path)\n",
    "loader = data.DataLoader(dataset=dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "num_of_classes = 101\n",
    "epochs=10\n",
    "\n",
    "\n",
    "## Embedding ##\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg_embedding = nn.Sequential(vgg16.features,\n",
    "#                    nn.AdaptiveAvgPool2d((1,1))).to(device)\n",
    "\n",
    "# bs_step = 0\n",
    "# for x, y, video_name in loader:\n",
    "#     if '{}.npy'.format(list(video_name)[0]) in  os.listdir('./ucf_embeddings/'): \n",
    "#         bs_step += bs\n",
    "#         continue\n",
    "#     x, y = x.to(device).squeeze(), y.squeeze().long().to(device)\n",
    "\n",
    "\n",
    "#     ## VGG Embedding ##\n",
    "#     embedding = vgg_embedding(x).squeeze().cpu().detach().numpy() # [30, 512]\n",
    "#     np.save('./ucf_embeddings/{}.npy'.format(list(video_name)[0]), embedding)    \n",
    "#     bs_step += bs\n",
    "#     if bs_step % 10 == 0: \n",
    "#         print(\"Batch {}/{} | {}\".format(bs_step, 9812, video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bezier Approximation and Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9812\n",
      "1000 9812\n",
      "2000 9812\n",
      "3000 9812\n",
      "4000 9812\n",
      "5000 9812\n",
      "6000 9812\n",
      "7000 9812\n",
      "8000 9812\n",
      "9000 9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9812, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load vgg16 embedding data ##\n",
    "video_names = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "for embedding in os.listdir('./ucf_embeddings'):\n",
    "    if embedding == '.ipynb_checkpoints': continue\n",
    "    loc1 = embedding.find('_')\n",
    "    loc2 = loc1 + embedding[loc1+1:].find('_')\n",
    "    labels.append(embedding[loc1+1:loc2+1])\n",
    "    video_names.append(embedding[:-4])\n",
    "  \n",
    "\n",
    "    embeddings.append(np.load('./ucf_embeddings/{}'.format(embedding)))\n",
    "\n",
    "\n",
    "y, _ = encoder(labels)\n",
    "\n",
    "embeddings = np.stack(embeddings)\n",
    "embeddings_pca = embeddings.reshape(embeddings.shape[0]*embeddings.shape[1],-1)\n",
    "\n",
    "## PCA ##\n",
    "pca = PCA(3)\n",
    "embeddings_pca = pca.fit_transform(embeddings_pca).reshape(embeddings.shape[0], embeddings.shape[1],-1)\n",
    "embeddings_pca.shape\n",
    "\n",
    "## Bezier Curve and Curvature ##\n",
    "k = dict()\n",
    "curves = []\n",
    "for i, embedding in enumerate(embeddings_pca):\n",
    "#     print(embedding.shape)\n",
    "    curves.append(bezier.Curve.from_nodes(embedding.T))\n",
    "    kappa = []\n",
    "    for s in range(30):\n",
    "        t = s / 30\n",
    "        tangent_vec = curves[i].evaluate_hodograph(t)\n",
    "        kappa.append(get_curvature(embedding.T, tangent_vec, t))\n",
    "    k[video_names[i]] = kappa\n",
    "    if i % 1000 == 0:\n",
    "        print(i, len(embeddings_pca))\n",
    "\n",
    "K = np.stack(list(k.values())) # [N, 30]\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the kappa vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax scaling and operation\n",
      "[1000 / 9812] video processing!\n",
      "[2000 / 9812] video processing!\n",
      "[3000 / 9812] video processing!\n",
      "[4000 / 9812] video processing!\n",
      "[5000 / 9812] video processing!\n",
      "[6000 / 9812] video processing!\n",
      "[7000 / 9812] video processing!\n",
      "[8000 / 9812] video processing!\n",
      "[9000 / 9812] video processing!\n",
      "(9812, 30, 512) (9812, 30, 3) (9812, 30) torch.Size([9812, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "## Elementwise operation ##\n",
    "if datastyle == 'elementwise':\n",
    "    print('just elementwise operation')\n",
    "    result = list()\n",
    "    for video in range(len(embeddings)):\n",
    "        elementwise = list()\n",
    "        for frame in range(30):\n",
    "            elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "        result.append(np.stack(elementwise))\n",
    "        if (video+1) % 1000 == 0: \n",
    "            print(\"[%d / %d] video processing!\" %(video+1, len(embeddings)))\n",
    "    result = torch.from_numpy(np.stack(result))\n",
    "#     print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)\n",
    "if datastyle == 'minmax': \n",
    "    print('minmax scaling and operation')\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    K = min_max_scaler.fit_transform(K.T).T\n",
    "    result = list()\n",
    "    for video in range(len(embeddings)):\n",
    "        elementwise = list()\n",
    "        for frame in range(30):\n",
    "            elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "        result.append(np.stack(elementwise))\n",
    "        if (video+1) % 1000 == 0: \n",
    "            print(\"[%d / %d] video processing!\" %(video+1, len(embeddings)))\n",
    "    result = torch.from_numpy(np.stack(result))\n",
    "    print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)\n",
    "if datastyle == 'std':\n",
    "    print('standard norm and operation')\n",
    "    standard_scaler = StandardScaler()\n",
    "    K = standard_scaler.fit_transform(K.T).T\n",
    "    result = list()\n",
    "    for video in range(len(embeddings)):\n",
    "        elementwise = list()\n",
    "        for frame in range(30):\n",
    "            elementwise.append(embeddings[video][frame] * K[video][frame])        \n",
    "        result.append(np.stack(elementwise))\n",
    "        if (video+1) % 1000 == 0: \n",
    "            print(\"[%d / %d] video processing!\" %(video+1, len(embeddings)))\n",
    "    result = torch.from_numpy(np.stack(result))\n",
    "#     print(embeddings.shape, embeddings_pca.shape, K.shape, result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data with Train and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax\n",
      "torch.Size([9812, 30, 512]) torch.Size([9812, 1])\n"
     ]
    }
   ],
   "source": [
    "## Data Setting for Action Recognition(Prediction) ##\n",
    "\n",
    "print(datastyle)\n",
    "if datastyle =='embeddings':\n",
    "    X = torch.from_numpy(embeddings)\n",
    "if datastyle == 'elementwise' or 'minmax' or 'std':\n",
    "    X = result\n",
    "Y = torch.from_numpy(y)\n",
    "\n",
    "## train / test split ##\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, shuffle=True, random_state=123)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_DNN\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.8475 | Accuracy 1.50\n",
      "Epoch 0| Batch 400/9812 | Loss 4.6528 | Accuracy 1.25\n",
      "Epoch 0| Batch 600/9812 | Loss 4.2174 | Accuracy 1.33\n",
      "Epoch 0| Batch 800/9812 | Loss 4.6797 | Accuracy 1.25\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.6687 | Accuracy 1.30\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.6380 | Accuracy 1.42\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.6536 | Accuracy 1.29\n",
      "Epoch 0| Batch 1600/9812 | Loss 4.6207 | Accuracy 1.44\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.4115 | Accuracy 1.56\n",
      "Epoch 0| Batch 2000/9812 | Loss 4.5504 | Accuracy 1.70\n",
      "Epoch 0| Batch 2200/9812 | Loss 4.4635 | Accuracy 1.73\n",
      "Epoch 0| Batch 2400/9812 | Loss 4.5632 | Accuracy 1.71\n",
      "Epoch 0| Batch 2600/9812 | Loss 4.7803 | Accuracy 1.73\n",
      "Epoch 0| Batch 2800/9812 | Loss 3.9862 | Accuracy 1.93\n",
      "Epoch 0| Batch 3000/9812 | Loss 1.6538 | Accuracy 2.07\n",
      "Epoch 0| Batch 3200/9812 | Loss 4.6072 | Accuracy 2.25\n",
      "Epoch 0| Batch 3400/9812 | Loss 5.2793 | Accuracy 2.38\n",
      "Epoch 0| Batch 3600/9812 | Loss 4.1778 | Accuracy 2.47\n",
      "Epoch 0| Batch 3800/9812 | Loss 4.1901 | Accuracy 2.68\n",
      "Epoch 0| Batch 4000/9812 | Loss 4.2182 | Accuracy 2.88\n",
      "Epoch 0| Batch 4200/9812 | Loss 5.6977 | Accuracy 3.05\n",
      "Epoch 0| Batch 4400/9812 | Loss 4.2869 | Accuracy 3.20\n",
      "Epoch 0| Batch 4600/9812 | Loss 3.0023 | Accuracy 3.41\n",
      "Epoch 0| Batch 4800/9812 | Loss 3.8497 | Accuracy 3.50\n",
      "Epoch 0| Batch 5000/9812 | Loss 3.6389 | Accuracy 3.48\n",
      "Epoch 0| Batch 5200/9812 | Loss 4.2901 | Accuracy 3.62\n",
      "Epoch 0| Batch 5400/9812 | Loss 4.2240 | Accuracy 3.74\n",
      "Epoch 0| Batch 5600/9812 | Loss 4.5324 | Accuracy 3.86\n",
      "Epoch 0| Batch 5800/9812 | Loss 4.2474 | Accuracy 3.95\n",
      "Epoch 0| Batch 6000/9812 | Loss 4.3499 | Accuracy 4.17\n",
      "Epoch 0| Batch 6200/9812 | Loss 4.7389 | Accuracy 4.32\n",
      "Epoch 0| Batch 6400/9812 | Loss 3.8656 | Accuracy 4.36\n",
      "Epoch 0| Batch 6600/9812 | Loss 3.9033 | Accuracy 4.58\n",
      "Epoch 0| Batch 6800/9812 | Loss 7.7366 | Accuracy 4.63\n",
      "Epoch 0| Batch 7000/9812 | Loss 4.6120 | Accuracy 4.73\n",
      "Epoch 0| Batch 7200/9812 | Loss 3.6389 | Accuracy 4.93\n",
      "Epoch 0| Batch 7400/9812 | Loss 2.0736 | Accuracy 5.14\n",
      "Epoch 0| Batch 7600/9812 | Loss 2.5948 | Accuracy 5.32\n",
      "Epoch 0| Batch 7800/9812 | Loss 3.7323 | Accuracy 5.37\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 3.1704 | Accuracy 5.39\n",
      "Epoch 1| Batch 200/9812 | Loss 3.3290 | Accuracy 9.00\n",
      "Epoch 1| Batch 400/9812 | Loss 3.9260 | Accuracy 10.00\n",
      "Epoch 1| Batch 600/9812 | Loss 3.5872 | Accuracy 11.67\n",
      "Epoch 1| Batch 800/9812 | Loss 0.8889 | Accuracy 11.38\n",
      "Epoch 1| Batch 1000/9812 | Loss 5.5425 | Accuracy 12.10\n",
      "Epoch 1| Batch 1200/9812 | Loss 4.2756 | Accuracy 12.42\n",
      "Epoch 1| Batch 1400/9812 | Loss 6.8131 | Accuracy 12.36\n",
      "Epoch 1| Batch 1600/9812 | Loss 3.7069 | Accuracy 12.38\n",
      "Epoch 1| Batch 1800/9812 | Loss 4.3367 | Accuracy 12.22\n",
      "Epoch 1| Batch 2000/9812 | Loss 3.7457 | Accuracy 11.95\n",
      "Epoch 1| Batch 2200/9812 | Loss 4.5758 | Accuracy 12.05\n",
      "Epoch 1| Batch 2400/9812 | Loss 3.1921 | Accuracy 12.42\n",
      "Epoch 1| Batch 2600/9812 | Loss 4.4831 | Accuracy 12.54\n",
      "Epoch 1| Batch 2800/9812 | Loss 2.3246 | Accuracy 12.75\n",
      "Epoch 1| Batch 3000/9812 | Loss 3.3126 | Accuracy 12.83\n",
      "Epoch 1| Batch 3200/9812 | Loss 3.6988 | Accuracy 12.94\n",
      "Epoch 1| Batch 3400/9812 | Loss 4.9177 | Accuracy 13.35\n",
      "Epoch 1| Batch 3600/9812 | Loss 3.6109 | Accuracy 13.44\n",
      "Epoch 1| Batch 3800/9812 | Loss 1.1675 | Accuracy 13.76\n",
      "Epoch 1| Batch 4000/9812 | Loss 1.6366 | Accuracy 13.75\n",
      "Epoch 1| Batch 4200/9812 | Loss 0.3123 | Accuracy 13.86\n",
      "Epoch 1| Batch 4400/9812 | Loss 5.4871 | Accuracy 13.93\n",
      "Epoch 1| Batch 4600/9812 | Loss 4.4179 | Accuracy 13.93\n",
      "Epoch 1| Batch 4800/9812 | Loss 3.0576 | Accuracy 14.04\n",
      "Epoch 1| Batch 5000/9812 | Loss 3.0517 | Accuracy 14.04\n",
      "Epoch 1| Batch 5200/9812 | Loss 4.9506 | Accuracy 14.06\n",
      "Epoch 1| Batch 5400/9812 | Loss 6.6106 | Accuracy 14.30\n",
      "Epoch 1| Batch 5600/9812 | Loss 3.7561 | Accuracy 14.68\n",
      "Epoch 1| Batch 5800/9812 | Loss 2.8345 | Accuracy 14.78\n",
      "Epoch 1| Batch 6000/9812 | Loss 1.5572 | Accuracy 15.03\n",
      "Epoch 1| Batch 6200/9812 | Loss 3.5568 | Accuracy 15.15\n",
      "Epoch 1| Batch 6400/9812 | Loss 3.9257 | Accuracy 15.19\n",
      "Epoch 1| Batch 6600/9812 | Loss 3.0368 | Accuracy 15.35\n",
      "Epoch 1| Batch 6800/9812 | Loss 3.2382 | Accuracy 15.47\n",
      "Epoch 1| Batch 7000/9812 | Loss 3.9082 | Accuracy 15.53\n",
      "Epoch 1| Batch 7200/9812 | Loss 3.6359 | Accuracy 15.53\n",
      "Epoch 1| Batch 7400/9812 | Loss 3.7388 | Accuracy 15.65\n",
      "Epoch 1| Batch 7600/9812 | Loss 3.4096 | Accuracy 15.72\n",
      "Epoch 1| Batch 7800/9812 | Loss 0.4303 | Accuracy 15.86\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 4.6861 | Accuracy 15.98\n",
      "Epoch 2| Batch 200/9812 | Loss 3.4050 | Accuracy 24.00\n",
      "Epoch 2| Batch 400/9812 | Loss 1.6754 | Accuracy 25.00\n",
      "Epoch 2| Batch 600/9812 | Loss 2.4402 | Accuracy 24.50\n",
      "Epoch 2| Batch 800/9812 | Loss 3.2190 | Accuracy 25.62\n",
      "Epoch 2| Batch 1000/9812 | Loss 5.4027 | Accuracy 25.30\n",
      "Epoch 2| Batch 1200/9812 | Loss 8.4721 | Accuracy 25.58\n",
      "Epoch 2| Batch 1400/9812 | Loss 3.0567 | Accuracy 25.36\n",
      "Epoch 2| Batch 1600/9812 | Loss 4.9062 | Accuracy 25.50\n",
      "Epoch 2| Batch 1800/9812 | Loss 4.3663 | Accuracy 25.06\n",
      "Epoch 2| Batch 2000/9812 | Loss 6.1378 | Accuracy 25.15\n",
      "Epoch 2| Batch 2200/9812 | Loss 2.2986 | Accuracy 25.36\n",
      "Epoch 2| Batch 2400/9812 | Loss 2.2800 | Accuracy 25.29\n",
      "Epoch 2| Batch 2600/9812 | Loss 2.6652 | Accuracy 25.38\n",
      "Epoch 2| Batch 2800/9812 | Loss 3.2639 | Accuracy 25.36\n",
      "Epoch 2| Batch 3000/9812 | Loss 3.4651 | Accuracy 25.80\n",
      "Epoch 2| Batch 3200/9812 | Loss 3.5346 | Accuracy 25.97\n",
      "Epoch 2| Batch 3400/9812 | Loss 4.6844 | Accuracy 25.97\n",
      "Epoch 2| Batch 3600/9812 | Loss 0.6901 | Accuracy 25.97\n",
      "Epoch 2| Batch 3800/9812 | Loss 3.0530 | Accuracy 26.18\n",
      "Epoch 2| Batch 4000/9812 | Loss 1.1682 | Accuracy 26.25\n",
      "Epoch 2| Batch 4200/9812 | Loss 3.5136 | Accuracy 26.40\n",
      "Epoch 2| Batch 4400/9812 | Loss 1.7485 | Accuracy 26.39\n",
      "Epoch 2| Batch 4600/9812 | Loss 1.7228 | Accuracy 26.41\n",
      "Epoch 2| Batch 4800/9812 | Loss 0.4006 | Accuracy 26.52\n",
      "Epoch 2| Batch 5000/9812 | Loss 2.1253 | Accuracy 26.62\n",
      "Epoch 2| Batch 5200/9812 | Loss 3.9676 | Accuracy 26.90\n",
      "Epoch 2| Batch 5400/9812 | Loss 4.2909 | Accuracy 26.81\n",
      "Epoch 2| Batch 5600/9812 | Loss 0.3088 | Accuracy 26.91\n",
      "Epoch 2| Batch 5800/9812 | Loss 2.7930 | Accuracy 27.02\n",
      "Epoch 2| Batch 6000/9812 | Loss 2.5364 | Accuracy 27.17\n",
      "Epoch 2| Batch 6200/9812 | Loss 1.7875 | Accuracy 27.26\n",
      "Epoch 2| Batch 6400/9812 | Loss 3.8285 | Accuracy 27.31\n",
      "Epoch 2| Batch 6600/9812 | Loss 4.0208 | Accuracy 27.53\n",
      "Epoch 2| Batch 6800/9812 | Loss 3.7766 | Accuracy 27.68\n",
      "Epoch 2| Batch 7000/9812 | Loss 0.0000 | Accuracy 27.83\n",
      "Epoch 2| Batch 7200/9812 | Loss 4.9314 | Accuracy 28.00\n",
      "Epoch 2| Batch 7400/9812 | Loss 4.2048 | Accuracy 28.09\n",
      "Epoch 2| Batch 7600/9812 | Loss 2.5349 | Accuracy 28.08\n",
      "Epoch 2| Batch 7800/9812 | Loss 2.4337 | Accuracy 28.23\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 1.7601 | Accuracy 28.19\n",
      "Epoch 3| Batch 200/9812 | Loss 4.3529 | Accuracy 40.00\n",
      "Epoch 3| Batch 400/9812 | Loss 0.1554 | Accuracy 36.75\n",
      "Epoch 3| Batch 600/9812 | Loss 2.5893 | Accuracy 34.17\n",
      "Epoch 3| Batch 800/9812 | Loss 2.1335 | Accuracy 34.12\n",
      "Epoch 3| Batch 1000/9812 | Loss 4.1211 | Accuracy 33.70\n",
      "Epoch 3| Batch 1200/9812 | Loss 1.4638 | Accuracy 34.17\n",
      "Epoch 3| Batch 1400/9812 | Loss 4.7180 | Accuracy 35.00\n",
      "Epoch 3| Batch 1600/9812 | Loss 2.0051 | Accuracy 35.00\n",
      "Epoch 3| Batch 1800/9812 | Loss 0.1719 | Accuracy 35.33\n",
      "Epoch 3| Batch 2000/9812 | Loss 4.3951 | Accuracy 35.25\n",
      "Epoch 3| Batch 2200/9812 | Loss 3.2955 | Accuracy 35.36\n",
      "Epoch 3| Batch 2400/9812 | Loss 2.6286 | Accuracy 35.83\n",
      "Epoch 3| Batch 2600/9812 | Loss 3.5761 | Accuracy 35.81\n",
      "Epoch 3| Batch 2800/9812 | Loss 13.7040 | Accuracy 35.89\n",
      "Epoch 3| Batch 3000/9812 | Loss 2.4474 | Accuracy 35.67\n",
      "Epoch 3| Batch 3200/9812 | Loss 3.9319 | Accuracy 35.91\n",
      "Epoch 3| Batch 3400/9812 | Loss 3.3725 | Accuracy 36.29\n",
      "Epoch 3| Batch 3600/9812 | Loss 4.1494 | Accuracy 36.67\n",
      "Epoch 3| Batch 3800/9812 | Loss 1.0857 | Accuracy 36.92\n",
      "Epoch 3| Batch 4000/9812 | Loss 1.8986 | Accuracy 36.67\n",
      "Epoch 3| Batch 4200/9812 | Loss 2.6049 | Accuracy 36.79\n",
      "Epoch 3| Batch 4400/9812 | Loss 1.7096 | Accuracy 36.80\n",
      "Epoch 3| Batch 4600/9812 | Loss 3.4821 | Accuracy 37.11\n",
      "Epoch 3| Batch 4800/9812 | Loss 1.6894 | Accuracy 37.27\n",
      "Epoch 3| Batch 5000/9812 | Loss 1.9892 | Accuracy 37.30\n",
      "Epoch 3| Batch 5200/9812 | Loss 3.0380 | Accuracy 37.56\n",
      "Epoch 3| Batch 5400/9812 | Loss 4.2772 | Accuracy 37.57\n",
      "Epoch 3| Batch 5600/9812 | Loss 3.9451 | Accuracy 37.64\n",
      "Epoch 3| Batch 5800/9812 | Loss 3.1048 | Accuracy 37.66\n",
      "Epoch 3| Batch 6000/9812 | Loss 3.4822 | Accuracy 37.78\n",
      "Epoch 3| Batch 6200/9812 | Loss 0.6227 | Accuracy 37.74\n",
      "Epoch 3| Batch 6400/9812 | Loss 2.8573 | Accuracy 37.86\n",
      "Epoch 3| Batch 6600/9812 | Loss 3.0374 | Accuracy 37.89\n",
      "Epoch 3| Batch 6800/9812 | Loss 3.1518 | Accuracy 37.99\n",
      "Epoch 3| Batch 7000/9812 | Loss 1.8409 | Accuracy 38.00\n",
      "Epoch 3| Batch 7200/9812 | Loss 0.1390 | Accuracy 38.22\n",
      "Epoch 3| Batch 7400/9812 | Loss 2.6978 | Accuracy 38.36\n",
      "Epoch 3| Batch 7600/9812 | Loss 2.6587 | Accuracy 38.53\n",
      "Epoch 3| Batch 7800/9812 | Loss 0.1390 | Accuracy 38.59\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 1.9221 | Accuracy 38.60\n",
      "Epoch 4| Batch 200/9812 | Loss 1.5491 | Accuracy 36.50\n",
      "Epoch 4| Batch 400/9812 | Loss 1.3370 | Accuracy 44.50\n",
      "Epoch 4| Batch 600/9812 | Loss 2.1739 | Accuracy 42.50\n",
      "Epoch 4| Batch 800/9812 | Loss 2.6751 | Accuracy 42.50\n",
      "Epoch 4| Batch 1000/9812 | Loss 1.6606 | Accuracy 44.30\n",
      "Epoch 4| Batch 1200/9812 | Loss 0.3057 | Accuracy 44.67\n",
      "Epoch 4| Batch 1400/9812 | Loss 0.3393 | Accuracy 45.00\n",
      "Epoch 4| Batch 1600/9812 | Loss 0.0001 | Accuracy 44.94\n",
      "Epoch 4| Batch 1800/9812 | Loss 3.5709 | Accuracy 45.17\n",
      "Epoch 4| Batch 2000/9812 | Loss 4.4133 | Accuracy 44.75\n",
      "Epoch 4| Batch 2200/9812 | Loss 2.0700 | Accuracy 44.64\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.4199 | Accuracy 45.42\n",
      "Epoch 4| Batch 2600/9812 | Loss 0.0022 | Accuracy 45.15\n",
      "Epoch 4| Batch 2800/9812 | Loss 0.0000 | Accuracy 44.79\n",
      "Epoch 4| Batch 3000/9812 | Loss 0.0690 | Accuracy 44.93\n",
      "Epoch 4| Batch 3200/9812 | Loss 2.3057 | Accuracy 45.00\n",
      "Epoch 4| Batch 3400/9812 | Loss 5.0572 | Accuracy 44.82\n",
      "Epoch 4| Batch 3600/9812 | Loss 1.4954 | Accuracy 45.28\n",
      "Epoch 4| Batch 3800/9812 | Loss 3.9124 | Accuracy 45.32\n",
      "Epoch 4| Batch 4000/9812 | Loss 1.0044 | Accuracy 45.45\n",
      "Epoch 4| Batch 4200/9812 | Loss 3.7994 | Accuracy 45.29\n",
      "Epoch 4| Batch 4400/9812 | Loss 0.0002 | Accuracy 45.50\n",
      "Epoch 4| Batch 4600/9812 | Loss 1.0913 | Accuracy 45.41\n",
      "Epoch 4| Batch 4800/9812 | Loss 2.1782 | Accuracy 45.60\n",
      "Epoch 4| Batch 5000/9812 | Loss 0.2475 | Accuracy 45.70\n",
      "Epoch 4| Batch 5200/9812 | Loss 1.2266 | Accuracy 45.77\n",
      "Epoch 4| Batch 5400/9812 | Loss 0.1495 | Accuracy 45.91\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.9309 | Accuracy 45.96\n",
      "Epoch 4| Batch 5800/9812 | Loss 0.2236 | Accuracy 45.90\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.2917 | Accuracy 46.02\n",
      "Epoch 4| Batch 6200/9812 | Loss 0.1971 | Accuracy 46.05\n",
      "Epoch 4| Batch 6400/9812 | Loss 0.8823 | Accuracy 46.25\n",
      "Epoch 4| Batch 6600/9812 | Loss 0.2629 | Accuracy 46.24\n",
      "Epoch 4| Batch 6800/9812 | Loss 2.9961 | Accuracy 46.37\n",
      "Epoch 4| Batch 7000/9812 | Loss 1.3352 | Accuracy 46.41\n",
      "Epoch 4| Batch 7200/9812 | Loss 0.2821 | Accuracy 46.38\n",
      "Epoch 4| Batch 7400/9812 | Loss 0.4258 | Accuracy 46.39\n",
      "Epoch 4| Batch 7600/9812 | Loss 2.1128 | Accuracy 46.53\n",
      "Epoch 4| Batch 7800/9812 | Loss 1.6924 | Accuracy 46.47\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 9.9752 | Accuracy 46.54\n",
      "Epoch 5| Batch 200/9812 | Loss 2.0153 | Accuracy 47.00\n",
      "Epoch 5| Batch 400/9812 | Loss 0.3601 | Accuracy 50.50\n",
      "Epoch 5| Batch 600/9812 | Loss 2.8704 | Accuracy 51.17\n",
      "Epoch 5| Batch 800/9812 | Loss 3.7856 | Accuracy 51.62\n",
      "Epoch 5| Batch 1000/9812 | Loss 1.7100 | Accuracy 51.60\n",
      "Epoch 5| Batch 1200/9812 | Loss 1.8004 | Accuracy 51.42\n",
      "Epoch 5| Batch 1400/9812 | Loss 1.9766 | Accuracy 51.21\n",
      "Epoch 5| Batch 1600/9812 | Loss 0.3783 | Accuracy 51.62\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.8286 | Accuracy 51.61\n",
      "Epoch 5| Batch 2000/9812 | Loss 2.4588 | Accuracy 51.65\n",
      "Epoch 5| Batch 2200/9812 | Loss 2.2908 | Accuracy 51.00\n",
      "Epoch 5| Batch 2400/9812 | Loss 0.0020 | Accuracy 50.79\n",
      "Epoch 5| Batch 2600/9812 | Loss 3.1509 | Accuracy 50.92\n",
      "Epoch 5| Batch 2800/9812 | Loss 0.1264 | Accuracy 50.93\n",
      "Epoch 5| Batch 3000/9812 | Loss 0.0000 | Accuracy 51.00\n",
      "Epoch 5| Batch 3200/9812 | Loss 2.5552 | Accuracy 50.72\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.2011 | Accuracy 50.88\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.0029 | Accuracy 51.08\n",
      "Epoch 5| Batch 3800/9812 | Loss 0.7065 | Accuracy 51.32\n",
      "Epoch 5| Batch 4000/9812 | Loss 4.5603 | Accuracy 51.27\n",
      "Epoch 5| Batch 4200/9812 | Loss 0.1908 | Accuracy 51.29\n",
      "Epoch 5| Batch 4400/9812 | Loss 0.1232 | Accuracy 51.43\n",
      "Epoch 5| Batch 4600/9812 | Loss 6.6785 | Accuracy 51.28\n",
      "Epoch 5| Batch 4800/9812 | Loss 2.1559 | Accuracy 51.12\n",
      "Epoch 5| Batch 5000/9812 | Loss 2.3806 | Accuracy 51.34\n",
      "Epoch 5| Batch 5200/9812 | Loss 0.2471 | Accuracy 51.33\n",
      "Epoch 5| Batch 5400/9812 | Loss 3.9615 | Accuracy 51.39\n",
      "Epoch 5| Batch 5600/9812 | Loss 1.3337 | Accuracy 51.36\n",
      "Epoch 5| Batch 5800/9812 | Loss 3.8122 | Accuracy 51.24\n",
      "Epoch 5| Batch 6000/9812 | Loss 2.8550 | Accuracy 51.25\n",
      "Epoch 5| Batch 6200/9812 | Loss 0.8815 | Accuracy 51.44\n",
      "Epoch 5| Batch 6400/9812 | Loss 1.7009 | Accuracy 51.55\n",
      "Epoch 5| Batch 6600/9812 | Loss 0.9260 | Accuracy 51.58\n",
      "Epoch 5| Batch 6800/9812 | Loss 0.0001 | Accuracy 51.82\n",
      "Epoch 5| Batch 7000/9812 | Loss 1.6189 | Accuracy 51.84\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.0050 | Accuracy 51.92\n",
      "Epoch 5| Batch 7400/9812 | Loss 0.0206 | Accuracy 51.93\n",
      "Epoch 5| Batch 7600/9812 | Loss 4.5354 | Accuracy 51.89\n",
      "Epoch 5| Batch 7800/9812 | Loss 1.6010 | Accuracy 51.91\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 2.4765 | Accuracy 51.92\n",
      "Epoch 6| Batch 200/9812 | Loss 1.3417 | Accuracy 52.50\n",
      "Epoch 6| Batch 400/9812 | Loss 1.7255 | Accuracy 53.00\n",
      "Epoch 6| Batch 600/9812 | Loss 0.0228 | Accuracy 54.33\n",
      "Epoch 6| Batch 800/9812 | Loss 3.0370 | Accuracy 55.00\n",
      "Epoch 6| Batch 1000/9812 | Loss 2.0580 | Accuracy 54.90\n",
      "Epoch 6| Batch 1200/9812 | Loss 1.0389 | Accuracy 55.17\n",
      "Epoch 6| Batch 1400/9812 | Loss 1.8556 | Accuracy 56.36\n",
      "Epoch 6| Batch 1600/9812 | Loss 2.6221 | Accuracy 57.06\n",
      "Epoch 6| Batch 1800/9812 | Loss 0.2018 | Accuracy 56.61\n",
      "Epoch 6| Batch 2000/9812 | Loss 0.0021 | Accuracy 57.20\n",
      "Epoch 6| Batch 2200/9812 | Loss 0.2531 | Accuracy 56.91\n",
      "Epoch 6| Batch 2400/9812 | Loss 0.8222 | Accuracy 57.33\n",
      "Epoch 6| Batch 2600/9812 | Loss 1.2396 | Accuracy 57.31\n",
      "Epoch 6| Batch 2800/9812 | Loss 1.7636 | Accuracy 57.39\n",
      "Epoch 6| Batch 3000/9812 | Loss 2.7639 | Accuracy 57.77\n",
      "Epoch 6| Batch 3200/9812 | Loss 0.0120 | Accuracy 57.81\n",
      "Epoch 6| Batch 3400/9812 | Loss 1.7547 | Accuracy 57.85\n",
      "Epoch 6| Batch 3600/9812 | Loss 3.1097 | Accuracy 57.75\n",
      "Epoch 6| Batch 3800/9812 | Loss 1.5721 | Accuracy 57.66\n",
      "Epoch 6| Batch 4000/9812 | Loss 0.1047 | Accuracy 57.62\n",
      "Epoch 6| Batch 4200/9812 | Loss 2.2582 | Accuracy 57.62\n",
      "Epoch 6| Batch 4400/9812 | Loss 0.4276 | Accuracy 57.64\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.0009 | Accuracy 57.59\n",
      "Epoch 6| Batch 4800/9812 | Loss 3.4542 | Accuracy 57.75\n",
      "Epoch 6| Batch 5000/9812 | Loss 0.3820 | Accuracy 57.68\n",
      "Epoch 6| Batch 5200/9812 | Loss 1.3398 | Accuracy 57.87\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.0002 | Accuracy 58.06\n",
      "Epoch 6| Batch 5600/9812 | Loss 1.4998 | Accuracy 58.11\n",
      "Epoch 6| Batch 5800/9812 | Loss 1.7367 | Accuracy 58.14\n",
      "Epoch 6| Batch 6000/9812 | Loss 0.8469 | Accuracy 58.10\n",
      "Epoch 6| Batch 6200/9812 | Loss 1.0009 | Accuracy 58.13\n",
      "Epoch 6| Batch 6400/9812 | Loss 1.6855 | Accuracy 58.02\n",
      "Epoch 6| Batch 6600/9812 | Loss 5.8214 | Accuracy 57.88\n",
      "Epoch 6| Batch 6800/9812 | Loss 2.9332 | Accuracy 57.90\n",
      "Epoch 6| Batch 7000/9812 | Loss 0.0002 | Accuracy 57.80\n",
      "Epoch 6| Batch 7200/9812 | Loss 1.7839 | Accuracy 57.69\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.3677 | Accuracy 57.80\n",
      "Epoch 6| Batch 7600/9812 | Loss 0.6809 | Accuracy 57.72\n",
      "Epoch 6| Batch 7800/9812 | Loss 0.4626 | Accuracy 57.72\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 1.2384 | Accuracy 57.71\n",
      "Epoch 7| Batch 200/9812 | Loss 0.0049 | Accuracy 54.00\n",
      "Epoch 7| Batch 400/9812 | Loss 1.0827 | Accuracy 57.25\n",
      "Epoch 7| Batch 600/9812 | Loss 0.0003 | Accuracy 59.17\n",
      "Epoch 7| Batch 800/9812 | Loss 2.0055 | Accuracy 58.38\n",
      "Epoch 7| Batch 1000/9812 | Loss 2.0450 | Accuracy 59.40\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.4031 | Accuracy 59.25\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.0009 | Accuracy 59.00\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.1232 | Accuracy 59.06\n",
      "Epoch 7| Batch 1800/9812 | Loss 0.2090 | Accuracy 59.00\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0457 | Accuracy 59.05\n",
      "Epoch 7| Batch 2200/9812 | Loss 1.1627 | Accuracy 59.32\n",
      "Epoch 7| Batch 2400/9812 | Loss 1.1566 | Accuracy 59.00\n",
      "Epoch 7| Batch 2600/9812 | Loss 0.0010 | Accuracy 59.54\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.2861 | Accuracy 59.64\n",
      "Epoch 7| Batch 3000/9812 | Loss 3.1023 | Accuracy 59.83\n",
      "Epoch 7| Batch 3200/9812 | Loss 3.1632 | Accuracy 59.81\n",
      "Epoch 7| Batch 3400/9812 | Loss 1.7886 | Accuracy 59.91\n",
      "Epoch 7| Batch 3600/9812 | Loss 0.0118 | Accuracy 59.97\n",
      "Epoch 7| Batch 3800/9812 | Loss 2.4000 | Accuracy 59.92\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.5774 | Accuracy 59.92\n",
      "Epoch 7| Batch 4200/9812 | Loss 0.6691 | Accuracy 60.10\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.1513 | Accuracy 60.00\n",
      "Epoch 7| Batch 4600/9812 | Loss 0.0424 | Accuracy 59.78\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.0031 | Accuracy 59.73\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.2209 | Accuracy 59.66\n",
      "Epoch 7| Batch 5200/9812 | Loss 1.4319 | Accuracy 59.62\n",
      "Epoch 7| Batch 5400/9812 | Loss 2.3972 | Accuracy 59.70\n",
      "Epoch 7| Batch 5600/9812 | Loss 1.1673 | Accuracy 59.84\n",
      "Epoch 7| Batch 5800/9812 | Loss 0.0009 | Accuracy 59.91\n",
      "Epoch 7| Batch 6000/9812 | Loss 1.8558 | Accuracy 59.77\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.0000 | Accuracy 59.95\n",
      "Epoch 7| Batch 6400/9812 | Loss 0.0240 | Accuracy 59.92\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.0179 | Accuracy 60.06\n",
      "Epoch 7| Batch 6800/9812 | Loss 0.0001 | Accuracy 60.28\n",
      "Epoch 7| Batch 7000/9812 | Loss 0.6608 | Accuracy 60.46\n",
      "Epoch 7| Batch 7200/9812 | Loss 2.6270 | Accuracy 60.57\n",
      "Epoch 7| Batch 7400/9812 | Loss 0.0865 | Accuracy 60.58\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.9970 | Accuracy 60.66\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.0000 | Accuracy 60.68\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 0.7139 | Accuracy 60.70\n",
      "Epoch 8| Batch 200/9812 | Loss 0.0009 | Accuracy 64.50\n",
      "Epoch 8| Batch 400/9812 | Loss 0.8749 | Accuracy 65.75\n",
      "Epoch 8| Batch 600/9812 | Loss 0.0003 | Accuracy 65.33\n",
      "Epoch 8| Batch 800/9812 | Loss 0.0050 | Accuracy 64.75\n",
      "Epoch 8| Batch 1000/9812 | Loss 2.9798 | Accuracy 65.90\n",
      "Epoch 8| Batch 1200/9812 | Loss 0.5012 | Accuracy 65.25\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.0048 | Accuracy 65.21\n",
      "Epoch 8| Batch 1600/9812 | Loss 5.6061 | Accuracy 65.06\n",
      "Epoch 8| Batch 1800/9812 | Loss 0.1738 | Accuracy 65.28\n",
      "Epoch 8| Batch 2000/9812 | Loss 1.4361 | Accuracy 65.00\n",
      "Epoch 8| Batch 2200/9812 | Loss 0.0043 | Accuracy 64.82\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0000 | Accuracy 65.00\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.3852 | Accuracy 65.42\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.5785 | Accuracy 65.29\n",
      "Epoch 8| Batch 3000/9812 | Loss 2.4367 | Accuracy 65.10\n",
      "Epoch 8| Batch 3200/9812 | Loss 0.1073 | Accuracy 65.16\n",
      "Epoch 8| Batch 3400/9812 | Loss 0.0112 | Accuracy 64.59\n",
      "Epoch 8| Batch 3600/9812 | Loss 2.9586 | Accuracy 64.31\n",
      "Epoch 8| Batch 3800/9812 | Loss 0.3307 | Accuracy 64.74\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.1909 | Accuracy 64.58\n",
      "Epoch 8| Batch 4200/9812 | Loss 0.8529 | Accuracy 64.52\n",
      "Epoch 8| Batch 4400/9812 | Loss 4.3035 | Accuracy 64.45\n",
      "Epoch 8| Batch 4600/9812 | Loss 3.8981 | Accuracy 64.43\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.0322 | Accuracy 64.40\n",
      "Epoch 8| Batch 5000/9812 | Loss 2.7547 | Accuracy 64.48\n",
      "Epoch 8| Batch 5200/9812 | Loss 1.1051 | Accuracy 64.42\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.0183 | Accuracy 64.26\n",
      "Epoch 8| Batch 5600/9812 | Loss 2.4593 | Accuracy 64.34\n",
      "Epoch 8| Batch 5800/9812 | Loss 1.3117 | Accuracy 64.33\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.3282 | Accuracy 64.27\n",
      "Epoch 8| Batch 6200/9812 | Loss 0.6273 | Accuracy 64.42\n",
      "Epoch 8| Batch 6400/9812 | Loss 0.5217 | Accuracy 64.53\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.0051 | Accuracy 64.52\n",
      "Epoch 8| Batch 6800/9812 | Loss 0.8861 | Accuracy 64.49\n",
      "Epoch 8| Batch 7000/9812 | Loss 1.2114 | Accuracy 64.51\n",
      "Epoch 8| Batch 7200/9812 | Loss 2.4386 | Accuracy 64.42\n",
      "Epoch 8| Batch 7400/9812 | Loss 3.3740 | Accuracy 64.36\n",
      "Epoch 8| Batch 7600/9812 | Loss 1.8553 | Accuracy 64.33\n",
      "Epoch 8| Batch 7800/9812 | Loss 3.6731 | Accuracy 64.36\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 4.4647 | Accuracy 64.48\n",
      "Epoch 9| Batch 200/9812 | Loss 1.8728 | Accuracy 70.00\n",
      "Epoch 9| Batch 400/9812 | Loss 1.1817 | Accuracy 71.00\n",
      "Epoch 9| Batch 600/9812 | Loss 0.0032 | Accuracy 70.33\n",
      "Epoch 9| Batch 800/9812 | Loss 1.6576 | Accuracy 70.00\n",
      "Epoch 9| Batch 1000/9812 | Loss 0.6506 | Accuracy 69.10\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.1808 | Accuracy 69.00\n",
      "Epoch 9| Batch 1400/9812 | Loss 0.1800 | Accuracy 69.86\n",
      "Epoch 9| Batch 1600/9812 | Loss 1.1767 | Accuracy 69.94\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.6731 | Accuracy 70.06\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.0030 | Accuracy 69.85\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.1778 | Accuracy 69.27\n",
      "Epoch 9| Batch 2400/9812 | Loss 0.0116 | Accuracy 69.17\n",
      "Epoch 9| Batch 2600/9812 | Loss 5.9693 | Accuracy 69.04\n",
      "Epoch 9| Batch 2800/9812 | Loss 1.5708 | Accuracy 69.04\n",
      "Epoch 9| Batch 3000/9812 | Loss 0.0104 | Accuracy 69.33\n",
      "Epoch 9| Batch 3200/9812 | Loss 0.1137 | Accuracy 69.00\n",
      "Epoch 9| Batch 3400/9812 | Loss 1.4647 | Accuracy 69.06\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0034 | Accuracy 68.64\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.1804 | Accuracy 68.76\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.3878 | Accuracy 68.72\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.2007 | Accuracy 68.86\n",
      "Epoch 9| Batch 4400/9812 | Loss 0.0031 | Accuracy 68.50\n",
      "Epoch 9| Batch 4600/9812 | Loss 0.2001 | Accuracy 68.59\n",
      "Epoch 9| Batch 4800/9812 | Loss 0.0001 | Accuracy 68.75\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.2361 | Accuracy 68.66\n",
      "Epoch 9| Batch 5200/9812 | Loss 0.1997 | Accuracy 68.71\n",
      "Epoch 9| Batch 5400/9812 | Loss 0.0404 | Accuracy 68.76\n",
      "Epoch 9| Batch 5600/9812 | Loss 2.6630 | Accuracy 68.86\n",
      "Epoch 9| Batch 5800/9812 | Loss 6.5713 | Accuracy 68.93\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.6051 | Accuracy 68.92\n",
      "Epoch 9| Batch 6200/9812 | Loss 1.1988 | Accuracy 69.00\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.0162 | Accuracy 68.94\n",
      "Epoch 9| Batch 6600/9812 | Loss 0.0129 | Accuracy 68.80\n",
      "Epoch 9| Batch 6800/9812 | Loss 0.0000 | Accuracy 68.66\n",
      "Epoch 9| Batch 7000/9812 | Loss 2.8435 | Accuracy 68.59\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.6747 | Accuracy 68.68\n",
      "Epoch 9| Batch 7400/9812 | Loss 0.2343 | Accuracy 68.69\n",
      "Epoch 9| Batch 7600/9812 | Loss 3.7080 | Accuracy 68.82\n",
      "Epoch 9| Batch 7800/9812 | Loss 0.0001 | Accuracy 68.81\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 1.2349 | Accuracy 68.79\n"
     ]
    }
   ],
   "source": [
    "## DNN ##\n",
    "h_in, h_out =  30*512, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 512, 256, 128, 128, 100\n",
    "DNN = UCF_DNN(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "DNN_criterion = nn.CrossEntropyLoss()\n",
    "DNN_optimizer = optim.Adam(DNN.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "## Training the DNN ##\n",
    "DNN.train()\n",
    "print(\"\\n\\n\\{}\\n\".format(DNN.__class__.__name__ ))\n",
    "epochs=10\n",
    "DNN.train()\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        DNN_optimizer.zero_grad()\n",
    "\n",
    "        y_pred = DNN(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = DNN_criterion(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        DNN_optimizer.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 69.28%\n"
     ]
    }
   ],
   "source": [
    "DNN.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = DNN(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastyle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_CNN1D\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 4.5626 | Accuracy 0.50\n",
      "Epoch 0| Batch 400/9812 | Loss 4.3926 | Accuracy 0.50\n",
      "Epoch 0| Batch 600/9812 | Loss 4.9996 | Accuracy 1.00\n",
      "Epoch 0| Batch 800/9812 | Loss 4.2780 | Accuracy 1.38\n",
      "Epoch 0| Batch 1000/9812 | Loss 5.1707 | Accuracy 1.10\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.5634 | Accuracy 1.33\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.2372 | Accuracy 1.50\n",
      "Epoch 0| Batch 1600/9812 | Loss 1.9279 | Accuracy 2.06\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.3252 | Accuracy 2.89\n",
      "Epoch 0| Batch 2000/9812 | Loss 3.9000 | Accuracy 4.60\n",
      "Epoch 0| Batch 2200/9812 | Loss 2.8411 | Accuracy 5.82\n",
      "Epoch 0| Batch 2400/9812 | Loss 6.1750 | Accuracy 7.04\n",
      "Epoch 0| Batch 2600/9812 | Loss 3.2412 | Accuracy 8.85\n",
      "Epoch 0| Batch 2800/9812 | Loss 0.2297 | Accuracy 10.29\n",
      "Epoch 0| Batch 3000/9812 | Loss 6.9798 | Accuracy 11.87\n",
      "Epoch 0| Batch 3200/9812 | Loss 1.2970 | Accuracy 12.94\n",
      "Epoch 0| Batch 3400/9812 | Loss 1.9360 | Accuracy 14.53\n",
      "Epoch 0| Batch 3600/9812 | Loss 0.8299 | Accuracy 16.25\n",
      "Epoch 0| Batch 3800/9812 | Loss 3.1337 | Accuracy 17.74\n",
      "Epoch 0| Batch 4000/9812 | Loss 2.2638 | Accuracy 19.30\n",
      "Epoch 0| Batch 4200/9812 | Loss 0.0232 | Accuracy 20.33\n",
      "Epoch 0| Batch 4400/9812 | Loss 3.3874 | Accuracy 21.55\n",
      "Epoch 0| Batch 4600/9812 | Loss 4.1331 | Accuracy 22.65\n",
      "Epoch 0| Batch 4800/9812 | Loss 2.0040 | Accuracy 23.77\n",
      "Epoch 0| Batch 5000/9812 | Loss 0.0833 | Accuracy 24.74\n",
      "Epoch 0| Batch 5200/9812 | Loss 1.4911 | Accuracy 25.58\n",
      "Epoch 0| Batch 5400/9812 | Loss 1.3665 | Accuracy 26.46\n",
      "Epoch 0| Batch 5600/9812 | Loss 3.5687 | Accuracy 27.34\n",
      "Epoch 0| Batch 5800/9812 | Loss 0.2212 | Accuracy 28.29\n",
      "Epoch 0| Batch 6000/9812 | Loss 4.2340 | Accuracy 29.32\n",
      "Epoch 0| Batch 6200/9812 | Loss 3.0861 | Accuracy 30.15\n",
      "Epoch 0| Batch 6400/9812 | Loss 0.4285 | Accuracy 31.16\n",
      "Epoch 0| Batch 6600/9812 | Loss 2.7617 | Accuracy 31.83\n",
      "Epoch 0| Batch 6800/9812 | Loss 2.5687 | Accuracy 32.62\n",
      "Epoch 0| Batch 7000/9812 | Loss 4.0721 | Accuracy 33.54\n",
      "Epoch 0| Batch 7200/9812 | Loss 0.7543 | Accuracy 34.17\n",
      "Epoch 0| Batch 7400/9812 | Loss 4.1773 | Accuracy 34.74\n",
      "Epoch 0| Batch 7600/9812 | Loss 0.0061 | Accuracy 35.41\n",
      "Epoch 0| Batch 7800/9812 | Loss 1.0260 | Accuracy 36.21\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 3.7705 | Accuracy 36.32\n",
      "Epoch 1| Batch 200/9812 | Loss 1.4470 | Accuracy 70.50\n",
      "Epoch 1| Batch 400/9812 | Loss 0.0715 | Accuracy 71.00\n",
      "Epoch 1| Batch 600/9812 | Loss 2.3074 | Accuracy 69.67\n",
      "Epoch 1| Batch 800/9812 | Loss 3.9050 | Accuracy 69.50\n",
      "Epoch 1| Batch 1000/9812 | Loss 0.0386 | Accuracy 70.50\n",
      "Epoch 1| Batch 1200/9812 | Loss 2.7968 | Accuracy 70.08\n",
      "Epoch 1| Batch 1400/9812 | Loss 3.3499 | Accuracy 70.43\n",
      "Epoch 1| Batch 1600/9812 | Loss 0.2422 | Accuracy 70.62\n",
      "Epoch 1| Batch 1800/9812 | Loss 2.4944 | Accuracy 70.39\n",
      "Epoch 1| Batch 2000/9812 | Loss 2.5187 | Accuracy 70.55\n",
      "Epoch 1| Batch 2200/9812 | Loss 0.0006 | Accuracy 70.73\n",
      "Epoch 1| Batch 2400/9812 | Loss 0.1482 | Accuracy 70.46\n",
      "Epoch 1| Batch 2600/9812 | Loss 0.0002 | Accuracy 70.73\n",
      "Epoch 1| Batch 2800/9812 | Loss 4.3316 | Accuracy 70.96\n",
      "Epoch 1| Batch 3000/9812 | Loss 0.7152 | Accuracy 71.43\n",
      "Epoch 1| Batch 3200/9812 | Loss 0.0003 | Accuracy 71.81\n",
      "Epoch 1| Batch 3400/9812 | Loss 0.1345 | Accuracy 71.97\n",
      "Epoch 1| Batch 3600/9812 | Loss 0.0100 | Accuracy 72.22\n",
      "Epoch 1| Batch 3800/9812 | Loss 0.0013 | Accuracy 72.29\n",
      "Epoch 1| Batch 4000/9812 | Loss 0.3102 | Accuracy 72.40\n",
      "Epoch 1| Batch 4200/9812 | Loss 2.3021 | Accuracy 72.48\n",
      "Epoch 1| Batch 4400/9812 | Loss 0.2094 | Accuracy 72.52\n",
      "Epoch 1| Batch 4600/9812 | Loss 0.7882 | Accuracy 72.87\n",
      "Epoch 1| Batch 4800/9812 | Loss 0.9829 | Accuracy 72.83\n",
      "Epoch 1| Batch 5000/9812 | Loss 0.0000 | Accuracy 73.00\n",
      "Epoch 1| Batch 5200/9812 | Loss 0.3053 | Accuracy 73.02\n",
      "Epoch 1| Batch 5400/9812 | Loss 0.0027 | Accuracy 73.13\n",
      "Epoch 1| Batch 5600/9812 | Loss 0.4775 | Accuracy 73.29\n",
      "Epoch 1| Batch 5800/9812 | Loss 0.3220 | Accuracy 73.31\n",
      "Epoch 1| Batch 6000/9812 | Loss 3.0835 | Accuracy 73.28\n",
      "Epoch 1| Batch 6200/9812 | Loss 0.8287 | Accuracy 73.31\n",
      "Epoch 1| Batch 6400/9812 | Loss 0.0280 | Accuracy 73.41\n",
      "Epoch 1| Batch 6600/9812 | Loss 1.5677 | Accuracy 73.76\n",
      "Epoch 1| Batch 6800/9812 | Loss 0.0015 | Accuracy 73.88\n",
      "Epoch 1| Batch 7000/9812 | Loss 0.0000 | Accuracy 74.00\n",
      "Epoch 1| Batch 7200/9812 | Loss 0.8431 | Accuracy 74.00\n",
      "Epoch 1| Batch 7400/9812 | Loss 1.2600 | Accuracy 74.22\n",
      "Epoch 1| Batch 7600/9812 | Loss 0.0412 | Accuracy 74.26\n",
      "Epoch 1| Batch 7800/9812 | Loss 0.0200 | Accuracy 74.36\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 1.8915 | Accuracy 74.35\n",
      "Epoch 2| Batch 200/9812 | Loss 0.6081 | Accuracy 87.00\n",
      "Epoch 2| Batch 400/9812 | Loss 2.1835 | Accuracy 85.25\n",
      "Epoch 2| Batch 600/9812 | Loss 0.0000 | Accuracy 85.17\n",
      "Epoch 2| Batch 800/9812 | Loss 0.0029 | Accuracy 85.38\n",
      "Epoch 2| Batch 1000/9812 | Loss 0.0095 | Accuracy 85.70\n",
      "Epoch 2| Batch 1200/9812 | Loss 0.6870 | Accuracy 86.00\n",
      "Epoch 2| Batch 1400/9812 | Loss 1.3132 | Accuracy 85.50\n",
      "Epoch 2| Batch 1600/9812 | Loss 0.2080 | Accuracy 85.38\n",
      "Epoch 2| Batch 1800/9812 | Loss 0.0564 | Accuracy 85.33\n",
      "Epoch 2| Batch 2000/9812 | Loss 0.0132 | Accuracy 85.60\n",
      "Epoch 2| Batch 2200/9812 | Loss 0.0002 | Accuracy 85.64\n",
      "Epoch 2| Batch 2400/9812 | Loss 1.0624 | Accuracy 85.17\n",
      "Epoch 2| Batch 2600/9812 | Loss 1.4632 | Accuracy 85.12\n",
      "Epoch 2| Batch 2800/9812 | Loss 3.5319 | Accuracy 84.61\n",
      "Epoch 2| Batch 3000/9812 | Loss 0.2022 | Accuracy 84.40\n",
      "Epoch 2| Batch 3200/9812 | Loss 0.1280 | Accuracy 84.72\n",
      "Epoch 2| Batch 3400/9812 | Loss 1.2674 | Accuracy 84.76\n",
      "Epoch 2| Batch 3600/9812 | Loss 0.8505 | Accuracy 84.53\n",
      "Epoch 2| Batch 3800/9812 | Loss 0.3543 | Accuracy 84.55\n",
      "Epoch 2| Batch 4000/9812 | Loss 0.1993 | Accuracy 84.42\n",
      "Epoch 2| Batch 4200/9812 | Loss 0.4508 | Accuracy 84.55\n",
      "Epoch 2| Batch 4400/9812 | Loss 0.0000 | Accuracy 84.70\n",
      "Epoch 2| Batch 4600/9812 | Loss 0.0000 | Accuracy 84.70\n",
      "Epoch 2| Batch 4800/9812 | Loss 0.6325 | Accuracy 84.75\n",
      "Epoch 2| Batch 5000/9812 | Loss 0.2580 | Accuracy 84.68\n",
      "Epoch 2| Batch 5200/9812 | Loss 0.0039 | Accuracy 84.69\n",
      "Epoch 2| Batch 5400/9812 | Loss 0.0806 | Accuracy 84.57\n",
      "Epoch 2| Batch 5600/9812 | Loss 0.0857 | Accuracy 84.73\n",
      "Epoch 2| Batch 5800/9812 | Loss 0.0510 | Accuracy 84.86\n",
      "Epoch 2| Batch 6000/9812 | Loss 0.0590 | Accuracy 84.80\n",
      "Epoch 2| Batch 6200/9812 | Loss 0.0010 | Accuracy 85.10\n",
      "Epoch 2| Batch 6400/9812 | Loss 0.0000 | Accuracy 85.12\n",
      "Epoch 2| Batch 6600/9812 | Loss 0.0868 | Accuracy 85.12\n",
      "Epoch 2| Batch 6800/9812 | Loss 0.0000 | Accuracy 85.15\n",
      "Epoch 2| Batch 7000/9812 | Loss 0.3266 | Accuracy 85.07\n",
      "Epoch 2| Batch 7200/9812 | Loss 0.0001 | Accuracy 85.12\n",
      "Epoch 2| Batch 7400/9812 | Loss 0.0000 | Accuracy 85.24\n",
      "Epoch 2| Batch 7600/9812 | Loss 1.4878 | Accuracy 85.13\n",
      "Epoch 2| Batch 7800/9812 | Loss 0.0417 | Accuracy 85.14\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 0.0000 | Accuracy 85.12\n",
      "Epoch 3| Batch 200/9812 | Loss 0.0417 | Accuracy 92.50\n",
      "Epoch 3| Batch 400/9812 | Loss 0.0004 | Accuracy 91.00\n",
      "Epoch 3| Batch 600/9812 | Loss 0.0077 | Accuracy 90.17\n",
      "Epoch 3| Batch 800/9812 | Loss 1.3272 | Accuracy 90.50\n",
      "Epoch 3| Batch 1000/9812 | Loss 0.0003 | Accuracy 90.40\n",
      "Epoch 3| Batch 1200/9812 | Loss 0.7884 | Accuracy 90.58\n",
      "Epoch 3| Batch 1400/9812 | Loss 2.0196 | Accuracy 90.29\n",
      "Epoch 3| Batch 1600/9812 | Loss 0.0205 | Accuracy 90.19\n",
      "Epoch 3| Batch 1800/9812 | Loss 0.0036 | Accuracy 90.39\n",
      "Epoch 3| Batch 2000/9812 | Loss 0.5662 | Accuracy 90.90\n",
      "Epoch 3| Batch 2200/9812 | Loss 0.4367 | Accuracy 91.00\n",
      "Epoch 3| Batch 2400/9812 | Loss 0.1196 | Accuracy 91.08\n",
      "Epoch 3| Batch 2600/9812 | Loss 0.0021 | Accuracy 90.77\n",
      "Epoch 3| Batch 2800/9812 | Loss 0.0046 | Accuracy 90.75\n",
      "Epoch 3| Batch 3000/9812 | Loss 0.0002 | Accuracy 90.70\n",
      "Epoch 3| Batch 3200/9812 | Loss 0.1946 | Accuracy 90.84\n",
      "Epoch 3| Batch 3400/9812 | Loss 0.0000 | Accuracy 90.88\n",
      "Epoch 3| Batch 3600/9812 | Loss 0.0473 | Accuracy 90.83\n",
      "Epoch 3| Batch 3800/9812 | Loss 0.0000 | Accuracy 90.87\n",
      "Epoch 3| Batch 4000/9812 | Loss 2.1552 | Accuracy 90.72\n",
      "Epoch 3| Batch 4200/9812 | Loss 0.0442 | Accuracy 90.74\n",
      "Epoch 3| Batch 4400/9812 | Loss 0.0369 | Accuracy 90.77\n",
      "Epoch 3| Batch 4600/9812 | Loss 0.0002 | Accuracy 90.87\n",
      "Epoch 3| Batch 4800/9812 | Loss 0.2747 | Accuracy 90.81\n",
      "Epoch 3| Batch 5000/9812 | Loss 0.0000 | Accuracy 90.74\n",
      "Epoch 3| Batch 5200/9812 | Loss 0.0004 | Accuracy 90.71\n",
      "Epoch 3| Batch 5400/9812 | Loss 0.0022 | Accuracy 90.83\n",
      "Epoch 3| Batch 5600/9812 | Loss 0.0007 | Accuracy 90.80\n",
      "Epoch 3| Batch 5800/9812 | Loss 0.0353 | Accuracy 90.83\n",
      "Epoch 3| Batch 6000/9812 | Loss 0.0510 | Accuracy 90.88\n",
      "Epoch 3| Batch 6200/9812 | Loss 0.0013 | Accuracy 90.95\n",
      "Epoch 3| Batch 6400/9812 | Loss 1.3194 | Accuracy 90.97\n",
      "Epoch 3| Batch 6600/9812 | Loss 0.0000 | Accuracy 91.09\n",
      "Epoch 3| Batch 6800/9812 | Loss 0.0007 | Accuracy 91.10\n",
      "Epoch 3| Batch 7000/9812 | Loss 0.0000 | Accuracy 91.16\n",
      "Epoch 3| Batch 7200/9812 | Loss 0.0029 | Accuracy 91.15\n",
      "Epoch 3| Batch 7400/9812 | Loss 0.0018 | Accuracy 91.19\n",
      "Epoch 3| Batch 7600/9812 | Loss 0.0034 | Accuracy 91.17\n",
      "Epoch 3| Batch 7800/9812 | Loss 0.0738 | Accuracy 91.26\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 0.0063 | Accuracy 91.27\n",
      "Epoch 4| Batch 200/9812 | Loss 0.6266 | Accuracy 93.00\n",
      "Epoch 4| Batch 400/9812 | Loss 0.0006 | Accuracy 93.75\n",
      "Epoch 4| Batch 600/9812 | Loss 0.0140 | Accuracy 94.67\n",
      "Epoch 4| Batch 800/9812 | Loss 0.0000 | Accuracy 94.62\n",
      "Epoch 4| Batch 1000/9812 | Loss 0.0149 | Accuracy 94.00\n",
      "Epoch 4| Batch 1200/9812 | Loss 0.0077 | Accuracy 94.00\n",
      "Epoch 4| Batch 1400/9812 | Loss 0.0000 | Accuracy 94.07\n",
      "Epoch 4| Batch 1600/9812 | Loss 0.0044 | Accuracy 94.56\n",
      "Epoch 4| Batch 1800/9812 | Loss 0.0000 | Accuracy 94.56\n",
      "Epoch 4| Batch 2000/9812 | Loss 0.0061 | Accuracy 94.70\n",
      "Epoch 4| Batch 2200/9812 | Loss 0.0001 | Accuracy 94.64\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.0006 | Accuracy 94.58\n",
      "Epoch 4| Batch 2600/9812 | Loss 0.0001 | Accuracy 94.58\n",
      "Epoch 4| Batch 2800/9812 | Loss 0.0210 | Accuracy 94.29\n",
      "Epoch 4| Batch 3000/9812 | Loss 0.0001 | Accuracy 94.17\n",
      "Epoch 4| Batch 3200/9812 | Loss 0.0007 | Accuracy 94.22\n",
      "Epoch 4| Batch 3400/9812 | Loss 0.0006 | Accuracy 94.32\n",
      "Epoch 4| Batch 3600/9812 | Loss 0.2074 | Accuracy 94.14\n",
      "Epoch 4| Batch 3800/9812 | Loss 0.0029 | Accuracy 94.08\n",
      "Epoch 4| Batch 4000/9812 | Loss 0.0043 | Accuracy 94.08\n",
      "Epoch 4| Batch 4200/9812 | Loss 0.0001 | Accuracy 94.00\n",
      "Epoch 4| Batch 4400/9812 | Loss 0.0014 | Accuracy 94.09\n",
      "Epoch 4| Batch 4600/9812 | Loss 0.0001 | Accuracy 94.22\n",
      "Epoch 4| Batch 4800/9812 | Loss 0.0107 | Accuracy 94.31\n",
      "Epoch 4| Batch 5000/9812 | Loss 0.0000 | Accuracy 94.30\n",
      "Epoch 4| Batch 5200/9812 | Loss 0.0000 | Accuracy 94.27\n",
      "Epoch 4| Batch 5400/9812 | Loss 0.0000 | Accuracy 94.31\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.0017 | Accuracy 94.34\n",
      "Epoch 4| Batch 5800/9812 | Loss 0.0881 | Accuracy 94.19\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.0000 | Accuracy 94.18\n",
      "Epoch 4| Batch 6200/9812 | Loss 0.0529 | Accuracy 94.13\n",
      "Epoch 4| Batch 6400/9812 | Loss 0.0003 | Accuracy 94.09\n",
      "Epoch 4| Batch 6600/9812 | Loss 0.0598 | Accuracy 94.06\n",
      "Epoch 4| Batch 6800/9812 | Loss 0.0011 | Accuracy 94.06\n",
      "Epoch 4| Batch 7000/9812 | Loss 0.0000 | Accuracy 94.10\n",
      "Epoch 4| Batch 7200/9812 | Loss 0.0000 | Accuracy 94.07\n",
      "Epoch 4| Batch 7400/9812 | Loss 0.0000 | Accuracy 94.04\n",
      "Epoch 4| Batch 7600/9812 | Loss 0.0000 | Accuracy 94.08\n",
      "Epoch 4| Batch 7800/9812 | Loss 0.0000 | Accuracy 94.15\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 0.0000 | Accuracy 94.16\n",
      "Epoch 5| Batch 200/9812 | Loss 0.0009 | Accuracy 96.50\n",
      "Epoch 5| Batch 400/9812 | Loss 0.0000 | Accuracy 96.00\n",
      "Epoch 5| Batch 600/9812 | Loss 0.0000 | Accuracy 96.17\n",
      "Epoch 5| Batch 800/9812 | Loss 0.0000 | Accuracy 96.62\n",
      "Epoch 5| Batch 1000/9812 | Loss 0.0003 | Accuracy 96.60\n",
      "Epoch 5| Batch 1200/9812 | Loss 0.0000 | Accuracy 96.58\n",
      "Epoch 5| Batch 1400/9812 | Loss 0.0000 | Accuracy 96.29\n",
      "Epoch 5| Batch 1600/9812 | Loss 0.0000 | Accuracy 96.31\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.0000 | Accuracy 96.33\n",
      "Epoch 5| Batch 2000/9812 | Loss 0.0000 | Accuracy 96.35\n",
      "Epoch 5| Batch 2200/9812 | Loss 0.0000 | Accuracy 96.09\n",
      "Epoch 5| Batch 2400/9812 | Loss 0.0000 | Accuracy 95.92\n",
      "Epoch 5| Batch 2600/9812 | Loss 0.0000 | Accuracy 96.08\n",
      "Epoch 5| Batch 2800/9812 | Loss 0.0030 | Accuracy 96.25\n",
      "Epoch 5| Batch 3000/9812 | Loss 0.0000 | Accuracy 96.17\n",
      "Epoch 5| Batch 3200/9812 | Loss 0.0017 | Accuracy 96.06\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.0077 | Accuracy 96.03\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.0283 | Accuracy 95.89\n",
      "Epoch 5| Batch 3800/9812 | Loss 0.0012 | Accuracy 95.95\n",
      "Epoch 5| Batch 4000/9812 | Loss 0.0084 | Accuracy 95.78\n",
      "Epoch 5| Batch 4200/9812 | Loss 0.0677 | Accuracy 95.81\n",
      "Epoch 5| Batch 4400/9812 | Loss 0.0543 | Accuracy 95.73\n",
      "Epoch 5| Batch 4600/9812 | Loss 0.0000 | Accuracy 95.80\n",
      "Epoch 5| Batch 4800/9812 | Loss 1.0278 | Accuracy 95.81\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.0238 | Accuracy 95.80\n",
      "Epoch 5| Batch 5200/9812 | Loss 0.0000 | Accuracy 95.87\n",
      "Epoch 5| Batch 5400/9812 | Loss 0.0115 | Accuracy 95.83\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.0000 | Accuracy 95.88\n",
      "Epoch 5| Batch 5800/9812 | Loss 0.0711 | Accuracy 95.86\n",
      "Epoch 5| Batch 6000/9812 | Loss 0.0012 | Accuracy 95.93\n",
      "Epoch 5| Batch 6200/9812 | Loss 0.0000 | Accuracy 95.84\n",
      "Epoch 5| Batch 6400/9812 | Loss 0.0000 | Accuracy 95.92\n",
      "Epoch 5| Batch 6600/9812 | Loss 0.0008 | Accuracy 95.89\n",
      "Epoch 5| Batch 6800/9812 | Loss 0.0000 | Accuracy 95.87\n",
      "Epoch 5| Batch 7000/9812 | Loss 0.0000 | Accuracy 95.89\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.0012 | Accuracy 95.92\n",
      "Epoch 5| Batch 7400/9812 | Loss 0.0015 | Accuracy 95.93\n",
      "Epoch 5| Batch 7600/9812 | Loss 0.0000 | Accuracy 96.03\n",
      "Epoch 5| Batch 7800/9812 | Loss 0.0002 | Accuracy 95.97\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 0.0000 | Accuracy 96.00\n",
      "Epoch 6| Batch 200/9812 | Loss 0.4122 | Accuracy 96.00\n",
      "Epoch 6| Batch 400/9812 | Loss 0.0001 | Accuracy 97.50\n",
      "Epoch 6| Batch 600/9812 | Loss 0.0007 | Accuracy 97.00\n",
      "Epoch 6| Batch 800/9812 | Loss 0.0013 | Accuracy 97.38\n",
      "Epoch 6| Batch 1000/9812 | Loss 2.6003 | Accuracy 97.20\n",
      "Epoch 6| Batch 1200/9812 | Loss 0.0011 | Accuracy 97.00\n",
      "Epoch 6| Batch 1400/9812 | Loss 0.0000 | Accuracy 96.93\n",
      "Epoch 6| Batch 1600/9812 | Loss 0.0000 | Accuracy 96.69\n",
      "Epoch 6| Batch 1800/9812 | Loss 0.0005 | Accuracy 96.61\n",
      "Epoch 6| Batch 2000/9812 | Loss 0.0181 | Accuracy 96.65\n",
      "Epoch 6| Batch 2200/9812 | Loss 0.0000 | Accuracy 96.50\n",
      "Epoch 6| Batch 2400/9812 | Loss 0.4358 | Accuracy 96.46\n",
      "Epoch 6| Batch 2600/9812 | Loss 0.0000 | Accuracy 96.42\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.0064 | Accuracy 96.36\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.0000 | Accuracy 96.50\n",
      "Epoch 6| Batch 3200/9812 | Loss 0.0003 | Accuracy 96.50\n",
      "Epoch 6| Batch 3400/9812 | Loss 0.0002 | Accuracy 96.53\n",
      "Epoch 6| Batch 3600/9812 | Loss 0.0007 | Accuracy 96.53\n",
      "Epoch 6| Batch 3800/9812 | Loss 0.6993 | Accuracy 96.63\n",
      "Epoch 6| Batch 4000/9812 | Loss 0.0002 | Accuracy 96.72\n",
      "Epoch 6| Batch 4200/9812 | Loss 0.7675 | Accuracy 96.81\n",
      "Epoch 6| Batch 4400/9812 | Loss 0.0000 | Accuracy 96.80\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.0000 | Accuracy 96.74\n",
      "Epoch 6| Batch 4800/9812 | Loss 0.0001 | Accuracy 96.75\n",
      "Epoch 6| Batch 5000/9812 | Loss 0.0002 | Accuracy 96.74\n",
      "Epoch 6| Batch 5200/9812 | Loss 0.0000 | Accuracy 96.75\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.0000 | Accuracy 96.78\n",
      "Epoch 6| Batch 5600/9812 | Loss 0.0383 | Accuracy 96.82\n",
      "Epoch 6| Batch 5800/9812 | Loss 0.0000 | Accuracy 96.88\n",
      "Epoch 6| Batch 6000/9812 | Loss 0.0013 | Accuracy 96.93\n",
      "Epoch 6| Batch 6200/9812 | Loss 0.0016 | Accuracy 96.89\n",
      "Epoch 6| Batch 6400/9812 | Loss 0.0000 | Accuracy 96.91\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.0000 | Accuracy 96.79\n",
      "Epoch 6| Batch 6800/9812 | Loss 0.1616 | Accuracy 96.79\n",
      "Epoch 6| Batch 7000/9812 | Loss 0.3085 | Accuracy 96.73\n",
      "Epoch 6| Batch 7200/9812 | Loss 0.0355 | Accuracy 96.75\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.0234 | Accuracy 96.77\n",
      "Epoch 6| Batch 7600/9812 | Loss 0.5102 | Accuracy 96.80\n",
      "Epoch 6| Batch 7800/9812 | Loss 0.2345 | Accuracy 96.83\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 0.0000 | Accuracy 96.83\n",
      "Epoch 7| Batch 200/9812 | Loss 0.0000 | Accuracy 98.50\n",
      "Epoch 7| Batch 400/9812 | Loss 0.0000 | Accuracy 99.00\n",
      "Epoch 7| Batch 600/9812 | Loss 0.0586 | Accuracy 98.50\n",
      "Epoch 7| Batch 800/9812 | Loss 0.0002 | Accuracy 97.75\n",
      "Epoch 7| Batch 1000/9812 | Loss 0.0000 | Accuracy 97.90\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.0000 | Accuracy 97.83\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.0000 | Accuracy 97.57\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.0002 | Accuracy 97.81\n",
      "Epoch 7| Batch 1800/9812 | Loss 0.0747 | Accuracy 97.72\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0000 | Accuracy 97.80\n",
      "Epoch 7| Batch 2200/9812 | Loss 0.0006 | Accuracy 97.82\n",
      "Epoch 7| Batch 2400/9812 | Loss 0.0426 | Accuracy 98.00\n",
      "Epoch 7| Batch 2600/9812 | Loss 0.0000 | Accuracy 97.96\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.0002 | Accuracy 98.04\n",
      "Epoch 7| Batch 3000/9812 | Loss 0.0109 | Accuracy 97.87\n",
      "Epoch 7| Batch 3200/9812 | Loss 0.0612 | Accuracy 97.84\n",
      "Epoch 7| Batch 3400/9812 | Loss 0.0127 | Accuracy 97.91\n",
      "Epoch 7| Batch 3600/9812 | Loss 0.0000 | Accuracy 97.75\n",
      "Epoch 7| Batch 3800/9812 | Loss 0.0000 | Accuracy 97.71\n",
      "Epoch 7| Batch 4000/9812 | Loss 0.0000 | Accuracy 97.70\n",
      "Epoch 7| Batch 4200/9812 | Loss 0.0003 | Accuracy 97.74\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.0337 | Accuracy 97.75\n",
      "Epoch 7| Batch 4600/9812 | Loss 0.0000 | Accuracy 97.70\n",
      "Epoch 7| Batch 4800/9812 | Loss 0.0014 | Accuracy 97.65\n",
      "Epoch 7| Batch 5000/9812 | Loss 0.0000 | Accuracy 97.68\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.0005 | Accuracy 97.71\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.0002 | Accuracy 97.67\n",
      "Epoch 7| Batch 5600/9812 | Loss 0.0000 | Accuracy 97.68\n",
      "Epoch 7| Batch 5800/9812 | Loss 0.1062 | Accuracy 97.66\n",
      "Epoch 7| Batch 6000/9812 | Loss 0.0016 | Accuracy 97.63\n",
      "Epoch 7| Batch 6200/9812 | Loss 0.0000 | Accuracy 97.60\n",
      "Epoch 7| Batch 6400/9812 | Loss 0.0133 | Accuracy 97.53\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.0037 | Accuracy 97.55\n",
      "Epoch 7| Batch 6800/9812 | Loss 0.0000 | Accuracy 97.51\n",
      "Epoch 7| Batch 7000/9812 | Loss 0.0001 | Accuracy 97.46\n",
      "Epoch 7| Batch 7200/9812 | Loss 0.0002 | Accuracy 97.53\n",
      "Epoch 7| Batch 7400/9812 | Loss 0.0000 | Accuracy 97.59\n",
      "Epoch 7| Batch 7600/9812 | Loss 0.0000 | Accuracy 97.63\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.0870 | Accuracy 97.60\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 0.4822 | Accuracy 97.62\n",
      "Epoch 8| Batch 200/9812 | Loss 0.0000 | Accuracy 99.50\n",
      "Epoch 8| Batch 400/9812 | Loss 0.0005 | Accuracy 99.50\n",
      "Epoch 8| Batch 600/9812 | Loss 2.4020 | Accuracy 99.00\n",
      "Epoch 8| Batch 800/9812 | Loss 0.0000 | Accuracy 98.88\n",
      "Epoch 8| Batch 1000/9812 | Loss 0.0001 | Accuracy 98.90\n",
      "Epoch 8| Batch 1200/9812 | Loss 0.0000 | Accuracy 98.83\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.0001 | Accuracy 98.71\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.0001 | Accuracy 98.75\n",
      "Epoch 8| Batch 1800/9812 | Loss 0.0000 | Accuracy 98.89\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.0011 | Accuracy 98.80\n",
      "Epoch 8| Batch 2200/9812 | Loss 0.0000 | Accuracy 98.73\n",
      "Epoch 8| Batch 2400/9812 | Loss 0.0003 | Accuracy 98.46\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.0000 | Accuracy 98.31\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.0000 | Accuracy 98.36\n",
      "Epoch 8| Batch 3000/9812 | Loss 0.0012 | Accuracy 98.40\n",
      "Epoch 8| Batch 3200/9812 | Loss 0.0000 | Accuracy 98.25\n",
      "Epoch 8| Batch 3400/9812 | Loss 0.0188 | Accuracy 98.24\n",
      "Epoch 8| Batch 3600/9812 | Loss 0.0000 | Accuracy 98.31\n",
      "Epoch 8| Batch 3800/9812 | Loss 0.2852 | Accuracy 98.21\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.1166 | Accuracy 98.15\n",
      "Epoch 8| Batch 4200/9812 | Loss 0.0000 | Accuracy 98.02\n",
      "Epoch 8| Batch 4400/9812 | Loss 0.0000 | Accuracy 98.05\n",
      "Epoch 8| Batch 4600/9812 | Loss 0.0000 | Accuracy 98.13\n",
      "Epoch 8| Batch 4800/9812 | Loss 0.0006 | Accuracy 98.12\n",
      "Epoch 8| Batch 5000/9812 | Loss 0.0000 | Accuracy 98.12\n",
      "Epoch 8| Batch 5200/9812 | Loss 0.0001 | Accuracy 98.10\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.0001 | Accuracy 98.11\n",
      "Epoch 8| Batch 5600/9812 | Loss 0.0001 | Accuracy 98.12\n",
      "Epoch 8| Batch 5800/9812 | Loss 0.0000 | Accuracy 98.03\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.0000 | Accuracy 98.07\n",
      "Epoch 8| Batch 6200/9812 | Loss 0.0000 | Accuracy 98.08\n",
      "Epoch 8| Batch 6400/9812 | Loss 0.1912 | Accuracy 98.06\n",
      "Epoch 8| Batch 6600/9812 | Loss 0.0000 | Accuracy 98.09\n",
      "Epoch 8| Batch 6800/9812 | Loss 0.0551 | Accuracy 98.07\n",
      "Epoch 8| Batch 7000/9812 | Loss 0.0000 | Accuracy 98.10\n",
      "Epoch 8| Batch 7200/9812 | Loss 0.0000 | Accuracy 98.14\n",
      "Epoch 8| Batch 7400/9812 | Loss 0.0000 | Accuracy 98.16\n",
      "Epoch 8| Batch 7600/9812 | Loss 0.0004 | Accuracy 98.16\n",
      "Epoch 8| Batch 7800/9812 | Loss 0.0002 | Accuracy 98.17\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 0.0001 | Accuracy 98.15\n",
      "Epoch 9| Batch 200/9812 | Loss 0.0007 | Accuracy 98.00\n",
      "Epoch 9| Batch 400/9812 | Loss 0.0000 | Accuracy 98.00\n",
      "Epoch 9| Batch 600/9812 | Loss 0.0000 | Accuracy 98.50\n",
      "Epoch 9| Batch 800/9812 | Loss 0.0130 | Accuracy 98.00\n",
      "Epoch 9| Batch 1000/9812 | Loss 0.0002 | Accuracy 97.90\n",
      "Epoch 9| Batch 1200/9812 | Loss 0.0000 | Accuracy 97.92\n",
      "Epoch 9| Batch 1400/9812 | Loss 0.0009 | Accuracy 98.00\n",
      "Epoch 9| Batch 1600/9812 | Loss 0.0000 | Accuracy 98.00\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.0477 | Accuracy 97.89\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.0000 | Accuracy 97.90\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.0003 | Accuracy 97.86\n",
      "Epoch 9| Batch 2400/9812 | Loss 0.0000 | Accuracy 98.04\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.0000 | Accuracy 98.19\n",
      "Epoch 9| Batch 2800/9812 | Loss 0.0002 | Accuracy 98.11\n",
      "Epoch 9| Batch 3000/9812 | Loss 0.0053 | Accuracy 98.13\n",
      "Epoch 9| Batch 3200/9812 | Loss 0.0000 | Accuracy 98.12\n",
      "Epoch 9| Batch 3400/9812 | Loss 0.0000 | Accuracy 98.15\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.0006 | Accuracy 98.22\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.0002 | Accuracy 98.32\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.0007 | Accuracy 98.38\n",
      "Epoch 9| Batch 4200/9812 | Loss 0.0001 | Accuracy 98.43\n",
      "Epoch 9| Batch 4400/9812 | Loss 0.0000 | Accuracy 98.43\n",
      "Epoch 9| Batch 4600/9812 | Loss 0.0001 | Accuracy 98.33\n",
      "Epoch 9| Batch 4800/9812 | Loss 0.0000 | Accuracy 98.15\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.0000 | Accuracy 98.06\n",
      "Epoch 9| Batch 5200/9812 | Loss 0.0018 | Accuracy 98.12\n",
      "Epoch 9| Batch 5400/9812 | Loss 0.0027 | Accuracy 98.19\n",
      "Epoch 9| Batch 5600/9812 | Loss 0.0005 | Accuracy 98.11\n",
      "Epoch 9| Batch 5800/9812 | Loss 0.0000 | Accuracy 98.09\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.0000 | Accuracy 98.12\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0000 | Accuracy 98.11\n",
      "Epoch 9| Batch 6400/9812 | Loss 0.0000 | Accuracy 98.11\n",
      "Epoch 9| Batch 6600/9812 | Loss 0.0000 | Accuracy 98.12\n",
      "Epoch 9| Batch 6800/9812 | Loss 0.0000 | Accuracy 98.18\n",
      "Epoch 9| Batch 7000/9812 | Loss 0.0001 | Accuracy 98.17\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.3381 | Accuracy 98.14\n",
      "Epoch 9| Batch 7400/9812 | Loss 0.0000 | Accuracy 98.18\n",
      "Epoch 9| Batch 7600/9812 | Loss 0.0000 | Accuracy 98.18\n",
      "Epoch 9| Batch 7800/9812 | Loss 0.0000 | Accuracy 98.21\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.0000 | Accuracy 98.22\n"
     ]
    }
   ],
   "source": [
    "## UCF_CNN1D ##\n",
    "h_in, h_out = 30, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 256, 128, 64, 1000, 500\n",
    "CNN_1D = UCF_CNN1D(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion_1D = nn.CrossEntropyLoss()\n",
    "optimizer_1D = optim.Adam(CNN_1D.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "print(\"\\n\\n\\{}\\n\".format(CNN_1D.__class__.__name__ ))\n",
    "\n",
    "## Training the DNN ##\n",
    "CNN_1D.train()\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        optimizer_1D.zero_grad()\n",
    "\n",
    "        y_pred = CNN_1D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = criterion_1D(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer_1D.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 89.61%\n"
     ]
    }
   ],
   "source": [
    "CNN_1D.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = CNN_1D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF 2D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\UCF_CNN2D\n",
      "\n",
      "Epoch 0| Batch 200/9812 | Loss 5.3948 | Accuracy 1.50\n",
      "Epoch 0| Batch 400/9812 | Loss 4.4116 | Accuracy 1.50\n",
      "Epoch 0| Batch 600/9812 | Loss 4.3590 | Accuracy 1.00\n",
      "Epoch 0| Batch 800/9812 | Loss 4.2572 | Accuracy 0.88\n",
      "Epoch 0| Batch 1000/9812 | Loss 4.5414 | Accuracy 1.00\n",
      "Epoch 0| Batch 1200/9812 | Loss 4.5702 | Accuracy 1.00\n",
      "Epoch 0| Batch 1400/9812 | Loss 4.9329 | Accuracy 1.14\n",
      "Epoch 0| Batch 1600/9812 | Loss 6.0980 | Accuracy 1.12\n",
      "Epoch 0| Batch 1800/9812 | Loss 4.8853 | Accuracy 1.17\n",
      "Epoch 0| Batch 2000/9812 | Loss 4.0295 | Accuracy 1.15\n",
      "Epoch 0| Batch 2200/9812 | Loss 5.1544 | Accuracy 1.09\n",
      "Epoch 0| Batch 2400/9812 | Loss 4.3634 | Accuracy 1.17\n",
      "Epoch 0| Batch 2600/9812 | Loss 4.5949 | Accuracy 1.19\n",
      "Epoch 0| Batch 2800/9812 | Loss 4.6042 | Accuracy 1.21\n",
      "Epoch 0| Batch 3000/9812 | Loss 4.6481 | Accuracy 1.27\n",
      "Epoch 0| Batch 3200/9812 | Loss 4.7417 | Accuracy 1.25\n",
      "Epoch 0| Batch 3400/9812 | Loss 4.6134 | Accuracy 1.24\n",
      "Epoch 0| Batch 3600/9812 | Loss 4.5941 | Accuracy 1.25\n",
      "Epoch 0| Batch 3800/9812 | Loss 4.6196 | Accuracy 1.32\n",
      "Epoch 0| Batch 4000/9812 | Loss 4.2748 | Accuracy 1.35\n",
      "Epoch 0| Batch 4200/9812 | Loss 4.2624 | Accuracy 1.36\n",
      "Epoch 0| Batch 4400/9812 | Loss 4.5755 | Accuracy 1.39\n",
      "Epoch 0| Batch 4600/9812 | Loss 4.2267 | Accuracy 1.50\n",
      "Epoch 0| Batch 4800/9812 | Loss 4.5679 | Accuracy 1.54\n",
      "Epoch 0| Batch 5000/9812 | Loss 4.8056 | Accuracy 1.56\n",
      "Epoch 0| Batch 5200/9812 | Loss 5.3834 | Accuracy 1.58\n",
      "Epoch 0| Batch 5400/9812 | Loss 4.4534 | Accuracy 1.54\n",
      "Epoch 0| Batch 5600/9812 | Loss 4.2696 | Accuracy 1.55\n",
      "Epoch 0| Batch 5800/9812 | Loss 3.8364 | Accuracy 1.55\n",
      "Epoch 0| Batch 6000/9812 | Loss 4.3911 | Accuracy 1.67\n",
      "Epoch 0| Batch 6200/9812 | Loss 4.4703 | Accuracy 1.81\n",
      "Epoch 0| Batch 6400/9812 | Loss 4.5378 | Accuracy 1.86\n",
      "Epoch 0| Batch 6600/9812 | Loss 4.4386 | Accuracy 1.86\n",
      "Epoch 0| Batch 6800/9812 | Loss 3.9826 | Accuracy 1.93\n",
      "Epoch 0| Batch 7000/9812 | Loss 4.1632 | Accuracy 2.03\n",
      "Epoch 0| Batch 7200/9812 | Loss 4.3994 | Accuracy 2.11\n",
      "Epoch 0| Batch 7400/9812 | Loss 4.9981 | Accuracy 2.18\n",
      "Epoch 0| Batch 7600/9812 | Loss 4.4701 | Accuracy 2.24\n",
      "Epoch 0| Batch 7800/9812 | Loss 3.5978 | Accuracy 2.31\n",
      "\n",
      "\n",
      "Epoch 0/10 | Loss 2.9620 | Accuracy 2.36\n",
      "Epoch 1| Batch 200/9812 | Loss 3.2316 | Accuracy 9.00\n",
      "Epoch 1| Batch 400/9812 | Loss 3.8604 | Accuracy 8.50\n",
      "Epoch 1| Batch 600/9812 | Loss 5.3945 | Accuracy 9.83\n",
      "Epoch 1| Batch 800/9812 | Loss 4.0517 | Accuracy 10.62\n",
      "Epoch 1| Batch 1000/9812 | Loss 4.3627 | Accuracy 10.60\n",
      "Epoch 1| Batch 1200/9812 | Loss 3.4540 | Accuracy 10.50\n",
      "Epoch 1| Batch 1400/9812 | Loss 3.9276 | Accuracy 11.07\n",
      "Epoch 1| Batch 1600/9812 | Loss 4.3936 | Accuracy 11.00\n",
      "Epoch 1| Batch 1800/9812 | Loss 4.8912 | Accuracy 11.33\n",
      "Epoch 1| Batch 2000/9812 | Loss 4.7884 | Accuracy 11.30\n",
      "Epoch 1| Batch 2200/9812 | Loss 4.2055 | Accuracy 11.68\n",
      "Epoch 1| Batch 2400/9812 | Loss 3.5743 | Accuracy 11.83\n",
      "Epoch 1| Batch 2600/9812 | Loss 1.0369 | Accuracy 12.08\n",
      "Epoch 1| Batch 2800/9812 | Loss 3.9335 | Accuracy 12.25\n",
      "Epoch 1| Batch 3000/9812 | Loss 3.2233 | Accuracy 12.37\n",
      "Epoch 1| Batch 3200/9812 | Loss 2.0420 | Accuracy 12.56\n",
      "Epoch 1| Batch 3400/9812 | Loss 5.3572 | Accuracy 12.74\n",
      "Epoch 1| Batch 3600/9812 | Loss 2.3536 | Accuracy 13.06\n",
      "Epoch 1| Batch 3800/9812 | Loss 5.6571 | Accuracy 13.21\n",
      "Epoch 1| Batch 4000/9812 | Loss 3.7659 | Accuracy 13.40\n",
      "Epoch 1| Batch 4200/9812 | Loss 3.0023 | Accuracy 13.50\n",
      "Epoch 1| Batch 4400/9812 | Loss 2.0618 | Accuracy 13.59\n",
      "Epoch 1| Batch 4600/9812 | Loss 4.6434 | Accuracy 13.80\n",
      "Epoch 1| Batch 4800/9812 | Loss 4.7869 | Accuracy 14.12\n",
      "Epoch 1| Batch 5000/9812 | Loss 2.1847 | Accuracy 14.48\n",
      "Epoch 1| Batch 5200/9812 | Loss 2.7830 | Accuracy 14.67\n",
      "Epoch 1| Batch 5400/9812 | Loss 4.2666 | Accuracy 14.87\n",
      "Epoch 1| Batch 5600/9812 | Loss 3.5817 | Accuracy 15.04\n",
      "Epoch 1| Batch 5800/9812 | Loss 2.9437 | Accuracy 15.36\n",
      "Epoch 1| Batch 6000/9812 | Loss 3.5009 | Accuracy 15.55\n",
      "Epoch 1| Batch 6200/9812 | Loss 5.1401 | Accuracy 15.60\n",
      "Epoch 1| Batch 6400/9812 | Loss 3.1436 | Accuracy 15.73\n",
      "Epoch 1| Batch 6600/9812 | Loss 4.2795 | Accuracy 15.94\n",
      "Epoch 1| Batch 6800/9812 | Loss 5.0417 | Accuracy 16.00\n",
      "Epoch 1| Batch 7000/9812 | Loss 0.0530 | Accuracy 16.13\n",
      "Epoch 1| Batch 7200/9812 | Loss 4.2101 | Accuracy 16.28\n",
      "Epoch 1| Batch 7400/9812 | Loss 2.6588 | Accuracy 16.46\n",
      "Epoch 1| Batch 7600/9812 | Loss 2.5625 | Accuracy 16.64\n",
      "Epoch 1| Batch 7800/9812 | Loss 3.4830 | Accuracy 16.76\n",
      "\n",
      "\n",
      "Epoch 1/10 | Loss 0.6158 | Accuracy 16.79\n",
      "Epoch 2| Batch 200/9812 | Loss 4.5201 | Accuracy 24.50\n",
      "Epoch 2| Batch 400/9812 | Loss 3.6589 | Accuracy 25.75\n",
      "Epoch 2| Batch 600/9812 | Loss 0.0536 | Accuracy 26.00\n",
      "Epoch 2| Batch 800/9812 | Loss 6.7065 | Accuracy 25.75\n",
      "Epoch 2| Batch 1000/9812 | Loss 3.9171 | Accuracy 25.50\n",
      "Epoch 2| Batch 1200/9812 | Loss 5.6196 | Accuracy 25.00\n",
      "Epoch 2| Batch 1400/9812 | Loss 1.1136 | Accuracy 25.29\n",
      "Epoch 2| Batch 1600/9812 | Loss 3.4457 | Accuracy 25.38\n",
      "Epoch 2| Batch 1800/9812 | Loss 5.1710 | Accuracy 24.94\n",
      "Epoch 2| Batch 2000/9812 | Loss 0.3764 | Accuracy 24.85\n",
      "Epoch 2| Batch 2200/9812 | Loss 3.3474 | Accuracy 24.91\n",
      "Epoch 2| Batch 2400/9812 | Loss 3.6887 | Accuracy 25.04\n",
      "Epoch 2| Batch 2600/9812 | Loss 2.8568 | Accuracy 25.46\n",
      "Epoch 2| Batch 2800/9812 | Loss 2.6070 | Accuracy 25.36\n",
      "Epoch 2| Batch 3000/9812 | Loss 1.9718 | Accuracy 25.17\n",
      "Epoch 2| Batch 3200/9812 | Loss 3.2998 | Accuracy 25.41\n",
      "Epoch 2| Batch 3400/9812 | Loss 2.6567 | Accuracy 25.44\n",
      "Epoch 2| Batch 3600/9812 | Loss 4.2137 | Accuracy 25.36\n",
      "Epoch 2| Batch 3800/9812 | Loss 0.0000 | Accuracy 25.42\n",
      "Epoch 2| Batch 4000/9812 | Loss 1.0150 | Accuracy 25.70\n",
      "Epoch 2| Batch 4200/9812 | Loss 6.0295 | Accuracy 25.57\n",
      "Epoch 2| Batch 4400/9812 | Loss 2.8903 | Accuracy 25.93\n",
      "Epoch 2| Batch 4600/9812 | Loss 3.6902 | Accuracy 26.02\n",
      "Epoch 2| Batch 4800/9812 | Loss 0.0056 | Accuracy 26.12\n",
      "Epoch 2| Batch 5000/9812 | Loss 2.9773 | Accuracy 26.12\n",
      "Epoch 2| Batch 5200/9812 | Loss 2.5698 | Accuracy 26.15\n",
      "Epoch 2| Batch 5400/9812 | Loss 1.7509 | Accuracy 26.31\n",
      "Epoch 2| Batch 5600/9812 | Loss 1.2053 | Accuracy 26.57\n",
      "Epoch 2| Batch 5800/9812 | Loss 2.4873 | Accuracy 26.72\n",
      "Epoch 2| Batch 6000/9812 | Loss 2.1200 | Accuracy 26.80\n",
      "Epoch 2| Batch 6200/9812 | Loss 2.1952 | Accuracy 26.82\n",
      "Epoch 2| Batch 6400/9812 | Loss 1.2492 | Accuracy 26.83\n",
      "Epoch 2| Batch 6600/9812 | Loss 2.0332 | Accuracy 26.91\n",
      "Epoch 2| Batch 6800/9812 | Loss 0.0003 | Accuracy 27.04\n",
      "Epoch 2| Batch 7000/9812 | Loss 1.1601 | Accuracy 27.11\n",
      "Epoch 2| Batch 7200/9812 | Loss 1.8140 | Accuracy 27.04\n",
      "Epoch 2| Batch 7400/9812 | Loss 2.9190 | Accuracy 27.08\n",
      "Epoch 2| Batch 7600/9812 | Loss 1.4129 | Accuracy 27.25\n",
      "Epoch 2| Batch 7800/9812 | Loss 0.4809 | Accuracy 27.42\n",
      "\n",
      "\n",
      "Epoch 2/10 | Loss 5.0277 | Accuracy 27.48\n",
      "Epoch 3| Batch 200/9812 | Loss 2.8053 | Accuracy 35.00\n",
      "Epoch 3| Batch 400/9812 | Loss 0.7073 | Accuracy 32.75\n",
      "Epoch 3| Batch 600/9812 | Loss 0.1788 | Accuracy 33.50\n",
      "Epoch 3| Batch 800/9812 | Loss 0.7182 | Accuracy 33.38\n",
      "Epoch 3| Batch 1000/9812 | Loss 1.8225 | Accuracy 33.20\n",
      "Epoch 3| Batch 1200/9812 | Loss 3.1872 | Accuracy 32.67\n",
      "Epoch 3| Batch 1400/9812 | Loss 2.1484 | Accuracy 32.43\n",
      "Epoch 3| Batch 1600/9812 | Loss 1.9838 | Accuracy 33.19\n",
      "Epoch 3| Batch 1800/9812 | Loss 3.4983 | Accuracy 32.83\n",
      "Epoch 3| Batch 2000/9812 | Loss 5.8960 | Accuracy 32.95\n",
      "Epoch 3| Batch 2200/9812 | Loss 0.0004 | Accuracy 33.14\n",
      "Epoch 3| Batch 2400/9812 | Loss 4.6084 | Accuracy 32.46\n",
      "Epoch 3| Batch 2600/9812 | Loss 0.5859 | Accuracy 32.62\n",
      "Epoch 3| Batch 2800/9812 | Loss 3.4662 | Accuracy 32.43\n",
      "Epoch 3| Batch 3000/9812 | Loss 3.3845 | Accuracy 32.50\n",
      "Epoch 3| Batch 3200/9812 | Loss 2.3294 | Accuracy 32.59\n",
      "Epoch 3| Batch 3400/9812 | Loss 2.9530 | Accuracy 32.76\n",
      "Epoch 3| Batch 3600/9812 | Loss 2.1252 | Accuracy 32.83\n",
      "Epoch 3| Batch 3800/9812 | Loss 0.6710 | Accuracy 32.89\n",
      "Epoch 3| Batch 4000/9812 | Loss 0.6410 | Accuracy 33.02\n",
      "Epoch 3| Batch 4200/9812 | Loss 1.1007 | Accuracy 33.10\n",
      "Epoch 3| Batch 4400/9812 | Loss 2.1328 | Accuracy 33.09\n",
      "Epoch 3| Batch 4600/9812 | Loss 2.2996 | Accuracy 33.02\n",
      "Epoch 3| Batch 4800/9812 | Loss 2.8711 | Accuracy 32.94\n",
      "Epoch 3| Batch 5000/9812 | Loss 6.3322 | Accuracy 32.66\n",
      "Epoch 3| Batch 5200/9812 | Loss 2.1301 | Accuracy 32.54\n",
      "Epoch 3| Batch 5400/9812 | Loss 3.3786 | Accuracy 32.87\n",
      "Epoch 3| Batch 5600/9812 | Loss 3.6810 | Accuracy 33.04\n",
      "Epoch 3| Batch 5800/9812 | Loss 1.8534 | Accuracy 33.09\n",
      "Epoch 3| Batch 6000/9812 | Loss 4.0422 | Accuracy 33.17\n",
      "Epoch 3| Batch 6200/9812 | Loss 3.2667 | Accuracy 33.15\n",
      "Epoch 3| Batch 6400/9812 | Loss 1.0017 | Accuracy 33.05\n",
      "Epoch 3| Batch 6600/9812 | Loss 2.4811 | Accuracy 33.15\n",
      "Epoch 3| Batch 6800/9812 | Loss 1.6227 | Accuracy 33.25\n",
      "Epoch 3| Batch 7000/9812 | Loss 3.2584 | Accuracy 33.31\n",
      "Epoch 3| Batch 7200/9812 | Loss 2.7485 | Accuracy 33.36\n",
      "Epoch 3| Batch 7400/9812 | Loss 6.6887 | Accuracy 33.36\n",
      "Epoch 3| Batch 7600/9812 | Loss 2.3052 | Accuracy 33.41\n",
      "Epoch 3| Batch 7800/9812 | Loss 1.5639 | Accuracy 33.47\n",
      "\n",
      "\n",
      "Epoch 3/10 | Loss 1.2080 | Accuracy 33.52\n",
      "Epoch 4| Batch 200/9812 | Loss 1.5758 | Accuracy 37.00\n",
      "Epoch 4| Batch 400/9812 | Loss 0.8963 | Accuracy 36.25\n",
      "Epoch 4| Batch 600/9812 | Loss 1.6515 | Accuracy 38.83\n",
      "Epoch 4| Batch 800/9812 | Loss 1.4886 | Accuracy 38.62\n",
      "Epoch 4| Batch 1000/9812 | Loss 0.4886 | Accuracy 38.30\n",
      "Epoch 4| Batch 1200/9812 | Loss 0.6072 | Accuracy 38.50\n",
      "Epoch 4| Batch 1400/9812 | Loss 2.2879 | Accuracy 38.50\n",
      "Epoch 4| Batch 1600/9812 | Loss 2.2812 | Accuracy 38.00\n",
      "Epoch 4| Batch 1800/9812 | Loss 4.1186 | Accuracy 38.06\n",
      "Epoch 4| Batch 2000/9812 | Loss 0.6875 | Accuracy 38.05\n",
      "Epoch 4| Batch 2200/9812 | Loss 2.5355 | Accuracy 38.09\n",
      "Epoch 4| Batch 2400/9812 | Loss 0.0060 | Accuracy 38.50\n",
      "Epoch 4| Batch 2600/9812 | Loss 0.3230 | Accuracy 38.46\n",
      "Epoch 4| Batch 2800/9812 | Loss 4.1825 | Accuracy 38.14\n",
      "Epoch 4| Batch 3000/9812 | Loss 2.2311 | Accuracy 38.00\n",
      "Epoch 4| Batch 3200/9812 | Loss 3.6359 | Accuracy 37.97\n",
      "Epoch 4| Batch 3400/9812 | Loss 0.0506 | Accuracy 38.06\n",
      "Epoch 4| Batch 3600/9812 | Loss 1.3557 | Accuracy 38.36\n",
      "Epoch 4| Batch 3800/9812 | Loss 2.4169 | Accuracy 38.37\n",
      "Epoch 4| Batch 4000/9812 | Loss 3.9671 | Accuracy 38.23\n",
      "Epoch 4| Batch 4200/9812 | Loss 1.4431 | Accuracy 38.12\n",
      "Epoch 4| Batch 4400/9812 | Loss 2.9518 | Accuracy 37.89\n",
      "Epoch 4| Batch 4600/9812 | Loss 2.3982 | Accuracy 37.87\n",
      "Epoch 4| Batch 4800/9812 | Loss 5.0709 | Accuracy 37.96\n",
      "Epoch 4| Batch 5000/9812 | Loss 5.9967 | Accuracy 38.00\n",
      "Epoch 4| Batch 5200/9812 | Loss 2.5045 | Accuracy 37.88\n",
      "Epoch 4| Batch 5400/9812 | Loss 1.3911 | Accuracy 38.02\n",
      "Epoch 4| Batch 5600/9812 | Loss 0.0354 | Accuracy 38.46\n",
      "Epoch 4| Batch 5800/9812 | Loss 0.0003 | Accuracy 38.69\n",
      "Epoch 4| Batch 6000/9812 | Loss 0.8667 | Accuracy 38.75\n",
      "Epoch 4| Batch 6200/9812 | Loss 0.8267 | Accuracy 38.69\n",
      "Epoch 4| Batch 6400/9812 | Loss 1.4228 | Accuracy 38.78\n",
      "Epoch 4| Batch 6600/9812 | Loss 2.8271 | Accuracy 38.91\n",
      "Epoch 4| Batch 6800/9812 | Loss 5.0489 | Accuracy 38.96\n",
      "Epoch 4| Batch 7000/9812 | Loss 1.2937 | Accuracy 38.99\n",
      "Epoch 4| Batch 7200/9812 | Loss 3.5642 | Accuracy 39.01\n",
      "Epoch 4| Batch 7400/9812 | Loss 1.4185 | Accuracy 38.88\n",
      "Epoch 4| Batch 7600/9812 | Loss 3.4510 | Accuracy 39.12\n",
      "Epoch 4| Batch 7800/9812 | Loss 1.6213 | Accuracy 39.03\n",
      "\n",
      "\n",
      "Epoch 4/10 | Loss 0.6543 | Accuracy 39.09\n",
      "Epoch 5| Batch 200/9812 | Loss 0.8427 | Accuracy 46.00\n",
      "Epoch 5| Batch 400/9812 | Loss 1.5523 | Accuracy 41.00\n",
      "Epoch 5| Batch 600/9812 | Loss 1.2322 | Accuracy 43.00\n",
      "Epoch 5| Batch 800/9812 | Loss 2.1005 | Accuracy 42.12\n",
      "Epoch 5| Batch 1000/9812 | Loss 3.1719 | Accuracy 42.10\n",
      "Epoch 5| Batch 1200/9812 | Loss 0.0169 | Accuracy 42.17\n",
      "Epoch 5| Batch 1400/9812 | Loss 3.1462 | Accuracy 42.50\n",
      "Epoch 5| Batch 1600/9812 | Loss 1.5862 | Accuracy 42.00\n",
      "Epoch 5| Batch 1800/9812 | Loss 0.1384 | Accuracy 42.11\n",
      "Epoch 5| Batch 2000/9812 | Loss 5.4235 | Accuracy 42.65\n",
      "Epoch 5| Batch 2200/9812 | Loss 4.9183 | Accuracy 42.77\n",
      "Epoch 5| Batch 2400/9812 | Loss 3.1118 | Accuracy 42.46\n",
      "Epoch 5| Batch 2600/9812 | Loss 4.7691 | Accuracy 42.96\n",
      "Epoch 5| Batch 2800/9812 | Loss 0.9114 | Accuracy 43.32\n",
      "Epoch 5| Batch 3000/9812 | Loss 3.3253 | Accuracy 43.40\n",
      "Epoch 5| Batch 3200/9812 | Loss 0.3373 | Accuracy 43.47\n",
      "Epoch 5| Batch 3400/9812 | Loss 0.5601 | Accuracy 43.65\n",
      "Epoch 5| Batch 3600/9812 | Loss 0.1990 | Accuracy 43.61\n",
      "Epoch 5| Batch 3800/9812 | Loss 1.6247 | Accuracy 43.42\n",
      "Epoch 5| Batch 4000/9812 | Loss 0.0144 | Accuracy 43.12\n",
      "Epoch 5| Batch 4200/9812 | Loss 1.6030 | Accuracy 43.00\n",
      "Epoch 5| Batch 4400/9812 | Loss 5.2609 | Accuracy 43.30\n",
      "Epoch 5| Batch 4600/9812 | Loss 3.1897 | Accuracy 43.30\n",
      "Epoch 5| Batch 4800/9812 | Loss 2.0528 | Accuracy 43.42\n",
      "Epoch 5| Batch 5000/9812 | Loss 0.1912 | Accuracy 43.50\n",
      "Epoch 5| Batch 5200/9812 | Loss 3.6355 | Accuracy 43.44\n",
      "Epoch 5| Batch 5400/9812 | Loss 0.0293 | Accuracy 43.39\n",
      "Epoch 5| Batch 5600/9812 | Loss 0.2086 | Accuracy 43.46\n",
      "Epoch 5| Batch 5800/9812 | Loss 1.0645 | Accuracy 43.36\n",
      "Epoch 5| Batch 6000/9812 | Loss 1.5712 | Accuracy 43.57\n",
      "Epoch 5| Batch 6200/9812 | Loss 0.2031 | Accuracy 43.55\n",
      "Epoch 5| Batch 6400/9812 | Loss 2.3008 | Accuracy 43.62\n",
      "Epoch 5| Batch 6600/9812 | Loss 6.2797 | Accuracy 43.62\n",
      "Epoch 5| Batch 6800/9812 | Loss 1.9309 | Accuracy 43.54\n",
      "Epoch 5| Batch 7000/9812 | Loss 3.8889 | Accuracy 43.57\n",
      "Epoch 5| Batch 7200/9812 | Loss 0.8219 | Accuracy 43.50\n",
      "Epoch 5| Batch 7400/9812 | Loss 3.9860 | Accuracy 43.47\n",
      "Epoch 5| Batch 7600/9812 | Loss 3.5804 | Accuracy 43.43\n",
      "Epoch 5| Batch 7800/9812 | Loss 0.4409 | Accuracy 43.58\n",
      "\n",
      "\n",
      "Epoch 5/10 | Loss 1.9174 | Accuracy 43.62\n",
      "Epoch 6| Batch 200/9812 | Loss 2.0408 | Accuracy 41.50\n",
      "Epoch 6| Batch 400/9812 | Loss 4.1756 | Accuracy 43.00\n",
      "Epoch 6| Batch 600/9812 | Loss 2.6130 | Accuracy 43.67\n",
      "Epoch 6| Batch 800/9812 | Loss 0.5031 | Accuracy 44.88\n",
      "Epoch 6| Batch 1000/9812 | Loss 0.7639 | Accuracy 46.00\n",
      "Epoch 6| Batch 1200/9812 | Loss 2.3183 | Accuracy 45.42\n",
      "Epoch 6| Batch 1400/9812 | Loss 1.3932 | Accuracy 45.93\n",
      "Epoch 6| Batch 1600/9812 | Loss 4.5359 | Accuracy 46.88\n",
      "Epoch 6| Batch 1800/9812 | Loss 2.4121 | Accuracy 47.44\n",
      "Epoch 6| Batch 2000/9812 | Loss 0.0034 | Accuracy 47.90\n",
      "Epoch 6| Batch 2200/9812 | Loss 1.2181 | Accuracy 47.59\n",
      "Epoch 6| Batch 2400/9812 | Loss 0.7433 | Accuracy 47.50\n",
      "Epoch 6| Batch 2600/9812 | Loss 0.3511 | Accuracy 47.88\n",
      "Epoch 6| Batch 2800/9812 | Loss 0.6871 | Accuracy 48.14\n",
      "Epoch 6| Batch 3000/9812 | Loss 0.0080 | Accuracy 47.90\n",
      "Epoch 6| Batch 3200/9812 | Loss 2.8677 | Accuracy 47.75\n",
      "Epoch 6| Batch 3400/9812 | Loss 4.0661 | Accuracy 47.74\n",
      "Epoch 6| Batch 3600/9812 | Loss 2.0549 | Accuracy 47.69\n",
      "Epoch 6| Batch 3800/9812 | Loss 0.3572 | Accuracy 47.76\n",
      "Epoch 6| Batch 4000/9812 | Loss 5.8436 | Accuracy 47.90\n",
      "Epoch 6| Batch 4200/9812 | Loss 2.0330 | Accuracy 47.81\n",
      "Epoch 6| Batch 4400/9812 | Loss 1.7386 | Accuracy 47.70\n",
      "Epoch 6| Batch 4600/9812 | Loss 0.1258 | Accuracy 47.65\n",
      "Epoch 6| Batch 4800/9812 | Loss 3.8199 | Accuracy 47.42\n",
      "Epoch 6| Batch 5000/9812 | Loss 1.7182 | Accuracy 47.62\n",
      "Epoch 6| Batch 5200/9812 | Loss 0.8116 | Accuracy 47.50\n",
      "Epoch 6| Batch 5400/9812 | Loss 0.6799 | Accuracy 47.67\n",
      "Epoch 6| Batch 5600/9812 | Loss 2.6752 | Accuracy 47.66\n",
      "Epoch 6| Batch 5800/9812 | Loss 4.2799 | Accuracy 47.66\n",
      "Epoch 6| Batch 6000/9812 | Loss 3.0879 | Accuracy 47.80\n",
      "Epoch 6| Batch 6200/9812 | Loss 4.6039 | Accuracy 47.68\n",
      "Epoch 6| Batch 6400/9812 | Loss 1.2631 | Accuracy 47.67\n",
      "Epoch 6| Batch 6600/9812 | Loss 0.8771 | Accuracy 47.64\n",
      "Epoch 6| Batch 6800/9812 | Loss 1.1771 | Accuracy 47.68\n",
      "Epoch 6| Batch 7000/9812 | Loss 0.0815 | Accuracy 47.50\n",
      "Epoch 6| Batch 7200/9812 | Loss 3.8977 | Accuracy 47.51\n",
      "Epoch 6| Batch 7400/9812 | Loss 0.9864 | Accuracy 47.53\n",
      "Epoch 6| Batch 7600/9812 | Loss 0.3787 | Accuracy 47.58\n",
      "Epoch 6| Batch 7800/9812 | Loss 0.0980 | Accuracy 47.56\n",
      "\n",
      "\n",
      "Epoch 6/10 | Loss 4.3239 | Accuracy 47.51\n",
      "Epoch 7| Batch 200/9812 | Loss 2.1380 | Accuracy 53.50\n",
      "Epoch 7| Batch 400/9812 | Loss 0.1623 | Accuracy 53.50\n",
      "Epoch 7| Batch 600/9812 | Loss 0.0728 | Accuracy 53.00\n",
      "Epoch 7| Batch 800/9812 | Loss 1.9610 | Accuracy 53.25\n",
      "Epoch 7| Batch 1000/9812 | Loss 0.0000 | Accuracy 52.70\n",
      "Epoch 7| Batch 1200/9812 | Loss 0.5488 | Accuracy 52.67\n",
      "Epoch 7| Batch 1400/9812 | Loss 0.2554 | Accuracy 51.79\n",
      "Epoch 7| Batch 1600/9812 | Loss 0.1131 | Accuracy 51.75\n",
      "Epoch 7| Batch 1800/9812 | Loss 0.7083 | Accuracy 51.17\n",
      "Epoch 7| Batch 2000/9812 | Loss 0.0188 | Accuracy 50.95\n",
      "Epoch 7| Batch 2200/9812 | Loss 2.0051 | Accuracy 50.95\n",
      "Epoch 7| Batch 2400/9812 | Loss 1.1977 | Accuracy 50.75\n",
      "Epoch 7| Batch 2600/9812 | Loss 2.3707 | Accuracy 50.69\n",
      "Epoch 7| Batch 2800/9812 | Loss 0.5874 | Accuracy 50.61\n",
      "Epoch 7| Batch 3000/9812 | Loss 4.7471 | Accuracy 50.90\n",
      "Epoch 7| Batch 3200/9812 | Loss 3.8551 | Accuracy 50.62\n",
      "Epoch 7| Batch 3400/9812 | Loss 2.0231 | Accuracy 50.71\n",
      "Epoch 7| Batch 3600/9812 | Loss 3.7743 | Accuracy 50.94\n",
      "Epoch 7| Batch 3800/9812 | Loss 0.2187 | Accuracy 51.11\n",
      "Epoch 7| Batch 4000/9812 | Loss 1.0181 | Accuracy 50.95\n",
      "Epoch 7| Batch 4200/9812 | Loss 2.9316 | Accuracy 51.10\n",
      "Epoch 7| Batch 4400/9812 | Loss 0.7490 | Accuracy 50.98\n",
      "Epoch 7| Batch 4600/9812 | Loss 1.7797 | Accuracy 50.59\n",
      "Epoch 7| Batch 4800/9812 | Loss 3.6482 | Accuracy 50.69\n",
      "Epoch 7| Batch 5000/9812 | Loss 2.9564 | Accuracy 50.84\n",
      "Epoch 7| Batch 5200/9812 | Loss 0.0720 | Accuracy 50.92\n",
      "Epoch 7| Batch 5400/9812 | Loss 0.0036 | Accuracy 50.91\n",
      "Epoch 7| Batch 5600/9812 | Loss 0.7954 | Accuracy 50.95\n",
      "Epoch 7| Batch 5800/9812 | Loss 0.3923 | Accuracy 51.10\n",
      "Epoch 7| Batch 6000/9812 | Loss 1.5897 | Accuracy 51.27\n",
      "Epoch 7| Batch 6200/9812 | Loss 2.8916 | Accuracy 51.29\n",
      "Epoch 7| Batch 6400/9812 | Loss 0.0207 | Accuracy 51.44\n",
      "Epoch 7| Batch 6600/9812 | Loss 0.0187 | Accuracy 51.42\n",
      "Epoch 7| Batch 6800/9812 | Loss 3.3162 | Accuracy 51.26\n",
      "Epoch 7| Batch 7000/9812 | Loss 5.3529 | Accuracy 51.50\n",
      "Epoch 7| Batch 7200/9812 | Loss 0.0353 | Accuracy 51.53\n",
      "Epoch 7| Batch 7400/9812 | Loss 2.2336 | Accuracy 51.55\n",
      "Epoch 7| Batch 7600/9812 | Loss 3.9957 | Accuracy 51.71\n",
      "Epoch 7| Batch 7800/9812 | Loss 0.0486 | Accuracy 51.76\n",
      "\n",
      "\n",
      "Epoch 7/10 | Loss 0.2810 | Accuracy 51.79\n",
      "Epoch 8| Batch 200/9812 | Loss 0.9761 | Accuracy 56.00\n",
      "Epoch 8| Batch 400/9812 | Loss 3.1997 | Accuracy 58.25\n",
      "Epoch 8| Batch 600/9812 | Loss 2.4733 | Accuracy 57.83\n",
      "Epoch 8| Batch 800/9812 | Loss 0.1153 | Accuracy 55.38\n",
      "Epoch 8| Batch 1000/9812 | Loss 1.3278 | Accuracy 54.60\n",
      "Epoch 8| Batch 1200/9812 | Loss 1.4886 | Accuracy 54.00\n",
      "Epoch 8| Batch 1400/9812 | Loss 0.0022 | Accuracy 54.00\n",
      "Epoch 8| Batch 1600/9812 | Loss 0.5042 | Accuracy 54.12\n",
      "Epoch 8| Batch 1800/9812 | Loss 4.0549 | Accuracy 53.89\n",
      "Epoch 8| Batch 2000/9812 | Loss 0.1622 | Accuracy 54.05\n",
      "Epoch 8| Batch 2200/9812 | Loss 1.8767 | Accuracy 53.95\n",
      "Epoch 8| Batch 2400/9812 | Loss 5.0344 | Accuracy 54.12\n",
      "Epoch 8| Batch 2600/9812 | Loss 0.3516 | Accuracy 54.19\n",
      "Epoch 8| Batch 2800/9812 | Loss 0.0014 | Accuracy 54.00\n",
      "Epoch 8| Batch 3000/9812 | Loss 0.0071 | Accuracy 53.97\n",
      "Epoch 8| Batch 3200/9812 | Loss 4.4764 | Accuracy 54.03\n",
      "Epoch 8| Batch 3400/9812 | Loss 3.4599 | Accuracy 53.85\n",
      "Epoch 8| Batch 3600/9812 | Loss 0.8578 | Accuracy 54.19\n",
      "Epoch 8| Batch 3800/9812 | Loss 0.9364 | Accuracy 54.68\n",
      "Epoch 8| Batch 4000/9812 | Loss 0.3971 | Accuracy 54.48\n",
      "Epoch 8| Batch 4200/9812 | Loss 2.2972 | Accuracy 54.38\n",
      "Epoch 8| Batch 4400/9812 | Loss 1.4662 | Accuracy 54.39\n",
      "Epoch 8| Batch 4600/9812 | Loss 0.8241 | Accuracy 54.63\n",
      "Epoch 8| Batch 4800/9812 | Loss 1.6876 | Accuracy 54.48\n",
      "Epoch 8| Batch 5000/9812 | Loss 0.0101 | Accuracy 54.38\n",
      "Epoch 8| Batch 5200/9812 | Loss 2.4009 | Accuracy 54.50\n",
      "Epoch 8| Batch 5400/9812 | Loss 0.0000 | Accuracy 54.33\n",
      "Epoch 8| Batch 5600/9812 | Loss 2.3104 | Accuracy 54.34\n",
      "Epoch 8| Batch 5800/9812 | Loss 0.0149 | Accuracy 54.52\n",
      "Epoch 8| Batch 6000/9812 | Loss 0.0000 | Accuracy 54.45\n",
      "Epoch 8| Batch 6200/9812 | Loss 2.2409 | Accuracy 54.26\n",
      "Epoch 8| Batch 6400/9812 | Loss 1.9068 | Accuracy 54.33\n",
      "Epoch 8| Batch 6600/9812 | Loss 3.6366 | Accuracy 54.23\n",
      "Epoch 8| Batch 6800/9812 | Loss 2.1886 | Accuracy 54.29\n",
      "Epoch 8| Batch 7000/9812 | Loss 3.0045 | Accuracy 54.26\n",
      "Epoch 8| Batch 7200/9812 | Loss 0.2882 | Accuracy 54.29\n",
      "Epoch 8| Batch 7400/9812 | Loss 1.4534 | Accuracy 54.30\n",
      "Epoch 8| Batch 7600/9812 | Loss 1.5368 | Accuracy 54.29\n",
      "Epoch 8| Batch 7800/9812 | Loss 2.8228 | Accuracy 54.24\n",
      "\n",
      "\n",
      "Epoch 8/10 | Loss 1.5410 | Accuracy 54.20\n",
      "Epoch 9| Batch 200/9812 | Loss 1.4034 | Accuracy 59.00\n",
      "Epoch 9| Batch 400/9812 | Loss 0.0212 | Accuracy 59.25\n",
      "Epoch 9| Batch 600/9812 | Loss 0.5790 | Accuracy 59.33\n",
      "Epoch 9| Batch 800/9812 | Loss 0.8111 | Accuracy 60.12\n",
      "Epoch 9| Batch 1000/9812 | Loss 2.5092 | Accuracy 60.40\n",
      "Epoch 9| Batch 1200/9812 | Loss 2.4411 | Accuracy 59.67\n",
      "Epoch 9| Batch 1400/9812 | Loss 3.4434 | Accuracy 59.00\n",
      "Epoch 9| Batch 1600/9812 | Loss 5.2516 | Accuracy 57.88\n",
      "Epoch 9| Batch 1800/9812 | Loss 0.4808 | Accuracy 58.11\n",
      "Epoch 9| Batch 2000/9812 | Loss 0.0383 | Accuracy 57.80\n",
      "Epoch 9| Batch 2200/9812 | Loss 0.2279 | Accuracy 57.91\n",
      "Epoch 9| Batch 2400/9812 | Loss 1.8294 | Accuracy 56.96\n",
      "Epoch 9| Batch 2600/9812 | Loss 0.0089 | Accuracy 57.31\n",
      "Epoch 9| Batch 2800/9812 | Loss 3.7758 | Accuracy 57.04\n",
      "Epoch 9| Batch 3000/9812 | Loss 1.2376 | Accuracy 56.53\n",
      "Epoch 9| Batch 3200/9812 | Loss 0.7436 | Accuracy 56.91\n",
      "Epoch 9| Batch 3400/9812 | Loss 1.7869 | Accuracy 56.76\n",
      "Epoch 9| Batch 3600/9812 | Loss 0.3892 | Accuracy 56.78\n",
      "Epoch 9| Batch 3800/9812 | Loss 0.2010 | Accuracy 56.87\n",
      "Epoch 9| Batch 4000/9812 | Loss 0.2547 | Accuracy 56.67\n",
      "Epoch 9| Batch 4200/9812 | Loss 3.0277 | Accuracy 56.62\n",
      "Epoch 9| Batch 4400/9812 | Loss 0.0065 | Accuracy 56.82\n",
      "Epoch 9| Batch 4600/9812 | Loss 1.6938 | Accuracy 56.89\n",
      "Epoch 9| Batch 4800/9812 | Loss 3.4869 | Accuracy 56.79\n",
      "Epoch 9| Batch 5000/9812 | Loss 0.4947 | Accuracy 56.74\n",
      "Epoch 9| Batch 5200/9812 | Loss 0.2942 | Accuracy 56.52\n",
      "Epoch 9| Batch 5400/9812 | Loss 0.0208 | Accuracy 56.44\n",
      "Epoch 9| Batch 5600/9812 | Loss 0.4207 | Accuracy 56.48\n",
      "Epoch 9| Batch 5800/9812 | Loss 0.0692 | Accuracy 56.34\n",
      "Epoch 9| Batch 6000/9812 | Loss 0.0000 | Accuracy 56.62\n",
      "Epoch 9| Batch 6200/9812 | Loss 0.0827 | Accuracy 56.53\n",
      "Epoch 9| Batch 6400/9812 | Loss 2.3472 | Accuracy 56.47\n",
      "Epoch 9| Batch 6600/9812 | Loss 1.0443 | Accuracy 56.53\n",
      "Epoch 9| Batch 6800/9812 | Loss 4.0674 | Accuracy 56.60\n",
      "Epoch 9| Batch 7000/9812 | Loss 1.3284 | Accuracy 56.59\n",
      "Epoch 9| Batch 7200/9812 | Loss 0.7020 | Accuracy 56.62\n",
      "Epoch 9| Batch 7400/9812 | Loss 1.9857 | Accuracy 56.72\n",
      "Epoch 9| Batch 7600/9812 | Loss 2.1588 | Accuracy 56.68\n",
      "Epoch 9| Batch 7800/9812 | Loss 3.6273 | Accuracy 56.64\n",
      "\n",
      "\n",
      "Epoch 9/10 | Loss 0.1482 | Accuracy 56.58\n"
     ]
    }
   ],
   "source": [
    "## UCF_CNN2D ##\n",
    "num_of_classes = 101\n",
    "h_in, h_out = 30, num_of_classes # UCF101\n",
    "h1, h2, h3, h4, h5 = 256, 128, 64, 1000, 500\n",
    "CNN_2D = UCF_CNN2D(h_in, h1, h2, h3, h4, h5, h_out).to(device)\n",
    "\n",
    "\n",
    "criterion_2D = nn.CrossEntropyLoss()\n",
    "optimizer_2D = optim.Adam(CNN_2D.parameters(), lr=0.0001,  weight_decay=1e-5)\n",
    "\n",
    "print(\"\\n\\n\\{}\\n\".format(CNN_2D.__class__.__name__ ))\n",
    "\n",
    "## Training the 2DCNN ##\n",
    "CNN_2D.train()\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bs_step = 0\n",
    "    correct = 0\n",
    "    for x, y in train_loader: \n",
    "\n",
    "        x, y_true = x.to(device), y.squeeze().long().to(device)\n",
    "        optimizer_2D.zero_grad()\n",
    "\n",
    "        y_pred = CNN_2D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "    \n",
    "        loss = criterion_2D(y_pred, y_true.unsqueeze(0))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer_2D.step()\n",
    "        bs_step += bs\n",
    "        correct += (predicted == y_true).sum().item()\n",
    "        if bs_step %200 == 0:\n",
    "            print(\"Epoch {}| Batch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, bs_step, 9812, loss.item(), 100 * correct / bs_step))\n",
    "\n",
    "\n",
    "    train_acc = 100 * correct / bs_step\n",
    "\n",
    "    print(\"\\n\\nEpoch {}/{} | Loss {:.4f} | Accuracy {:2.2f}\".format(\n",
    "            epoch, epochs, loss.item(), train_acc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  The test accuracy 60.67%\n"
     ]
    }
   ],
   "source": [
    "CNN_2D.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x, labels = x.to(device), y.squeeze().long().to(device)\n",
    "\n",
    "        y_pred = CNN_2D(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "\n",
    "print(\"\\n\\n\\n  The test accuracy {:2.2f}%\".format(test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UCFdataset(data.Dataset):\n",
    "#     def __init__(self, upper, lower, dir_path): # 'train', 'validation'\n",
    "#         super(UCFdataset, self).__init__()\n",
    "        \n",
    "#         self.file_list, self.y, self.video_names = self.file_load(upper, lower, dir_path)\n",
    "        \n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         x = jpg2np(self.file_list[index]) / 255. # (30, 3, 240, 320)\n",
    "#         self.x_data = torch.from_numpy(x).float()\n",
    "#         self.y_data = torch.from_numpy(self.y[index]).float()\n",
    "#         return self.x_data, self.y_data, self.video_names[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.y.shape[0]\n",
    "    \n",
    "#     def file_load(self, upper, lower, dir_path):\n",
    "#         \"\"\"\n",
    "#         return the input file path list\n",
    "#         \"\"\"\n",
    "#         data_path = []\n",
    "#         video_imgs_path = os.path.join(os.getcwd(), dir_path)\n",
    "#         folders = os.listdir(video_imgs_path)\n",
    "\n",
    "#         frames = {}\n",
    "#         for folder in folders:\n",
    "#             path = os.path.join(video_imgs_path, folder)\n",
    "#             frames[folder] = len(os.listdir(path))\n",
    "\n",
    "#         video_names = []\n",
    "#         for video_name, num_of_frames in zip(list(frames.keys()), list(frames.values())):\n",
    "#             if upper <= num_of_frames and num_of_frames <= lower:\n",
    "#                 video_names.append(video_name)\n",
    "#         video_names = natsort.natsorted(video_names)\n",
    "                \n",
    "#         print(\"Select The number of frames between [%d, %d] of UCF101 Dataset\" %(upper, lower))\n",
    "#         print('The number of selected videos is', len(video_names))\n",
    "\n",
    "\n",
    "#         data_path = [os.path.join(video_imgs_path, video_name) for video_name in video_names]\n",
    "\n",
    "#         labels = []\n",
    "#         for label in video_names:\n",
    "#             loc1 = label.find('_')\n",
    "#             loc2 = loc1 + label[loc1+1:].find('_')\n",
    "#             labels.append(label[loc1+1:loc2+1])\n",
    "#         y, _ = encoder(labels)\n",
    "# #         y = torch.tensor(y, dtype=torch.float) # [N(13320, 1)\n",
    "\n",
    "#         return data_path, y, video_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms-tcn",
   "language": "python",
   "name": "ms-tcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
